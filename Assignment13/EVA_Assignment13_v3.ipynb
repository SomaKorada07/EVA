{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA_Assignment13_v3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOPOrH3Bhw-Y",
        "colab_type": "code",
        "outputId": "c7af524f-962a-437a-90ab-b4bddf68a631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Importing NumPy (Numerical Python) which is a library consisting of multidimensional array objects and a collection of routines for processing those arrays. It also gives an alias to the library.\n",
        "import numpy as np\n",
        "\n",
        "% matplotlib inline\n",
        "np.random.seed(2017)\n",
        "\n",
        "from keras import regularizers\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Activation, Flatten, Dropout, GlobalAveragePooling2D, Input, Dense\n",
        "from keras.layers.merge import add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import Callback\n",
        "# Importing the callbacks of Keras.\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.datasets import cifar10\n",
        "from keras import backend as K"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCDoIVT9OOH4",
        "colab_type": "code",
        "outputId": "6317c606-1ad5-45c9-8c7d-313e01067deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Mounting the Google Drive to save the weights\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5mkJO53h-Zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128 #@param [\"512\", \"256\", \"128\"] {type:\"raw\"}\n",
        "MOMENTUM = 0.9 #@param [\"0.9\", \"0.95\", \"0.975\"] {type:\"raw\"}\n",
        "WEIGHT_DECAY = 5e-4 #@param [\"0.000125\", \"0.00025\", \"0.0005\", \"5e-4\"] {type:\"raw\"}\n",
        "LEARNING_RATE = 0.1 #@param [\"0.4\", \"0.2\", \"0.1\"] {type:\"raw\"}\n",
        "EPOCHS = 300 #@param {type:\"slider\", min:0, max:300, step:1}\n",
        "WARMUP = 5 #@param {type:\"slider\", min:0, max:24, step:1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WR3LO8tidAc",
        "colab_type": "code",
        "outputId": "02adff83-791e-4e5e-fbdf-165bf3d1165b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# CIFAR10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. Importing CIFAR10 dataset from Keras.\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "# Loading the CIFAR10 60000 Training and 10000 Test data into respective numpy arrays\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "n_train, n_test = X_train.shape[0], X_test.shape[0]\n",
        "img_size = X_train.shape[1]\n",
        "n_classes = y_train.max() + 1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJq-7uMpmU6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')/255\n",
        "X_test = X_test.astype('float32')/255\n",
        "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2smHikv8OChz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7S_Eg5lODYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(validation_iterator, test_y, model):\n",
        "    result = model.evaluate_generator(validation_iterator, steps = len(validation_iterator))\n",
        "    predicted_class = np.argmax(result, axis=1)\n",
        "    true_class = np.argmax(test_y, axis=1)\n",
        "    num_correct = np.sum(predicted_class == true_class) \n",
        "    accuracy = float(num_correct)/result.shape[0]\n",
        "    return (accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMpsqn_O0CvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_examples(X,y,classes):\n",
        "    rows = int(np.ceil(len(X)/5))\n",
        "    if X.shape[1] > 64:\n",
        "        multiplier = 2\n",
        "    else:\n",
        "        multiplier = 1\n",
        "    fig = plt.figure(figsize=(10*multiplier, rows*2*multiplier))\n",
        "    for idx in np.arange(len(X)):\n",
        "        img = X[idx]\n",
        "        assert (len(img.shape)==3 and img.shape[2] in [1,3,4]) or len(img.shape)==2\n",
        "        ax = fig.add_subplot(rows, 5, idx + 1, xticks=[], yticks=[])\n",
        "        cmap = None\n",
        "        if (len(img.shape)==3 and img.shape[2]==1) or len(img.shape)==2:\n",
        "            cmap=\"binary\"\n",
        "        if len(img.shape)==3 and img.shape[2]==1:\n",
        "            img = img.reshape((img.shape[0],img.shape[1]))\n",
        "        ax.imshow(img,cmap=cmap)\n",
        "        ax.set_title(classes[np.argmax(y[idx])])\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFJt6i8e0IzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cifar10_labels():\n",
        "    return ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMBWETcC0wi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def min_max_scale(X):\n",
        "  return (X - np.min(X))/(np.max(X)-np.min(X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDK_7gDlHEFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cutout_eraser_and_padcrop(p=0.5, s_l=0.05, s_h=0.3, r_1=0.3, r_2=1 / 0.3, max_erasures_per_image=1, pixel_level=True, random_crop_size=(32, 32), padding_pixels=4):\n",
        "    \"\"\"\n",
        "    :param p:\n",
        "    :param s_l: Minimum Area Proportion of Original that may be cut\n",
        "    :param s_h: Maximum Area Proportion of Original that may be cut\n",
        "    :param r_1: Min Aspect Ratio\n",
        "    :param r_2: Max Aspect Ratio\n",
        "    :param max_erasures_per_image:\n",
        "    :param pixel_level:\n",
        "    :return: Eraser to be used as Preprocessing Function\n",
        "    \"\"\"\n",
        "    assert max_erasures_per_image >= 1\n",
        "\n",
        "    def eraser(input_img):\n",
        "        v_l = np.min(input_img)\n",
        "        v_h = np.max(input_img)\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        mx = np.random.randint(1, max_erasures_per_image + 1)\n",
        "        # print(\"Erasures = \",mx,end =\", \")\n",
        "        for i in range(mx):\n",
        "            while True:\n",
        "                s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "                r = np.random.uniform(r_1, r_2)\n",
        "                w = int(np.sqrt(s / r))\n",
        "                h = int(np.sqrt(s * r))\n",
        "                left = np.random.randint(0, img_w)\n",
        "                top = np.random.randint(0, img_h)\n",
        "\n",
        "                if left + w <= img_w and top + h <= img_h:\n",
        "                    break\n",
        "\n",
        "            # print(\"W = \",w,\"H = \",h,end =\", \")\n",
        "\n",
        "            if pixel_level:\n",
        "                # print(np.max(img_c),np.min(img_c),v_l,v_h)\n",
        "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "                # print(c.shape,np.min(c),np.max(c),np.median(c))\n",
        "            else:\n",
        "                c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "            input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        # print()\n",
        "        return input_img\n",
        "     \n",
        "    def random_crop(input_image):\n",
        "          # Note: image_data_format is 'channel_last'\n",
        "          assert input_image.shape[2] == 3\n",
        "          \n",
        "          #Pad by 4 pixels\n",
        "          img = cv2.copyMakeBorder(input_image, padding_pixels, padding_pixels, padding_pixels, padding_pixels, cv2.BORDER_REPLICATE)\n",
        "          \n",
        "          height, width = img.shape[0], img.shape[1]\n",
        "          dy, dx = random_crop_size\n",
        "          x = np.random.randint(0, width - dx + 1)\n",
        "          y = np.random.randint(0, height - dy + 1)\n",
        "          return img[y:(y+dy), x:(x+dx), :]\n",
        "        \n",
        "    def preproc_image(input_image):\n",
        "      #return eraser\n",
        "      return eraser(random_crop(input_image))\n",
        "\n",
        "    return preproc_image\n",
        "      \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBPdtlO75o2O",
        "colab_type": "code",
        "outputId": "1e911da9-b7c2-4aaa-c235-c8daf935a195",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "   featurewise_center=True,\n",
        "   featurewise_std_normalization=True,\n",
        "   horizontal_flip=True,                 # randomly flip images                                     \n",
        "   preprocessing_function=get_cutout_eraser_and_padcrop(p=0.75, s_l=0.05, s_h=0.2, r_1=0.3, r_2=1 / 0.3, max_erasures_per_image=1, pixel_level=True))\n",
        "\n",
        "_ = datagen.fit(X_train)\n",
        "train_iterator = datagen.flow(X_train, Y_train, batch_size=100,shuffle=False)\n",
        "\n",
        "X_e, Y_e = train_iterator.next()\n",
        "X_e = min_max_scale(X_e)\n",
        "show_examples(X_e[0:10], Y_e[0:10], classes = get_cifar10_labels())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAAD4CAYAAADvq+IEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 720x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo_GNPlGV_Bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen_validation = ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True,)\n",
        "datagen.mean = np.array([0.4914, 0.4822, 0.4465], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "datagen.std = np.array([0.2023, 0.1994, 0.2010], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "datagen.fit(X_train)\n",
        "datagen_validation.fit(X_train)\n",
        "train_iterator = datagen.flow(X_train, Y_train, batch_size=256,shuffle=True)\n",
        "validation_iterator = datagen_validation.flow(X_test, Y_test, batch_size=512,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oETUKntCiqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _conv_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLbFteMhCj1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
        "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        for i in range(repetitions):\n",
        "            init_strides = (1, 1)\n",
        "            if i == 0 and not is_first_layer:\n",
        "                init_strides = (2, 2)\n",
        "            input = block_function(filters=filters, init_strides=init_strides,\n",
        "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
        "        return input\n",
        "\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgobaLQNDEJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
        "                           strides=init_strides,\n",
        "                           padding=\"same\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
        "                                  strides=init_strides)(input)\n",
        "\n",
        "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvmD6gNhHy65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _bn_relu_conv(**conv_params):\n",
        "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
        "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(activation)\n",
        "\n",
        "    return f\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqPiKSNRH-8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _shortcut(input, residual):\n",
        "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "    input_shape = K.int_shape(input)\n",
        "    residual_shape = K.int_shape(residual)\n",
        "    stride_width = int(round(input_shape[1] / residual_shape[1]))\n",
        "    stride_height = int(round(input_shape[2] / residual_shape[2]))\n",
        "    equal_channels = input_shape[3] == residual_shape[3]\n",
        "\n",
        "    shortcut = input\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
        "        shortcut = Conv2D(filters=residual_shape[3],\n",
        "                          kernel_size=(1, 1),\n",
        "                          strides=(stride_width, stride_height),\n",
        "                          padding=\"valid\",\n",
        "                          kernel_initializer=\"he_normal\",\n",
        "                          kernel_regularizer=l2(0.0001))(input)\n",
        "\n",
        "    return add([shortcut, residual])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvUCI7nrDNKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN -> relu block\n",
        "    \"\"\"\n",
        "    norm = BatchNormalization(axis=3)(input)\n",
        "    return Activation(\"relu\")(norm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10HGAQbDDg6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape=(32, 32, 3)\n",
        "num_outputs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMTngMZGDN_T",
        "colab_type": "code",
        "outputId": "51c7a3b2-2774-4a3e-c1d8-6c70e9dde91d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "#Define ResNet18 model\n",
        "input = Input(shape=input_shape)\n",
        "conv1 = _conv_bn_relu(filters=32, kernel_size=(3, 3), strides=(1, 1))(input)\n",
        "#pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
        "\n",
        "block = conv1\n",
        "\n",
        "#B1\n",
        "filters = 32\n",
        "block = _residual_block(basic_block, filters=filters, repetitions=2, is_first_layer=True)(block)\n",
        "\n",
        "#B2\n",
        "filters = 64\n",
        "block = _residual_block(basic_block, filters=filters, repetitions=2, is_first_layer=False)(block)\n",
        "\n",
        "#B3\n",
        "filters = 128\n",
        "block = _residual_block(basic_block, filters=filters, repetitions=2, is_first_layer=False)(block)\n",
        "\n",
        "#B4\n",
        "filters = 256\n",
        "block = _residual_block(basic_block, filters=filters, repetitions=2, is_first_layer=False)(block)\n",
        "\n",
        "# Last activation\n",
        "block = _bn_relu(block)\n",
        "\n",
        "# Classifier block\n",
        "pool = GlobalAveragePooling2D()(block)\n",
        "dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n",
        "              activation=\"softmax\")(pool)\n",
        "\n",
        "model = Model(inputs=input, outputs=dense)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0804 14:17:46.128683 140074430699392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0804 14:17:46.162058 140074430699392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0804 14:17:46.168642 140074430699392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0804 14:17:46.209613 140074430699392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0804 14:17:46.210499 140074430699392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0804 14:17:48.893613 140074430699392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRKvgcZrFgIj",
        "colab_type": "code",
        "outputId": "7088c9d0-969e-4b71-808f-c6fadd4e97ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 32)   896         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 32)   9248        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 32)   9248        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 32)   0           activation_1[0][0]               \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 32)   9248        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 32)   0           add_1[0][0]                      \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 64)   18496       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 64)   2112        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 64)   36928       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 64)   0           conv2d_8[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 64)   36928       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 64)   36928       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 64)   0           add_3[0][0]                      \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 8, 8, 128)    73856       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 8, 8, 128)    512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 8, 8, 128)    0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 128)    8320        add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 8, 8, 128)    147584      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 128)    0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 8, 8, 128)    512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 8, 8, 128)    0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 128)    147584      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 8, 8, 128)    512         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 8, 8, 128)    0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 128)    147584      activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 128)    0           add_5[0][0]                      \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 128)    512         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 8, 8, 128)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 256)    295168      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 4, 4, 256)    1024        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 4, 4, 256)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 4, 4, 256)    33024       add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 4, 4, 256)    590080      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 4, 4, 256)    0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 256)    1024        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 256)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 4, 4, 256)    590080      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 4, 4, 256)    1024        conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 256)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 4, 4, 256)    590080      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 4, 4, 256)    0           add_7[0][0]                      \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 4, 4, 256)    1024        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 4, 4, 256)    0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 256)          0           activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           2570        global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 2,803,018\n",
            "Trainable params: 2,799,114\n",
            "Non-trainable params: 3,904\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4DmCxcNgEq0",
        "colab_type": "code",
        "outputId": "ef2177e1-b017-471d-9a03-30d098e02b9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/SomaKorada07/EIP/master/utils/clr.py\""
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-04 14:17:51--  https://raw.githubusercontent.com/SomaKorada07/EIP/master/utils/clr.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7928 (7.7K) [text/plain]\n",
            "Saving to: ‘clr.py’\n",
            "\n",
            "\rclr.py                0%[                    ]       0  --.-KB/s               \rclr.py              100%[===================>]   7.74K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2019-08-04 14:17:51 (7.13 MB/s) - ‘clr.py’ saved [7928/7928]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNNpKihGPwLJ",
        "colab_type": "code",
        "outputId": "1fba325c-55e8-4ecb-d206-6402f2188c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/SomaKorada07/EIP/master/utils/lr_finder.py\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-04 14:17:53--  https://raw.githubusercontent.com/SomaKorada07/EIP/master/utils/lr_finder.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14968 (15K) [text/plain]\n",
            "Saving to: ‘lr_finder.py’\n",
            "\n",
            "\rlr_finder.py          0%[                    ]       0  --.-KB/s               \rlr_finder.py        100%[===================>]  14.62K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-08-04 14:17:53 (1.45 MB/s) - ‘lr_finder.py’ saved [14968/14968]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obJeeUUCiGaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from lr_finder import LRFinder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRoHy2TUct44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_epoch = 1\n",
        "\n",
        "weights_file = 'weights/model.h5'\n",
        "model_checkpoint = ModelCheckpoint(weights_file, monitor='val_acc', save_best_only=True,\n",
        "                                   save_weights_only=True, mode='max')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl2hw4mZcI2f",
        "colab_type": "code",
        "outputId": "c1de465a-1ab2-4467-ba58-4084d72140f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Exponential lr finder\n",
        "# USE THIS FOR A LARGE RANGE SEARCH\n",
        "# Uncomment the validation_data flag to reduce speed but get a better idea of the learning rate\n",
        "#lr_finder = LRFinder(n_train, BATCH_SIZE, minimum_lr=1e-3, maximum_lr=10.,\n",
        "                     #lr_scale='exp',\n",
        "                     #validation_data=(X_test, Y_test),  # use the validation data for losses\n",
        "                     #validation_sample_rate=5,\n",
        "                     #save_dir='weights/', verbose=True)\n",
        "\n",
        "# Linear lr finder\n",
        "# USE THIS FOR A CLOSE SEARCH\n",
        "# Uncomment the validation_data flag to reduce speed but get a better idea of the learning rate\n",
        "lr_finder = LRFinder(n_train, BATCH_SIZE, minimum_lr=5e-4, maximum_lr=1e-2,\n",
        "                      lr_scale='linear',\n",
        "                      validation_data=(X_test, y_test),  # use the validation data for losses\n",
        "                      validation_sample_rate=5,\n",
        "                      save_dir='weights/', verbose=True)\n",
        "\n",
        "\n",
        "# plot the previous values if present\n",
        "LRFinder.plot_schedule_from_file('weights/', clip_beginning=10, clip_endding=5)\n",
        "\n",
        "optimizer = SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE, shuffle=True),\n",
        "                        steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
        "                        validation_data=(X_test, Y_test),\n",
        "                        epochs=nb_epoch, verbose=1,\n",
        "                        callbacks=[lr_finder, model_checkpoint])\n",
        "\n",
        "lr_finder.plot_schedule(clip_beginning=10, clip_endding=5)\n",
        "\n",
        "scores = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)\n",
        "for score, metric_name in zip(scores, model.metrics_names):\n",
        "    print(\"%s : %0.4f\" % (metric_name, score))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0804 14:17:56.545651 140074430699392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights/losses.npy and weights/lrs.npy could not be found at directory : {weights/}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0804 14:17:56.907587 140074430699392 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 35:10 - loss: 2.8008 - acc: 0.1250 - LRFinder: val_loss: 2.8388 - lr = 0.00050000 \n",
            "  2/390 [..............................] - ETA: 18:55 - loss: 2.8004 - acc: 0.1289 - LRFinder: val_loss: 2.8378 - lr = 0.00052436 \n",
            "  3/390 [..............................] - ETA: 12:52 - loss: 2.8043 - acc: 0.1276 - LRFinder: val_loss: 2.8411 - lr = 0.00054872 \n",
            "  4/390 [..............................] - ETA: 9:51 - loss: 2.8179 - acc: 0.1211  - LRFinder: val_loss: 2.8418 - lr = 0.00057308 \n",
            "  5/390 [..............................] - ETA: 8:02 - loss: 2.8165 - acc: 0.1313 - LRFinder: val_loss: 2.8486 - lr = 0.00059744 \n",
            "  6/390 [..............................] - ETA: 6:49 - loss: 2.8246 - acc: 0.1276 - LRFinder: val_loss: 2.8288 - lr = 0.00062179 \n",
            "  7/390 [..............................] - ETA: 5:57 - loss: 2.8205 - acc: 0.1306 - LRFinder: val_loss: 2.8389 - lr = 0.00064615 \n",
            "  8/390 [..............................] - ETA: 5:19 - loss: 2.8261 - acc: 0.1289 - LRFinder: val_loss: 2.8601 - lr = 0.00067051 \n",
            "  9/390 [..............................] - ETA: 4:48 - loss: 2.8226 - acc: 0.1302 - LRFinder: val_loss: 2.8476 - lr = 0.00069487 \n",
            " 10/390 [..............................] - ETA: 4:24 - loss: 2.8210 - acc: 0.1313 - LRFinder: val_loss: 2.8728 - lr = 0.00071923 \n",
            " 11/390 [..............................] - ETA: 4:04 - loss: 2.8181 - acc: 0.1307 - LRFinder: val_loss: 2.8592 - lr = 0.00074359 \n",
            " 12/390 [..............................] - ETA: 3:48 - loss: 2.8189 - acc: 0.1322 - LRFinder: val_loss: 2.8592 - lr = 0.00076795 \n",
            " 13/390 [>.............................] - ETA: 3:33 - loss: 2.8192 - acc: 0.1292 - LRFinder: val_loss: 2.8613 - lr = 0.00079231 \n",
            " 14/390 [>.............................] - ETA: 3:22 - loss: 2.8170 - acc: 0.1295 - LRFinder: val_loss: 2.8388 - lr = 0.00081667 \n",
            " 15/390 [>.............................] - ETA: 3:11 - loss: 2.8215 - acc: 0.1266 - LRFinder: val_loss: 2.8605 - lr = 0.00084103 \n",
            " 16/390 [>.............................] - ETA: 3:02 - loss: 2.8173 - acc: 0.1279 - LRFinder: val_loss: 2.8704 - lr = 0.00086538 \n",
            " 17/390 [>.............................] - ETA: 2:54 - loss: 2.8130 - acc: 0.1296 - LRFinder: val_loss: 2.8542 - lr = 0.00088974 \n",
            " 18/390 [>.............................] - ETA: 2:46 - loss: 2.8100 - acc: 0.1319 - LRFinder: val_loss: 2.8499 - lr = 0.00091410 \n",
            " 19/390 [>.............................] - ETA: 2:40 - loss: 2.8070 - acc: 0.1349 - LRFinder: val_loss: 2.8292 - lr = 0.00093846 \n",
            " 20/390 [>.............................] - ETA: 2:34 - loss: 2.8069 - acc: 0.1355 - LRFinder: val_loss: 2.8324 - lr = 0.00096282 \n",
            " 21/390 [>.............................] - ETA: 2:29 - loss: 2.8072 - acc: 0.1358 - LRFinder: val_loss: 2.8276 - lr = 0.00098718 \n",
            " 22/390 [>.............................] - ETA: 2:24 - loss: 2.8068 - acc: 0.1357 - LRFinder: val_loss: 2.8410 - lr = 0.00101154 \n",
            " 23/390 [>.............................] - ETA: 2:20 - loss: 2.8075 - acc: 0.1338 - LRFinder: val_loss: 2.8447 - lr = 0.00103590 \n",
            " 24/390 [>.............................] - ETA: 2:15 - loss: 2.8063 - acc: 0.1322 - LRFinder: val_loss: 2.8300 - lr = 0.00106026 \n",
            " 25/390 [>.............................] - ETA: 2:12 - loss: 2.8070 - acc: 0.1303 - LRFinder: val_loss: 2.8386 - lr = 0.00108462 \n",
            " 26/390 [=>............................] - ETA: 2:08 - loss: 2.8054 - acc: 0.1304 - LRFinder: val_loss: 2.8165 - lr = 0.00110897 \n",
            " 27/390 [=>............................] - ETA: 2:05 - loss: 2.8027 - acc: 0.1337 - LRFinder: val_loss: 2.8240 - lr = 0.00113333 \n",
            " 28/390 [=>............................] - ETA: 2:02 - loss: 2.8022 - acc: 0.1334 - LRFinder: val_loss: 2.8397 - lr = 0.00115769 \n",
            " 29/390 [=>............................] - ETA: 1:59 - loss: 2.8006 - acc: 0.1342 - LRFinder: val_loss: 2.8431 - lr = 0.00118205 \n",
            " 30/390 [=>............................] - ETA: 1:56 - loss: 2.7987 - acc: 0.1346 - LRFinder: val_loss: 2.8397 - lr = 0.00120641 \n",
            " 31/390 [=>............................] - ETA: 1:54 - loss: 2.7976 - acc: 0.1348 - LRFinder: val_loss: 2.8231 - lr = 0.00123077 \n",
            " 32/390 [=>............................] - ETA: 1:52 - loss: 2.7968 - acc: 0.1343 - LRFinder: val_loss: 2.8178 - lr = 0.00125513 \n",
            " 33/390 [=>............................] - ETA: 1:49 - loss: 2.7959 - acc: 0.1359 - LRFinder: val_loss: 2.8279 - lr = 0.00127949 \n",
            " 34/390 [=>............................] - ETA: 1:47 - loss: 2.7942 - acc: 0.1376 - LRFinder: val_loss: 2.8250 - lr = 0.00130385 \n",
            " 35/390 [=>............................] - ETA: 1:45 - loss: 2.7943 - acc: 0.1377 - LRFinder: val_loss: 2.8396 - lr = 0.00132821 \n",
            " 36/390 [=>............................] - ETA: 1:44 - loss: 2.7930 - acc: 0.1365 - LRFinder: val_loss: 2.8373 - lr = 0.00135256 \n",
            " 37/390 [=>............................] - ETA: 1:42 - loss: 2.7917 - acc: 0.1364 - LRFinder: val_loss: 2.8357 - lr = 0.00137692 \n",
            " 38/390 [=>............................] - ETA: 1:40 - loss: 2.7907 - acc: 0.1369 - LRFinder: val_loss: 2.8445 - lr = 0.00140128 \n",
            " 39/390 [==>...........................] - ETA: 1:39 - loss: 2.7886 - acc: 0.1382 - LRFinder: val_loss: 2.8312 - lr = 0.00142564 \n",
            " 40/390 [==>...........................] - ETA: 1:37 - loss: 2.7866 - acc: 0.1398 - LRFinder: val_loss: 2.8235 - lr = 0.00145000 \n",
            " 41/390 [==>...........................] - ETA: 1:36 - loss: 2.7858 - acc: 0.1401 - LRFinder: val_loss: 2.8216 - lr = 0.00147436 \n",
            " 42/390 [==>...........................] - ETA: 1:34 - loss: 2.7850 - acc: 0.1401 - LRFinder: val_loss: 2.8507 - lr = 0.00149872 \n",
            " 43/390 [==>...........................] - ETA: 1:33 - loss: 2.7826 - acc: 0.1414 - LRFinder: val_loss: 2.8190 - lr = 0.00152308 \n",
            " 44/390 [==>...........................] - ETA: 1:31 - loss: 2.7819 - acc: 0.1413 - LRFinder: val_loss: 2.8313 - lr = 0.00154744 \n",
            " 45/390 [==>...........................] - ETA: 1:30 - loss: 2.7817 - acc: 0.1410 - LRFinder: val_loss: 2.8254 - lr = 0.00157179 \n",
            " 46/390 [==>...........................] - ETA: 1:29 - loss: 2.7797 - acc: 0.1423 - LRFinder: val_loss: 2.8426 - lr = 0.00159615 \n",
            " 47/390 [==>...........................] - ETA: 1:28 - loss: 2.7773 - acc: 0.1444 - LRFinder: val_loss: 2.8330 - lr = 0.00162051 \n",
            " 48/390 [==>...........................] - ETA: 1:27 - loss: 2.7759 - acc: 0.1455 - LRFinder: val_loss: 2.8552 - lr = 0.00164487 \n",
            " 49/390 [==>...........................] - ETA: 1:26 - loss: 2.7745 - acc: 0.1465 - LRFinder: val_loss: 2.8446 - lr = 0.00166923 \n",
            " 50/390 [==>...........................] - ETA: 1:25 - loss: 2.7724 - acc: 0.1472 - LRFinder: val_loss: 2.8382 - lr = 0.00169359 \n",
            " 51/390 [==>...........................] - ETA: 1:24 - loss: 2.7703 - acc: 0.1494 - LRFinder: val_loss: 2.8495 - lr = 0.00171795 \n",
            " 52/390 [===>..........................] - ETA: 1:23 - loss: 2.7687 - acc: 0.1505 - LRFinder: val_loss: 2.8326 - lr = 0.00174231 \n",
            " 53/390 [===>..........................] - ETA: 1:22 - loss: 2.7665 - acc: 0.1521 - LRFinder: val_loss: 2.8212 - lr = 0.00176667 \n",
            " 54/390 [===>..........................] - ETA: 1:21 - loss: 2.7654 - acc: 0.1528 - LRFinder: val_loss: 2.8713 - lr = 0.00179103 \n",
            " 55/390 [===>..........................] - ETA: 1:20 - loss: 2.7639 - acc: 0.1531 - LRFinder: val_loss: 2.8362 - lr = 0.00181538 \n",
            " 56/390 [===>..........................] - ETA: 1:19 - loss: 2.7626 - acc: 0.1535 - LRFinder: val_loss: 2.8303 - lr = 0.00183974 \n",
            " 57/390 [===>..........................] - ETA: 1:18 - loss: 2.7610 - acc: 0.1542 - LRFinder: val_loss: 2.8738 - lr = 0.00186410 \n",
            " 58/390 [===>..........................] - ETA: 1:17 - loss: 2.7591 - acc: 0.1554 - LRFinder: val_loss: 2.8734 - lr = 0.00188846 \n",
            " 59/390 [===>..........................] - ETA: 1:17 - loss: 2.7582 - acc: 0.1560 - LRFinder: val_loss: 2.8485 - lr = 0.00191282 \n",
            " 60/390 [===>..........................] - ETA: 1:16 - loss: 2.7571 - acc: 0.1562 - LRFinder: val_loss: 2.8663 - lr = 0.00193718 \n",
            " 61/390 [===>..........................] - ETA: 1:15 - loss: 2.7549 - acc: 0.1574 - LRFinder: val_loss: 2.8449 - lr = 0.00196154 \n",
            " 62/390 [===>..........................] - ETA: 1:14 - loss: 2.7536 - acc: 0.1581 - LRFinder: val_loss: 2.8610 - lr = 0.00198590 \n",
            " 63/390 [===>..........................] - ETA: 1:14 - loss: 2.7522 - acc: 0.1589 - LRFinder: val_loss: 2.8405 - lr = 0.00201026 \n",
            " 64/390 [===>..........................] - ETA: 1:13 - loss: 2.7504 - acc: 0.1597 - LRFinder: val_loss: 2.8822 - lr = 0.00203462 \n",
            " 65/390 [====>.........................] - ETA: 1:12 - loss: 2.7493 - acc: 0.1607 - LRFinder: val_loss: 2.8687 - lr = 0.00205897 \n",
            " 66/390 [====>.........................] - ETA: 1:12 - loss: 2.7486 - acc: 0.1603 - LRFinder: val_loss: 2.8412 - lr = 0.00208333 \n",
            " 67/390 [====>.........................] - ETA: 1:11 - loss: 2.7474 - acc: 0.1606 - LRFinder: val_loss: 2.8521 - lr = 0.00210769 \n",
            " 68/390 [====>.........................] - ETA: 1:11 - loss: 2.7462 - acc: 0.1615 - LRFinder: val_loss: 2.8368 - lr = 0.00213205 \n",
            " 69/390 [====>.........................] - ETA: 1:10 - loss: 2.7446 - acc: 0.1623 - LRFinder: val_loss: 2.8396 - lr = 0.00215641 \n",
            " 70/390 [====>.........................] - ETA: 1:09 - loss: 2.7429 - acc: 0.1634 - LRFinder: val_loss: 2.8687 - lr = 0.00218077 \n",
            " 71/390 [====>.........................] - ETA: 1:09 - loss: 2.7412 - acc: 0.1637 - LRFinder: val_loss: 2.8468 - lr = 0.00220513 \n",
            " 72/390 [====>.........................] - ETA: 1:08 - loss: 2.7393 - acc: 0.1651 - LRFinder: val_loss: 2.8370 - lr = 0.00222949 \n",
            " 73/390 [====>.........................] - ETA: 1:08 - loss: 2.7377 - acc: 0.1658 - LRFinder: val_loss: 2.8671 - lr = 0.00225385 \n",
            " 74/390 [====>.........................] - ETA: 1:07 - loss: 2.7359 - acc: 0.1663 - LRFinder: val_loss: 2.9020 - lr = 0.00227821 \n",
            " 75/390 [====>.........................] - ETA: 1:07 - loss: 2.7341 - acc: 0.1674 - LRFinder: val_loss: 2.8846 - lr = 0.00230256 \n",
            " 76/390 [====>.........................] - ETA: 1:06 - loss: 2.7330 - acc: 0.1679 - LRFinder: val_loss: 2.8855 - lr = 0.00232692 \n",
            " 77/390 [====>.........................] - ETA: 1:06 - loss: 2.7316 - acc: 0.1681 - LRFinder: val_loss: 2.8434 - lr = 0.00235128 \n",
            " 78/390 [=====>........................] - ETA: 1:05 - loss: 2.7301 - acc: 0.1684 - LRFinder: val_loss: 2.8454 - lr = 0.00237564 \n",
            " 79/390 [=====>........................] - ETA: 1:05 - loss: 2.7286 - acc: 0.1688 - LRFinder: val_loss: 2.8549 - lr = 0.00240000 \n",
            " 80/390 [=====>........................] - ETA: 1:04 - loss: 2.7273 - acc: 0.1698 - LRFinder: val_loss: 2.8463 - lr = 0.00242436 \n",
            " 81/390 [=====>........................] - ETA: 1:04 - loss: 2.7256 - acc: 0.1707 - LRFinder: val_loss: 2.8920 - lr = 0.00244872 \n",
            " 82/390 [=====>........................] - ETA: 1:03 - loss: 2.7237 - acc: 0.1712 - LRFinder: val_loss: 2.8660 - lr = 0.00247308 \n",
            " 83/390 [=====>........................] - ETA: 1:03 - loss: 2.7218 - acc: 0.1722 - LRFinder: val_loss: 2.8744 - lr = 0.00249744 \n",
            " 84/390 [=====>........................] - ETA: 1:02 - loss: 2.7205 - acc: 0.1724 - LRFinder: val_loss: 2.8496 - lr = 0.00252179 \n",
            " 85/390 [=====>........................] - ETA: 1:02 - loss: 2.7195 - acc: 0.1728 - LRFinder: val_loss: 2.8311 - lr = 0.00254615 \n",
            " 86/390 [=====>........................] - ETA: 1:01 - loss: 2.7186 - acc: 0.1733 - LRFinder: val_loss: 2.8592 - lr = 0.00257051 \n",
            " 87/390 [=====>........................] - ETA: 1:01 - loss: 2.7167 - acc: 0.1739 - LRFinder: val_loss: 2.8472 - lr = 0.00259487 \n",
            " 88/390 [=====>........................] - ETA: 1:00 - loss: 2.7150 - acc: 0.1746 - LRFinder: val_loss: 2.8679 - lr = 0.00261923 \n",
            " 89/390 [=====>........................] - ETA: 1:00 - loss: 2.7133 - acc: 0.1761 - LRFinder: val_loss: 2.8376 - lr = 0.00264359 \n",
            " 90/390 [=====>........................] - ETA: 1:00 - loss: 2.7111 - acc: 0.1779 - LRFinder: val_loss: 2.8533 - lr = 0.00266795 \n",
            " 91/390 [======>.......................] - ETA: 59s - loss: 2.7092 - acc: 0.1787  - LRFinder: val_loss: 2.8238 - lr = 0.00269231 \n",
            " 92/390 [======>.......................] - ETA: 59s - loss: 2.7077 - acc: 0.1793 - LRFinder: val_loss: 2.8681 - lr = 0.00271667 \n",
            " 93/390 [======>.......................] - ETA: 58s - loss: 2.7065 - acc: 0.1797 - LRFinder: val_loss: 2.8465 - lr = 0.00274103 \n",
            " 94/390 [======>.......................] - ETA: 58s - loss: 2.7052 - acc: 0.1804 - LRFinder: val_loss: 2.8617 - lr = 0.00276538 \n",
            " 95/390 [======>.......................] - ETA: 58s - loss: 2.7046 - acc: 0.1804 - LRFinder: val_loss: 2.8532 - lr = 0.00278974 \n",
            " 96/390 [======>.......................] - ETA: 57s - loss: 2.7034 - acc: 0.1808 - LRFinder: val_loss: 2.8846 - lr = 0.00281410 \n",
            " 97/390 [======>.......................] - ETA: 57s - loss: 2.7015 - acc: 0.1817 - LRFinder: val_loss: 2.8575 - lr = 0.00283846 \n",
            " 98/390 [======>.......................] - ETA: 57s - loss: 2.6996 - acc: 0.1822 - LRFinder: val_loss: 2.8775 - lr = 0.00286282 \n",
            " 99/390 [======>.......................] - ETA: 56s - loss: 2.6974 - acc: 0.1830 - LRFinder: val_loss: 2.8752 - lr = 0.00288718 \n",
            "100/390 [======>.......................] - ETA: 56s - loss: 2.6952 - acc: 0.1837 - LRFinder: val_loss: 2.8793 - lr = 0.00291154 \n",
            "101/390 [======>.......................] - ETA: 56s - loss: 2.6933 - acc: 0.1846 - LRFinder: val_loss: 2.8480 - lr = 0.00293590 \n",
            "102/390 [======>.......................] - ETA: 55s - loss: 2.6918 - acc: 0.1846 - LRFinder: val_loss: 2.8695 - lr = 0.00296026 \n",
            "103/390 [======>.......................] - ETA: 55s - loss: 2.6901 - acc: 0.1858 - LRFinder: val_loss: 2.9126 - lr = 0.00298462 \n",
            "104/390 [=======>......................] - ETA: 54s - loss: 2.6890 - acc: 0.1868 - LRFinder: val_loss: 2.8578 - lr = 0.00300897 \n",
            "105/390 [=======>......................] - ETA: 54s - loss: 2.6870 - acc: 0.1879 - LRFinder: val_loss: 2.9030 - lr = 0.00303333 \n",
            "106/390 [=======>......................] - ETA: 54s - loss: 2.6852 - acc: 0.1883 - LRFinder: val_loss: 2.8634 - lr = 0.00305769 \n",
            "107/390 [=======>......................] - ETA: 53s - loss: 2.6833 - acc: 0.1891 - LRFinder: val_loss: 2.8700 - lr = 0.00308205 \n",
            "108/390 [=======>......................] - ETA: 53s - loss: 2.6812 - acc: 0.1905 - LRFinder: val_loss: 2.8535 - lr = 0.00310641 \n",
            "109/390 [=======>......................] - ETA: 53s - loss: 2.6798 - acc: 0.1903 - LRFinder: val_loss: 2.9027 - lr = 0.00313077 \n",
            "110/390 [=======>......................] - ETA: 53s - loss: 2.6780 - acc: 0.1907 - LRFinder: val_loss: 2.8730 - lr = 0.00315513 \n",
            "111/390 [=======>......................] - ETA: 52s - loss: 2.6770 - acc: 0.1909 - LRFinder: val_loss: 2.9318 - lr = 0.00317949 \n",
            "112/390 [=======>......................] - ETA: 52s - loss: 2.6754 - acc: 0.1915 - LRFinder: val_loss: 2.8942 - lr = 0.00320385 \n",
            "113/390 [=======>......................] - ETA: 52s - loss: 2.6738 - acc: 0.1923 - LRFinder: val_loss: 2.8808 - lr = 0.00322821 \n",
            "114/390 [=======>......................] - ETA: 51s - loss: 2.6727 - acc: 0.1927 - LRFinder: val_loss: 2.8992 - lr = 0.00325256 \n",
            "115/390 [=======>......................] - ETA: 51s - loss: 2.6708 - acc: 0.1939 - LRFinder: val_loss: 2.9316 - lr = 0.00327692 \n",
            "116/390 [=======>......................] - ETA: 51s - loss: 2.6696 - acc: 0.1941 - LRFinder: val_loss: 2.9015 - lr = 0.00330128 \n",
            "117/390 [========>.....................] - ETA: 50s - loss: 2.6678 - acc: 0.1949 - LRFinder: val_loss: 2.8729 - lr = 0.00332564 \n",
            "118/390 [========>.....................] - ETA: 50s - loss: 2.6662 - acc: 0.1954 - LRFinder: val_loss: 2.9185 - lr = 0.00335000 \n",
            "119/390 [========>.....................] - ETA: 50s - loss: 2.6649 - acc: 0.1958 - LRFinder: val_loss: 2.8750 - lr = 0.00337436 \n",
            "120/390 [========>.....................] - ETA: 50s - loss: 2.6639 - acc: 0.1960 - LRFinder: val_loss: 2.9731 - lr = 0.00339872 \n",
            "121/390 [========>.....................] - ETA: 49s - loss: 2.6620 - acc: 0.1967 - LRFinder: val_loss: 2.9532 - lr = 0.00342308 \n",
            "122/390 [========>.....................] - ETA: 49s - loss: 2.6602 - acc: 0.1976 - LRFinder: val_loss: 2.9463 - lr = 0.00344744 \n",
            "123/390 [========>.....................] - ETA: 49s - loss: 2.6578 - acc: 0.1986 - LRFinder: val_loss: 2.9256 - lr = 0.00347179 \n",
            "124/390 [========>.....................] - ETA: 48s - loss: 2.6561 - acc: 0.1990 - LRFinder: val_loss: 2.9528 - lr = 0.00349615 \n",
            "125/390 [========>.....................] - ETA: 48s - loss: 2.6539 - acc: 0.1998 - LRFinder: val_loss: 2.8966 - lr = 0.00352051 \n",
            "126/390 [========>.....................] - ETA: 48s - loss: 2.6525 - acc: 0.2000 - LRFinder: val_loss: 2.9437 - lr = 0.00354487 \n",
            "127/390 [========>.....................] - ETA: 48s - loss: 2.6511 - acc: 0.1999 - LRFinder: val_loss: 2.9820 - lr = 0.00356923 \n",
            "128/390 [========>.....................] - ETA: 47s - loss: 2.6494 - acc: 0.2002 - LRFinder: val_loss: 2.9981 - lr = 0.00359359 \n",
            "129/390 [========>.....................] - ETA: 47s - loss: 2.6483 - acc: 0.2008 - LRFinder: val_loss: 2.9561 - lr = 0.00361795 \n",
            "130/390 [=========>....................] - ETA: 47s - loss: 2.6467 - acc: 0.2010 - LRFinder: val_loss: 2.9688 - lr = 0.00364231 \n",
            "131/390 [=========>....................] - ETA: 46s - loss: 2.6464 - acc: 0.2010 - LRFinder: val_loss: 2.9609 - lr = 0.00366667 \n",
            "132/390 [=========>....................] - ETA: 46s - loss: 2.6447 - acc: 0.2017 - LRFinder: val_loss: 2.9617 - lr = 0.00369103 \n",
            "133/390 [=========>....................] - ETA: 46s - loss: 2.6437 - acc: 0.2020 - LRFinder: val_loss: 3.0674 - lr = 0.00371538 \n",
            "134/390 [=========>....................] - ETA: 46s - loss: 2.6420 - acc: 0.2026 - LRFinder: val_loss: 2.9188 - lr = 0.00373974 \n",
            "135/390 [=========>....................] - ETA: 45s - loss: 2.6406 - acc: 0.2030 - LRFinder: val_loss: 3.0285 - lr = 0.00376410 \n",
            "136/390 [=========>....................] - ETA: 45s - loss: 2.6391 - acc: 0.2035 - LRFinder: val_loss: 3.0540 - lr = 0.00378846 \n",
            "137/390 [=========>....................] - ETA: 45s - loss: 2.6381 - acc: 0.2043 - LRFinder: val_loss: 3.0043 - lr = 0.00381282 \n",
            "138/390 [=========>....................] - ETA: 45s - loss: 2.6365 - acc: 0.2049 - LRFinder: val_loss: 3.0549 - lr = 0.00383718 \n",
            "139/390 [=========>....................] - ETA: 44s - loss: 2.6349 - acc: 0.2058 - LRFinder: val_loss: 3.0969 - lr = 0.00386154 \n",
            "140/390 [=========>....................] - ETA: 44s - loss: 2.6339 - acc: 0.2063 - LRFinder: val_loss: 3.0397 - lr = 0.00388590 \n",
            "141/390 [=========>....................] - ETA: 44s - loss: 2.6333 - acc: 0.2066 - LRFinder: val_loss: 3.0104 - lr = 0.00391026 \n",
            "142/390 [=========>....................] - ETA: 44s - loss: 2.6324 - acc: 0.2068 - LRFinder: val_loss: 3.0726 - lr = 0.00393462 \n",
            "143/390 [==========>...................] - ETA: 43s - loss: 2.6314 - acc: 0.2066 - LRFinder: val_loss: 3.0696 - lr = 0.00395897 \n",
            "144/390 [==========>...................] - ETA: 43s - loss: 2.6298 - acc: 0.2073 - LRFinder: val_loss: 3.0930 - lr = 0.00398333 \n",
            "145/390 [==========>...................] - ETA: 43s - loss: 2.6278 - acc: 0.2085 - LRFinder: val_loss: 3.0464 - lr = 0.00400769 \n",
            "146/390 [==========>...................] - ETA: 43s - loss: 2.6263 - acc: 0.2093 - LRFinder: val_loss: 3.0985 - lr = 0.00403205 \n",
            "147/390 [==========>...................] - ETA: 42s - loss: 2.6248 - acc: 0.2098 - LRFinder: val_loss: 3.0987 - lr = 0.00405641 \n",
            "148/390 [==========>...................] - ETA: 42s - loss: 2.6239 - acc: 0.2104 - LRFinder: val_loss: 3.1263 - lr = 0.00408077 \n",
            "149/390 [==========>...................] - ETA: 42s - loss: 2.6221 - acc: 0.2107 - LRFinder: val_loss: 3.0856 - lr = 0.00410513 \n",
            "150/390 [==========>...................] - ETA: 42s - loss: 2.6210 - acc: 0.2107 - LRFinder: val_loss: 3.0216 - lr = 0.00412949 \n",
            "151/390 [==========>...................] - ETA: 42s - loss: 2.6193 - acc: 0.2118 - LRFinder: val_loss: 3.0143 - lr = 0.00415385 \n",
            "152/390 [==========>...................] - ETA: 41s - loss: 2.6184 - acc: 0.2118 - LRFinder: val_loss: 3.0956 - lr = 0.00417821 \n",
            "153/390 [==========>...................] - ETA: 41s - loss: 2.6177 - acc: 0.2122 - LRFinder: val_loss: 3.1544 - lr = 0.00420256 \n",
            "154/390 [==========>...................] - ETA: 41s - loss: 2.6160 - acc: 0.2132 - LRFinder: val_loss: 3.1475 - lr = 0.00422692 \n",
            "155/390 [==========>...................] - ETA: 41s - loss: 2.6143 - acc: 0.2139 - LRFinder: val_loss: 3.1370 - lr = 0.00425128 \n",
            "156/390 [===========>..................] - ETA: 40s - loss: 2.6125 - acc: 0.2144 - LRFinder: val_loss: 3.2001 - lr = 0.00427564 \n",
            "157/390 [===========>..................] - ETA: 40s - loss: 2.6116 - acc: 0.2143 - LRFinder: val_loss: 3.0923 - lr = 0.00430000 \n",
            "158/390 [===========>..................] - ETA: 40s - loss: 2.6103 - acc: 0.2150 - LRFinder: val_loss: 3.2041 - lr = 0.00432436 \n",
            "159/390 [===========>..................] - ETA: 40s - loss: 2.6090 - acc: 0.2155 - LRFinder: val_loss: 3.2953 - lr = 0.00434872 \n",
            "160/390 [===========>..................] - ETA: 39s - loss: 2.6077 - acc: 0.2162 - LRFinder: val_loss: 3.1424 - lr = 0.00437308 \n",
            "161/390 [===========>..................] - ETA: 39s - loss: 2.6070 - acc: 0.2165 - LRFinder: val_loss: 3.1923 - lr = 0.00439744 \n",
            "162/390 [===========>..................] - ETA: 39s - loss: 2.6061 - acc: 0.2168 - LRFinder: val_loss: 3.2265 - lr = 0.00442179 \n",
            "163/390 [===========>..................] - ETA: 39s - loss: 2.6046 - acc: 0.2175 - LRFinder: val_loss: 3.2515 - lr = 0.00444615 \n",
            "164/390 [===========>..................] - ETA: 39s - loss: 2.6039 - acc: 0.2177 - LRFinder: val_loss: 3.2023 - lr = 0.00447051 \n",
            "165/390 [===========>..................] - ETA: 38s - loss: 2.6025 - acc: 0.2182 - LRFinder: val_loss: 3.2543 - lr = 0.00449487 \n",
            "166/390 [===========>..................] - ETA: 38s - loss: 2.6013 - acc: 0.2187 - LRFinder: val_loss: 3.2711 - lr = 0.00451923 \n",
            "167/390 [===========>..................] - ETA: 38s - loss: 2.6003 - acc: 0.2190 - LRFinder: val_loss: 3.2276 - lr = 0.00454359 \n",
            "168/390 [===========>..................] - ETA: 38s - loss: 2.5993 - acc: 0.2195 - LRFinder: val_loss: 3.1575 - lr = 0.00456795 \n",
            "169/390 [============>.................] - ETA: 38s - loss: 2.5980 - acc: 0.2200 - LRFinder: val_loss: 3.2528 - lr = 0.00459231 \n",
            "170/390 [============>.................] - ETA: 37s - loss: 2.5967 - acc: 0.2207 - LRFinder: val_loss: 3.2830 - lr = 0.00461667 \n",
            "171/390 [============>.................] - ETA: 37s - loss: 2.5948 - acc: 0.2218 - LRFinder: val_loss: 3.3138 - lr = 0.00464103 \n",
            "172/390 [============>.................] - ETA: 37s - loss: 2.5933 - acc: 0.2221 - LRFinder: val_loss: 3.1270 - lr = 0.00466538 \n",
            "173/390 [============>.................] - ETA: 37s - loss: 2.5924 - acc: 0.2220 - LRFinder: val_loss: 3.2540 - lr = 0.00468974 \n",
            "174/390 [============>.................] - ETA: 36s - loss: 2.5915 - acc: 0.2223 - LRFinder: val_loss: 3.2236 - lr = 0.00471410 \n",
            "175/390 [============>.................] - ETA: 36s - loss: 2.5902 - acc: 0.2224 - LRFinder: val_loss: 3.2062 - lr = 0.00473846 \n",
            "176/390 [============>.................] - ETA: 36s - loss: 2.5887 - acc: 0.2233 - LRFinder: val_loss: 3.2131 - lr = 0.00476282 \n",
            "177/390 [============>.................] - ETA: 36s - loss: 2.5880 - acc: 0.2236 - LRFinder: val_loss: 3.2762 - lr = 0.00478718 \n",
            "178/390 [============>.................] - ETA: 36s - loss: 2.5869 - acc: 0.2241 - LRFinder: val_loss: 3.2686 - lr = 0.00481154 \n",
            "179/390 [============>.................] - ETA: 35s - loss: 2.5854 - acc: 0.2248 - LRFinder: val_loss: 3.2946 - lr = 0.00483590 \n",
            "180/390 [============>.................] - ETA: 35s - loss: 2.5845 - acc: 0.2253 - LRFinder: val_loss: 3.3135 - lr = 0.00486026 \n",
            "181/390 [============>.................] - ETA: 35s - loss: 2.5837 - acc: 0.2256 - LRFinder: val_loss: 3.1883 - lr = 0.00488462 \n",
            "182/390 [=============>................] - ETA: 35s - loss: 2.5823 - acc: 0.2258 - LRFinder: val_loss: 3.3098 - lr = 0.00490897 \n",
            "183/390 [=============>................] - ETA: 35s - loss: 2.5813 - acc: 0.2260 - LRFinder: val_loss: 3.2809 - lr = 0.00493333 \n",
            "184/390 [=============>................] - ETA: 34s - loss: 2.5805 - acc: 0.2259 - LRFinder: val_loss: 3.3456 - lr = 0.00495769 \n",
            "185/390 [=============>................] - ETA: 34s - loss: 2.5797 - acc: 0.2263 - LRFinder: val_loss: 3.2237 - lr = 0.00498205 \n",
            "186/390 [=============>................] - ETA: 34s - loss: 2.5789 - acc: 0.2266 - LRFinder: val_loss: 3.2244 - lr = 0.00500641 \n",
            "187/390 [=============>................] - ETA: 34s - loss: 2.5783 - acc: 0.2266 - LRFinder: val_loss: 3.3318 - lr = 0.00503077 \n",
            "188/390 [=============>................] - ETA: 34s - loss: 2.5773 - acc: 0.2269 - LRFinder: val_loss: 3.1234 - lr = 0.00505513 \n",
            "189/390 [=============>................] - ETA: 33s - loss: 2.5762 - acc: 0.2274 - LRFinder: val_loss: 3.3829 - lr = 0.00507949 \n",
            "190/390 [=============>................] - ETA: 33s - loss: 2.5747 - acc: 0.2280 - LRFinder: val_loss: 3.2672 - lr = 0.00510385 \n",
            "191/390 [=============>................] - ETA: 33s - loss: 2.5738 - acc: 0.2284 - LRFinder: val_loss: 3.2894 - lr = 0.00512821 \n",
            "192/390 [=============>................] - ETA: 33s - loss: 2.5724 - acc: 0.2291 - LRFinder: val_loss: 3.3597 - lr = 0.00515256 \n",
            "193/390 [=============>................] - ETA: 33s - loss: 2.5710 - acc: 0.2295 - LRFinder: val_loss: 3.3343 - lr = 0.00517692 \n",
            "194/390 [=============>................] - ETA: 32s - loss: 2.5697 - acc: 0.2297 - LRFinder: val_loss: 3.4346 - lr = 0.00520128 \n",
            "195/390 [==============>...............] - ETA: 32s - loss: 2.5683 - acc: 0.2306 - LRFinder: val_loss: 3.4097 - lr = 0.00522564 \n",
            "196/390 [==============>...............] - ETA: 32s - loss: 2.5670 - acc: 0.2309 - LRFinder: val_loss: 3.3534 - lr = 0.00525000 \n",
            "197/390 [==============>...............] - ETA: 32s - loss: 2.5661 - acc: 0.2311 - LRFinder: val_loss: 3.3236 - lr = 0.00527436 \n",
            "198/390 [==============>...............] - ETA: 32s - loss: 2.5650 - acc: 0.2315 - LRFinder: val_loss: 3.4189 - lr = 0.00529872 \n",
            "199/390 [==============>...............] - ETA: 31s - loss: 2.5645 - acc: 0.2318 - LRFinder: val_loss: 3.2761 - lr = 0.00532308 \n",
            "200/390 [==============>...............] - ETA: 31s - loss: 2.5630 - acc: 0.2322 - LRFinder: val_loss: 3.3939 - lr = 0.00534744 \n",
            "201/390 [==============>...............] - ETA: 31s - loss: 2.5622 - acc: 0.2326 - LRFinder: val_loss: 3.2733 - lr = 0.00537179 \n",
            "202/390 [==============>...............] - ETA: 31s - loss: 2.5614 - acc: 0.2326 - LRFinder: val_loss: 3.4084 - lr = 0.00539615 \n",
            "203/390 [==============>...............] - ETA: 31s - loss: 2.5605 - acc: 0.2330 - LRFinder: val_loss: 3.4350 - lr = 0.00542051 \n",
            "204/390 [==============>...............] - ETA: 30s - loss: 2.5592 - acc: 0.2337 - LRFinder: val_loss: 3.4557 - lr = 0.00544487 \n",
            "205/390 [==============>...............] - ETA: 30s - loss: 2.5585 - acc: 0.2342 - LRFinder: val_loss: 3.5000 - lr = 0.00546923 \n",
            "206/390 [==============>...............] - ETA: 30s - loss: 2.5577 - acc: 0.2346 - LRFinder: val_loss: 3.5071 - lr = 0.00549359 \n",
            "207/390 [==============>...............] - ETA: 30s - loss: 2.5558 - acc: 0.2353 - LRFinder: val_loss: 3.4933 - lr = 0.00551795 \n",
            "208/390 [===============>..............] - ETA: 30s - loss: 2.5543 - acc: 0.2358 - LRFinder: val_loss: 3.4868 - lr = 0.00554231 \n",
            "209/390 [===============>..............] - ETA: 30s - loss: 2.5531 - acc: 0.2361 - LRFinder: val_loss: 3.3797 - lr = 0.00556667 \n",
            "210/390 [===============>..............] - ETA: 29s - loss: 2.5521 - acc: 0.2364 - LRFinder: val_loss: 3.4823 - lr = 0.00559103 \n",
            "211/390 [===============>..............] - ETA: 29s - loss: 2.5511 - acc: 0.2369 - LRFinder: val_loss: 3.5213 - lr = 0.00561538 \n",
            "212/390 [===============>..............] - ETA: 29s - loss: 2.5501 - acc: 0.2370 - LRFinder: val_loss: 3.4062 - lr = 0.00563974 \n",
            "213/390 [===============>..............] - ETA: 29s - loss: 2.5492 - acc: 0.2374 - LRFinder: val_loss: 3.5096 - lr = 0.00566410 \n",
            "214/390 [===============>..............] - ETA: 29s - loss: 2.5484 - acc: 0.2376 - LRFinder: val_loss: 3.3343 - lr = 0.00568846 \n",
            "215/390 [===============>..............] - ETA: 28s - loss: 2.5470 - acc: 0.2382 - LRFinder: val_loss: 3.4308 - lr = 0.00571282 \n",
            "216/390 [===============>..............] - ETA: 28s - loss: 2.5459 - acc: 0.2387 - LRFinder: val_loss: 3.4496 - lr = 0.00573718 \n",
            "217/390 [===============>..............] - ETA: 28s - loss: 2.5450 - acc: 0.2390 - LRFinder: val_loss: 3.4545 - lr = 0.00576154 \n",
            "218/390 [===============>..............] - ETA: 28s - loss: 2.5438 - acc: 0.2393 - LRFinder: val_loss: 3.4446 - lr = 0.00578590 \n",
            "219/390 [===============>..............] - ETA: 28s - loss: 2.5432 - acc: 0.2397 - LRFinder: val_loss: 3.4869 - lr = 0.00581026 \n",
            "220/390 [===============>..............] - ETA: 27s - loss: 2.5428 - acc: 0.2398 - LRFinder: val_loss: 3.5061 - lr = 0.00583462 \n",
            "221/390 [================>.............] - ETA: 27s - loss: 2.5420 - acc: 0.2399 - LRFinder: val_loss: 3.4866 - lr = 0.00585897 \n",
            "222/390 [================>.............] - ETA: 27s - loss: 2.5410 - acc: 0.2400 - LRFinder: val_loss: 3.4678 - lr = 0.00588333 \n",
            "223/390 [================>.............] - ETA: 27s - loss: 2.5396 - acc: 0.2408 - LRFinder: val_loss: 3.5203 - lr = 0.00590769 \n",
            "224/390 [================>.............] - ETA: 27s - loss: 2.5385 - acc: 0.2410 - LRFinder: val_loss: 3.5836 - lr = 0.00593205 \n",
            "225/390 [================>.............] - ETA: 27s - loss: 2.5372 - acc: 0.2415 - LRFinder: val_loss: 3.4376 - lr = 0.00595641 \n",
            "226/390 [================>.............] - ETA: 26s - loss: 2.5355 - acc: 0.2422 - LRFinder: val_loss: 3.4504 - lr = 0.00598077 \n",
            "227/390 [================>.............] - ETA: 26s - loss: 2.5349 - acc: 0.2422 - LRFinder: val_loss: 3.4485 - lr = 0.00600513 \n",
            "228/390 [================>.............] - ETA: 26s - loss: 2.5341 - acc: 0.2427 - LRFinder: val_loss: 3.4044 - lr = 0.00602949 \n",
            "229/390 [================>.............] - ETA: 26s - loss: 2.5326 - acc: 0.2433 - LRFinder: val_loss: 3.5412 - lr = 0.00605385 \n",
            "230/390 [================>.............] - ETA: 26s - loss: 2.5312 - acc: 0.2438 - LRFinder: val_loss: 3.4587 - lr = 0.00607821 \n",
            "231/390 [================>.............] - ETA: 26s - loss: 2.5295 - acc: 0.2446 - LRFinder: val_loss: 3.3492 - lr = 0.00610256 \n",
            "232/390 [================>.............] - ETA: 25s - loss: 2.5281 - acc: 0.2452 - LRFinder: val_loss: 3.4819 - lr = 0.00612692 \n",
            "233/390 [================>.............] - ETA: 25s - loss: 2.5272 - acc: 0.2455 - LRFinder: val_loss: 3.4301 - lr = 0.00615128 \n",
            "234/390 [=================>............] - ETA: 25s - loss: 2.5258 - acc: 0.2461 - LRFinder: val_loss: 3.3309 - lr = 0.00617564 \n",
            "235/390 [=================>............] - ETA: 25s - loss: 2.5246 - acc: 0.2466 - LRFinder: val_loss: 3.6121 - lr = 0.00620000 \n",
            "236/390 [=================>............] - ETA: 25s - loss: 2.5235 - acc: 0.2475 - LRFinder: val_loss: 3.4676 - lr = 0.00622436 \n",
            "237/390 [=================>............] - ETA: 24s - loss: 2.5224 - acc: 0.2479 - LRFinder: val_loss: 3.5141 - lr = 0.00624872 \n",
            "238/390 [=================>............] - ETA: 24s - loss: 2.5211 - acc: 0.2486 - LRFinder: val_loss: 3.4485 - lr = 0.00627308 \n",
            "239/390 [=================>............] - ETA: 24s - loss: 2.5205 - acc: 0.2486 - LRFinder: val_loss: 3.5374 - lr = 0.00629744 \n",
            "240/390 [=================>............] - ETA: 24s - loss: 2.5197 - acc: 0.2487 - LRFinder: val_loss: 3.5867 - lr = 0.00632179 \n",
            "241/390 [=================>............] - ETA: 24s - loss: 2.5184 - acc: 0.2492 - LRFinder: val_loss: 3.5296 - lr = 0.00634615 \n",
            "242/390 [=================>............] - ETA: 24s - loss: 2.5170 - acc: 0.2496 - LRFinder: val_loss: 3.6419 - lr = 0.00637051 \n",
            "243/390 [=================>............] - ETA: 23s - loss: 2.5158 - acc: 0.2501 - LRFinder: val_loss: 3.6750 - lr = 0.00639487 \n",
            "244/390 [=================>............] - ETA: 23s - loss: 2.5149 - acc: 0.2504 - LRFinder: val_loss: 3.5803 - lr = 0.00641923 \n",
            "245/390 [=================>............] - ETA: 23s - loss: 2.5139 - acc: 0.2505 - LRFinder: val_loss: 3.4862 - lr = 0.00644359 \n",
            "246/390 [=================>............] - ETA: 23s - loss: 2.5130 - acc: 0.2508 - LRFinder: val_loss: 3.5894 - lr = 0.00646795 \n",
            "247/390 [==================>...........] - ETA: 23s - loss: 2.5118 - acc: 0.2512 - LRFinder: val_loss: 3.5470 - lr = 0.00649231 \n",
            "248/390 [==================>...........] - ETA: 23s - loss: 2.5111 - acc: 0.2515 - LRFinder: val_loss: 3.5730 - lr = 0.00651667 \n",
            "249/390 [==================>...........] - ETA: 22s - loss: 2.5101 - acc: 0.2516 - LRFinder: val_loss: 3.6670 - lr = 0.00654103 \n",
            "250/390 [==================>...........] - ETA: 22s - loss: 2.5089 - acc: 0.2520 - LRFinder: val_loss: 3.5919 - lr = 0.00656538 \n",
            "251/390 [==================>...........] - ETA: 22s - loss: 2.5074 - acc: 0.2526 - LRFinder: val_loss: 3.5913 - lr = 0.00658974 \n",
            "252/390 [==================>...........] - ETA: 22s - loss: 2.5063 - acc: 0.2528 - LRFinder: val_loss: 3.5966 - lr = 0.00661410 \n",
            "253/390 [==================>...........] - ETA: 22s - loss: 2.5058 - acc: 0.2530 - LRFinder: val_loss: 3.6683 - lr = 0.00663846 \n",
            "254/390 [==================>...........] - ETA: 21s - loss: 2.5045 - acc: 0.2534 - LRFinder: val_loss: 3.5717 - lr = 0.00666282 \n",
            "255/390 [==================>...........] - ETA: 21s - loss: 2.5035 - acc: 0.2536 - LRFinder: val_loss: 3.5546 - lr = 0.00668718 \n",
            "256/390 [==================>...........] - ETA: 21s - loss: 2.5023 - acc: 0.2539 - LRFinder: val_loss: 3.5989 - lr = 0.00671154 \n",
            "257/390 [==================>...........] - ETA: 21s - loss: 2.5011 - acc: 0.2541 - LRFinder: val_loss: 3.7196 - lr = 0.00673590 \n",
            "258/390 [==================>...........] - ETA: 21s - loss: 2.5006 - acc: 0.2544 - LRFinder: val_loss: 3.7486 - lr = 0.00676026 \n",
            "259/390 [==================>...........] - ETA: 21s - loss: 2.4998 - acc: 0.2547 - LRFinder: val_loss: 3.8576 - lr = 0.00678462 \n",
            "260/390 [===================>..........] - ETA: 20s - loss: 2.4989 - acc: 0.2549 - LRFinder: val_loss: 3.7586 - lr = 0.00680897 \n",
            "261/390 [===================>..........] - ETA: 20s - loss: 2.4979 - acc: 0.2552 - LRFinder: val_loss: 3.8502 - lr = 0.00683333 \n",
            "262/390 [===================>..........] - ETA: 20s - loss: 2.4971 - acc: 0.2555 - LRFinder: val_loss: 3.7144 - lr = 0.00685769 \n",
            "263/390 [===================>..........] - ETA: 20s - loss: 2.4960 - acc: 0.2559 - LRFinder: val_loss: 3.8130 - lr = 0.00688205 \n",
            "264/390 [===================>..........] - ETA: 20s - loss: 2.4948 - acc: 0.2562 - LRFinder: val_loss: 3.7559 - lr = 0.00690641 \n",
            "265/390 [===================>..........] - ETA: 20s - loss: 2.4940 - acc: 0.2565 - LRFinder: val_loss: 3.8771 - lr = 0.00693077 \n",
            "266/390 [===================>..........] - ETA: 19s - loss: 2.4932 - acc: 0.2567 - LRFinder: val_loss: 3.8840 - lr = 0.00695513 \n",
            "267/390 [===================>..........] - ETA: 19s - loss: 2.4920 - acc: 0.2572 - LRFinder: val_loss: 3.7899 - lr = 0.00697949 \n",
            "268/390 [===================>..........] - ETA: 19s - loss: 2.4914 - acc: 0.2572 - LRFinder: val_loss: 3.8404 - lr = 0.00700385 \n",
            "269/390 [===================>..........] - ETA: 19s - loss: 2.4904 - acc: 0.2574 - LRFinder: val_loss: 3.7875 - lr = 0.00702821 \n",
            "270/390 [===================>..........] - ETA: 19s - loss: 2.4890 - acc: 0.2578 - LRFinder: val_loss: 3.9302 - lr = 0.00705256 \n",
            "271/390 [===================>..........] - ETA: 19s - loss: 2.4875 - acc: 0.2586 - LRFinder: val_loss: 3.7102 - lr = 0.00707692 \n",
            "272/390 [===================>..........] - ETA: 18s - loss: 2.4863 - acc: 0.2591 - LRFinder: val_loss: 3.8179 - lr = 0.00710128 \n",
            "273/390 [====================>.........] - ETA: 18s - loss: 2.4855 - acc: 0.2594 - LRFinder: val_loss: 3.7023 - lr = 0.00712564 \n",
            "274/390 [====================>.........] - ETA: 18s - loss: 2.4845 - acc: 0.2596 - LRFinder: val_loss: 3.8274 - lr = 0.00715000 \n",
            "275/390 [====================>.........] - ETA: 18s - loss: 2.4837 - acc: 0.2599 - LRFinder: val_loss: 3.8861 - lr = 0.00717436 \n",
            "276/390 [====================>.........] - ETA: 18s - loss: 2.4825 - acc: 0.2603 - LRFinder: val_loss: 3.9351 - lr = 0.00719872 \n",
            "277/390 [====================>.........] - ETA: 18s - loss: 2.4816 - acc: 0.2607 - LRFinder: val_loss: 3.8835 - lr = 0.00722308 \n",
            "278/390 [====================>.........] - ETA: 17s - loss: 2.4804 - acc: 0.2611 - LRFinder: val_loss: 3.9529 - lr = 0.00724744 \n",
            "279/390 [====================>.........] - ETA: 17s - loss: 2.4792 - acc: 0.2615 - LRFinder: val_loss: 3.8538 - lr = 0.00727179 \n",
            "280/390 [====================>.........] - ETA: 17s - loss: 2.4782 - acc: 0.2619 - LRFinder: val_loss: 3.7088 - lr = 0.00729615 \n",
            "281/390 [====================>.........] - ETA: 17s - loss: 2.4773 - acc: 0.2623 - LRFinder: val_loss: 3.7502 - lr = 0.00732051 \n",
            "282/390 [====================>.........] - ETA: 17s - loss: 2.4766 - acc: 0.2625 - LRFinder: val_loss: 3.7637 - lr = 0.00734487 \n",
            "283/390 [====================>.........] - ETA: 17s - loss: 2.4755 - acc: 0.2629 - LRFinder: val_loss: 3.8088 - lr = 0.00736923 \n",
            "284/390 [====================>.........] - ETA: 16s - loss: 2.4746 - acc: 0.2631 - LRFinder: val_loss: 3.7871 - lr = 0.00739359 \n",
            "285/390 [====================>.........] - ETA: 16s - loss: 2.4741 - acc: 0.2634 - LRFinder: val_loss: 3.8350 - lr = 0.00741795 \n",
            "286/390 [=====================>........] - ETA: 16s - loss: 2.4729 - acc: 0.2640 - LRFinder: val_loss: 3.7997 - lr = 0.00744231 \n",
            "287/390 [=====================>........] - ETA: 16s - loss: 2.4717 - acc: 0.2644 - LRFinder: val_loss: 3.7128 - lr = 0.00746667 \n",
            "288/390 [=====================>........] - ETA: 16s - loss: 2.4711 - acc: 0.2645 - LRFinder: val_loss: 3.5882 - lr = 0.00749103 \n",
            "289/390 [=====================>........] - ETA: 16s - loss: 2.4698 - acc: 0.2650 - LRFinder: val_loss: 3.7235 - lr = 0.00751538 \n",
            "290/390 [=====================>........] - ETA: 15s - loss: 2.4694 - acc: 0.2650 - LRFinder: val_loss: 3.7777 - lr = 0.00753974 \n",
            "291/390 [=====================>........] - ETA: 15s - loss: 2.4684 - acc: 0.2653 - LRFinder: val_loss: 3.8057 - lr = 0.00756410 \n",
            "292/390 [=====================>........] - ETA: 15s - loss: 2.4674 - acc: 0.2657 - LRFinder: val_loss: 3.7401 - lr = 0.00758846 \n",
            "293/390 [=====================>........] - ETA: 15s - loss: 2.4665 - acc: 0.2660 - LRFinder: val_loss: 3.9001 - lr = 0.00761282 \n",
            "294/390 [=====================>........] - ETA: 15s - loss: 2.4655 - acc: 0.2664 - LRFinder: val_loss: 3.7731 - lr = 0.00763718 \n",
            "295/390 [=====================>........] - ETA: 15s - loss: 2.4646 - acc: 0.2666 - LRFinder: val_loss: 3.9008 - lr = 0.00766154 \n",
            "296/390 [=====================>........] - ETA: 14s - loss: 2.4637 - acc: 0.2667 - LRFinder: val_loss: 4.0410 - lr = 0.00768590 \n",
            "297/390 [=====================>........] - ETA: 14s - loss: 2.4625 - acc: 0.2671 - LRFinder: val_loss: 3.7888 - lr = 0.00771026 \n",
            "298/390 [=====================>........] - ETA: 14s - loss: 2.4613 - acc: 0.2675 - LRFinder: val_loss: 3.8217 - lr = 0.00773462 \n",
            "299/390 [======================>.......] - ETA: 14s - loss: 2.4602 - acc: 0.2681 - LRFinder: val_loss: 3.7806 - lr = 0.00775897 \n",
            "300/390 [======================>.......] - ETA: 14s - loss: 2.4596 - acc: 0.2684 - LRFinder: val_loss: 3.7745 - lr = 0.00778333 \n",
            "301/390 [======================>.......] - ETA: 14s - loss: 2.4584 - acc: 0.2688 - LRFinder: val_loss: 3.8293 - lr = 0.00780769 \n",
            "302/390 [======================>.......] - ETA: 13s - loss: 2.4575 - acc: 0.2692 - LRFinder: val_loss: 3.7834 - lr = 0.00783205 \n",
            "303/390 [======================>.......] - ETA: 13s - loss: 2.4564 - acc: 0.2695 - LRFinder: val_loss: 3.8702 - lr = 0.00785641 \n",
            "304/390 [======================>.......] - ETA: 13s - loss: 2.4557 - acc: 0.2697 - LRFinder: val_loss: 3.9627 - lr = 0.00788077 \n",
            "305/390 [======================>.......] - ETA: 13s - loss: 2.4544 - acc: 0.2703 - LRFinder: val_loss: 4.0040 - lr = 0.00790513 \n",
            "306/390 [======================>.......] - ETA: 13s - loss: 2.4541 - acc: 0.2705 - LRFinder: val_loss: 3.7374 - lr = 0.00792949 \n",
            "307/390 [======================>.......] - ETA: 13s - loss: 2.4533 - acc: 0.2707 - LRFinder: val_loss: 3.9761 - lr = 0.00795385 \n",
            "308/390 [======================>.......] - ETA: 12s - loss: 2.4525 - acc: 0.2710 - LRFinder: val_loss: 3.8976 - lr = 0.00797821 \n",
            "309/390 [======================>.......] - ETA: 12s - loss: 2.4513 - acc: 0.2715 - LRFinder: val_loss: 3.9892 - lr = 0.00800256 \n",
            "310/390 [======================>.......] - ETA: 12s - loss: 2.4505 - acc: 0.2718 - LRFinder: val_loss: 3.8361 - lr = 0.00802692 \n",
            "311/390 [======================>.......] - ETA: 12s - loss: 2.4498 - acc: 0.2720 - LRFinder: val_loss: 3.8222 - lr = 0.00805128 \n",
            "312/390 [=======================>......] - ETA: 12s - loss: 2.4490 - acc: 0.2722 - LRFinder: val_loss: 3.8120 - lr = 0.00807564 \n",
            "313/390 [=======================>......] - ETA: 12s - loss: 2.4484 - acc: 0.2724 - LRFinder: val_loss: 3.9273 - lr = 0.00810000 \n",
            "314/390 [=======================>......] - ETA: 11s - loss: 2.4475 - acc: 0.2727 - LRFinder: val_loss: 3.9457 - lr = 0.00812436 \n",
            "315/390 [=======================>......] - ETA: 11s - loss: 2.4470 - acc: 0.2728 - LRFinder: val_loss: 3.7280 - lr = 0.00814872 \n",
            "316/390 [=======================>......] - ETA: 11s - loss: 2.4461 - acc: 0.2732 - LRFinder: val_loss: 3.9683 - lr = 0.00817308 \n",
            "317/390 [=======================>......] - ETA: 11s - loss: 2.4453 - acc: 0.2735 - LRFinder: val_loss: 3.8833 - lr = 0.00819744 \n",
            "318/390 [=======================>......] - ETA: 11s - loss: 2.4445 - acc: 0.2737 - LRFinder: val_loss: 4.1851 - lr = 0.00822179 \n",
            "319/390 [=======================>......] - ETA: 11s - loss: 2.4438 - acc: 0.2739 - LRFinder: val_loss: 3.9545 - lr = 0.00824615 \n",
            "320/390 [=======================>......] - ETA: 11s - loss: 2.4431 - acc: 0.2744 - LRFinder: val_loss: 4.1681 - lr = 0.00827051 \n",
            "321/390 [=======================>......] - ETA: 10s - loss: 2.4423 - acc: 0.2749 - LRFinder: val_loss: 3.9869 - lr = 0.00829487 \n",
            "322/390 [=======================>......] - ETA: 10s - loss: 2.4412 - acc: 0.2752 - LRFinder: val_loss: 3.9796 - lr = 0.00831923 \n",
            "323/390 [=======================>......] - ETA: 10s - loss: 2.4401 - acc: 0.2757 - LRFinder: val_loss: 3.9740 - lr = 0.00834359 \n",
            "324/390 [=======================>......] - ETA: 10s - loss: 2.4394 - acc: 0.2759 - LRFinder: val_loss: 4.0404 - lr = 0.00836795 \n",
            "325/390 [========================>.....] - ETA: 10s - loss: 2.4387 - acc: 0.2763 - LRFinder: val_loss: 4.1346 - lr = 0.00839231 \n",
            "326/390 [========================>.....] - ETA: 10s - loss: 2.4379 - acc: 0.2765 - LRFinder: val_loss: 3.9963 - lr = 0.00841667 \n",
            "327/390 [========================>.....] - ETA: 9s - loss: 2.4371 - acc: 0.2768  - LRFinder: val_loss: 4.0277 - lr = 0.00844103 \n",
            "328/390 [========================>.....] - ETA: 9s - loss: 2.4364 - acc: 0.2770 - LRFinder: val_loss: 4.0076 - lr = 0.00846538 \n",
            "329/390 [========================>.....] - ETA: 9s - loss: 2.4358 - acc: 0.2772 - LRFinder: val_loss: 4.2220 - lr = 0.00848974 \n",
            "330/390 [========================>.....] - ETA: 9s - loss: 2.4346 - acc: 0.2777 - LRFinder: val_loss: 3.9684 - lr = 0.00851410 \n",
            "331/390 [========================>.....] - ETA: 9s - loss: 2.4336 - acc: 0.2781 - LRFinder: val_loss: 3.9817 - lr = 0.00853846 \n",
            "332/390 [========================>.....] - ETA: 9s - loss: 2.4330 - acc: 0.2784 - LRFinder: val_loss: 3.9167 - lr = 0.00856282 \n",
            "333/390 [========================>.....] - ETA: 8s - loss: 2.4322 - acc: 0.2787 - LRFinder: val_loss: 3.8966 - lr = 0.00858718 \n",
            "334/390 [========================>.....] - ETA: 8s - loss: 2.4317 - acc: 0.2787 - LRFinder: val_loss: 3.8649 - lr = 0.00861154 \n",
            "335/390 [========================>.....] - ETA: 8s - loss: 2.4305 - acc: 0.2791 - LRFinder: val_loss: 3.8638 - lr = 0.00863590 \n",
            "336/390 [========================>.....] - ETA: 8s - loss: 2.4297 - acc: 0.2793 - LRFinder: val_loss: 3.9261 - lr = 0.00866026 \n",
            "337/390 [========================>.....] - ETA: 8s - loss: 2.4292 - acc: 0.2795 - LRFinder: val_loss: 4.0211 - lr = 0.00868462 \n",
            "338/390 [=========================>....] - ETA: 8s - loss: 2.4285 - acc: 0.2796 - LRFinder: val_loss: 3.9622 - lr = 0.00870897 \n",
            "339/390 [=========================>....] - ETA: 7s - loss: 2.4278 - acc: 0.2800 - LRFinder: val_loss: 3.8672 - lr = 0.00873333 \n",
            "340/390 [=========================>....] - ETA: 7s - loss: 2.4269 - acc: 0.2803 - LRFinder: val_loss: 3.8241 - lr = 0.00875769 \n",
            "341/390 [=========================>....] - ETA: 7s - loss: 2.4259 - acc: 0.2808 - LRFinder: val_loss: 3.8753 - lr = 0.00878205 \n",
            "342/390 [=========================>....] - ETA: 7s - loss: 2.4253 - acc: 0.2809 - LRFinder: val_loss: 4.0144 - lr = 0.00880641 \n",
            "343/390 [=========================>....] - ETA: 7s - loss: 2.4245 - acc: 0.2812 - LRFinder: val_loss: 3.7292 - lr = 0.00883077 \n",
            "344/390 [=========================>....] - ETA: 7s - loss: 2.4242 - acc: 0.2812 - LRFinder: val_loss: 4.0300 - lr = 0.00885513 \n",
            "345/390 [=========================>....] - ETA: 7s - loss: 2.4234 - acc: 0.2815 - LRFinder: val_loss: 4.0354 - lr = 0.00887949 \n",
            "346/390 [=========================>....] - ETA: 6s - loss: 2.4226 - acc: 0.2816 - LRFinder: val_loss: 3.9421 - lr = 0.00890385 \n",
            "347/390 [=========================>....] - ETA: 6s - loss: 2.4218 - acc: 0.2821 - LRFinder: val_loss: 3.9210 - lr = 0.00892821 \n",
            "348/390 [=========================>....] - ETA: 6s - loss: 2.4211 - acc: 0.2823 - LRFinder: val_loss: 3.8152 - lr = 0.00895256 \n",
            "349/390 [=========================>....] - ETA: 6s - loss: 2.4201 - acc: 0.2825 - LRFinder: val_loss: 3.8059 - lr = 0.00897692 \n",
            "350/390 [=========================>....] - ETA: 6s - loss: 2.4189 - acc: 0.2827 - LRFinder: val_loss: 3.8981 - lr = 0.00900128 \n",
            "351/390 [==========================>...] - ETA: 6s - loss: 2.4179 - acc: 0.2832 - LRFinder: val_loss: 3.9879 - lr = 0.00902564 \n",
            "352/390 [==========================>...] - ETA: 5s - loss: 2.4172 - acc: 0.2834 - LRFinder: val_loss: 3.7553 - lr = 0.00905000 \n",
            "353/390 [==========================>...] - ETA: 5s - loss: 2.4162 - acc: 0.2837 - LRFinder: val_loss: 3.8943 - lr = 0.00907436 \n",
            "354/390 [==========================>...] - ETA: 5s - loss: 2.4153 - acc: 0.2841 - LRFinder: val_loss: 3.7872 - lr = 0.00909872 \n",
            "355/390 [==========================>...] - ETA: 5s - loss: 2.4143 - acc: 0.2845 - LRFinder: val_loss: 3.9290 - lr = 0.00912308 \n",
            "356/390 [==========================>...] - ETA: 5s - loss: 2.4137 - acc: 0.2848 - LRFinder: val_loss: 3.9523 - lr = 0.00914744 \n",
            "357/390 [==========================>...] - ETA: 5s - loss: 2.4128 - acc: 0.2850 - LRFinder: val_loss: 3.8264 - lr = 0.00917179 \n",
            "358/390 [==========================>...] - ETA: 4s - loss: 2.4119 - acc: 0.2854 - LRFinder: val_loss: 3.8985 - lr = 0.00919615 \n",
            "359/390 [==========================>...] - ETA: 4s - loss: 2.4114 - acc: 0.2855 - LRFinder: val_loss: 3.7307 - lr = 0.00922051 \n",
            "360/390 [==========================>...] - ETA: 4s - loss: 2.4108 - acc: 0.2858 - LRFinder: val_loss: 3.6762 - lr = 0.00924487 \n",
            "361/390 [==========================>...] - ETA: 4s - loss: 2.4102 - acc: 0.2860 - LRFinder: val_loss: 3.5673 - lr = 0.00926923 \n",
            "362/390 [==========================>...] - ETA: 4s - loss: 2.4098 - acc: 0.2861 - LRFinder: val_loss: 3.5445 - lr = 0.00929359 \n",
            "363/390 [==========================>...] - ETA: 4s - loss: 2.4085 - acc: 0.2865 - LRFinder: val_loss: 3.5579 - lr = 0.00931795 \n",
            "364/390 [===========================>..] - ETA: 4s - loss: 2.4079 - acc: 0.2868 - LRFinder: val_loss: 3.5393 - lr = 0.00934231 \n",
            "365/390 [===========================>..] - ETA: 3s - loss: 2.4072 - acc: 0.2870 - LRFinder: val_loss: 3.7047 - lr = 0.00936667 \n",
            "366/390 [===========================>..] - ETA: 3s - loss: 2.4060 - acc: 0.2874 - LRFinder: val_loss: 3.5594 - lr = 0.00939103 \n",
            "367/390 [===========================>..] - ETA: 3s - loss: 2.4053 - acc: 0.2877 - LRFinder: val_loss: 3.8559 - lr = 0.00941538 \n",
            "368/390 [===========================>..] - ETA: 3s - loss: 2.4050 - acc: 0.2879 - LRFinder: val_loss: 3.6744 - lr = 0.00943974 \n",
            "369/390 [===========================>..] - ETA: 3s - loss: 2.4044 - acc: 0.2881 - LRFinder: val_loss: 3.7961 - lr = 0.00946410 \n",
            "370/390 [===========================>..] - ETA: 3s - loss: 2.4035 - acc: 0.2884 - LRFinder: val_loss: 3.4463 - lr = 0.00948846 \n",
            "371/390 [===========================>..] - ETA: 2s - loss: 2.4026 - acc: 0.2886 - LRFinder: val_loss: 3.7948 - lr = 0.00951282 \n",
            "372/390 [===========================>..] - ETA: 2s - loss: 2.4020 - acc: 0.2889 - LRFinder: val_loss: 3.6862 - lr = 0.00953718 \n",
            "373/390 [===========================>..] - ETA: 2s - loss: 2.4010 - acc: 0.2893 - LRFinder: val_loss: 3.7504 - lr = 0.00956154 \n",
            "374/390 [===========================>..] - ETA: 2s - loss: 2.4002 - acc: 0.2896 - LRFinder: val_loss: 3.6195 - lr = 0.00958590 \n",
            "375/390 [===========================>..] - ETA: 2s - loss: 2.3995 - acc: 0.2898 - LRFinder: val_loss: 3.4345 - lr = 0.00961026 \n",
            "376/390 [===========================>..] - ETA: 2s - loss: 2.3987 - acc: 0.2900 - LRFinder: val_loss: 3.5403 - lr = 0.00963462 \n",
            "377/390 [============================>.] - ETA: 2s - loss: 2.3981 - acc: 0.2901 - LRFinder: val_loss: 3.5771 - lr = 0.00965897 \n",
            "378/390 [============================>.] - ETA: 1s - loss: 2.3970 - acc: 0.2906 - LRFinder: val_loss: 3.5200 - lr = 0.00968333 \n",
            "379/390 [============================>.] - ETA: 1s - loss: 2.3960 - acc: 0.2908 - LRFinder: val_loss: 3.5520 - lr = 0.00970769 \n",
            "380/390 [============================>.] - ETA: 1s - loss: 2.3952 - acc: 0.2912 - LRFinder: val_loss: 3.3862 - lr = 0.00973205 \n",
            "381/390 [============================>.] - ETA: 1s - loss: 2.3944 - acc: 0.2914 - LRFinder: val_loss: 3.5917 - lr = 0.00975641 \n",
            "382/390 [============================>.] - ETA: 1s - loss: 2.3938 - acc: 0.2916 - LRFinder: val_loss: 3.7782 - lr = 0.00978077 \n",
            "383/390 [============================>.] - ETA: 1s - loss: 2.3930 - acc: 0.2918 - LRFinder: val_loss: 3.8409 - lr = 0.00980513 \n",
            "384/390 [============================>.] - ETA: 0s - loss: 2.3919 - acc: 0.2921 - LRFinder: val_loss: 3.7961 - lr = 0.00982949 \n",
            "385/390 [============================>.] - ETA: 0s - loss: 2.3915 - acc: 0.2923 - LRFinder: val_loss: 3.6989 - lr = 0.00985385 \n",
            "386/390 [============================>.] - ETA: 0s - loss: 2.3907 - acc: 0.2925 - LRFinder: val_loss: 3.8617 - lr = 0.00987821 \n",
            "387/390 [============================>.] - ETA: 0s - loss: 2.3899 - acc: 0.2929 - LRFinder: val_loss: 3.8913 - lr = 0.00990256 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 2.3891 - acc: 0.2932 - LRFinder: val_loss: 3.8596 - lr = 0.00992692 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 2.3885 - acc: 0.2935 - LRFinder: val_loss: 3.7537 - lr = 0.00995128 \n",
            " - LRFinder: val_loss: 3.9161 - lr = 0.00997564 \n",
            "390/390 [==============================] - 62s 159ms/step - loss: 2.3879 - acc: 0.2936 - val_loss: 3.7852 - val_acc: 0.1897\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {weights/}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAESCAYAAAAR2wXeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhRDChkjoCyGEDZHQF0IIG/L/Tzfn5PmmxAIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 136us/step\n",
            "loss : 3.7852\n",
            "acc : 0.1897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvdluHr_ibMV",
        "colab_type": "text"
      },
      "source": [
        "#BEST LR IS 0.0018"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_vRk5eWdW8a",
        "colab_type": "code",
        "outputId": "a857bf2b-5c2c-463b-cc5b-28942119fc20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "MOMENTUMS = [0.9, 0.95, 0.99]\n",
        "nb_epoch = 1\n",
        "\n",
        "for momentum in MOMENTUMS:\n",
        "#     # Learning rate range obtained from `find_lr_schedule.py`\n",
        "#     # NOTE : Minimum is 10x smaller than the max found above !\n",
        "#     # NOTE : It is preferable to use the validation data here to get a correct value\n",
        "      lr_finder = LRFinder(n_train, BATCH_SIZE, minimum_lr=0.0018, maximum_lr=1e-2,\n",
        "                          validation_data=(X_test, Y_test),\n",
        "                          validation_sample_rate=5,\n",
        "                          lr_scale='linear', save_dir='weights/momentum/momentum-%s/' % str(momentum),\n",
        "                          verbose=True)\n",
        "\n",
        "#     # set the weight_decay here !\n",
        "#     # lr doesnt matter as it will be over written by the callback\n",
        "      optimizer = SGD(lr=0.0018, momentum=momentum, nesterov=True)\n",
        "      model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "#         # Fit the model on the batches generated by datagen.flow().\n",
        "      model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE, shuffle=True),\n",
        "                             steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
        "                             validation_data=(X_test, Y_test),\n",
        "                             epochs=nb_epoch, verbose=1,\n",
        "                             callbacks=[lr_finder])\n",
        "\n",
        "# from plot we see, the model isnt impacted by the weight_decay very much at all\n",
        "# so we can use any of them.\n",
        "\n",
        "for momentum in MOMENTUMS:\n",
        "    directory = 'weights/momentum/momentum-%s/' % str(momentum)\n",
        "\n",
        "    losses, lrs = LRFinder.restore_schedule_from_dir(directory, 10, 5)\n",
        "    plt.plot(lrs, losses, label='momentum=%0.2f' % momentum)\n",
        "\n",
        "plt.title(\"Momentum\")\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 10:55 - loss: 2.1594 - acc: 0.4062 - LRFinder: val_loss: 3.6278 - lr = 0.00180000 \n",
            "  2/390 [..............................] - ETA: 7:12 - loss: 2.1287 - acc: 0.3906  - LRFinder: val_loss: 3.7114 - lr = 0.00182103 \n",
            "  3/390 [..............................] - ETA: 5:09 - loss: 2.1042 - acc: 0.4010 - LRFinder: val_loss: 3.8295 - lr = 0.00184205 \n",
            "  4/390 [..............................] - ETA: 4:05 - loss: 2.0879 - acc: 0.3965 - LRFinder: val_loss: 3.7566 - lr = 0.00186308 \n",
            "  5/390 [..............................] - ETA: 3:26 - loss: 2.0960 - acc: 0.3891 - LRFinder: val_loss: 3.6994 - lr = 0.00188410 \n",
            "  6/390 [..............................] - ETA: 3:00 - loss: 2.0909 - acc: 0.3958 - LRFinder: val_loss: 3.8270 - lr = 0.00190513 \n",
            "  7/390 [..............................] - ETA: 2:42 - loss: 2.0904 - acc: 0.4007 - LRFinder: val_loss: 3.6768 - lr = 0.00192615 \n",
            "  8/390 [..............................] - ETA: 2:28 - loss: 2.0912 - acc: 0.3994 - LRFinder: val_loss: 3.7561 - lr = 0.00194718 \n",
            "  9/390 [..............................] - ETA: 2:17 - loss: 2.0948 - acc: 0.4010 - LRFinder: val_loss: 3.7202 - lr = 0.00196821 \n",
            " 10/390 [..............................] - ETA: 2:09 - loss: 2.0950 - acc: 0.3984 - LRFinder: val_loss: 3.7839 - lr = 0.00198923 \n",
            " 11/390 [..............................] - ETA: 2:02 - loss: 2.0944 - acc: 0.3949 - LRFinder: val_loss: 3.6793 - lr = 0.00201026 \n",
            " 12/390 [..............................] - ETA: 1:56 - loss: 2.0904 - acc: 0.3978 - LRFinder: val_loss: 3.8173 - lr = 0.00203128 \n",
            " 13/390 [>.............................] - ETA: 1:50 - loss: 2.0957 - acc: 0.3960 - LRFinder: val_loss: 3.8759 - lr = 0.00205231 \n",
            " 14/390 [>.............................] - ETA: 1:46 - loss: 2.0948 - acc: 0.3929 - LRFinder: val_loss: 3.8642 - lr = 0.00207333 \n",
            " 15/390 [>.............................] - ETA: 1:42 - loss: 2.0976 - acc: 0.3937 - LRFinder: val_loss: 3.9564 - lr = 0.00209436 \n",
            " 16/390 [>.............................] - ETA: 1:39 - loss: 2.1001 - acc: 0.3931 - LRFinder: val_loss: 3.7517 - lr = 0.00211538 \n",
            " 17/390 [>.............................] - ETA: 1:36 - loss: 2.0976 - acc: 0.3934 - LRFinder: val_loss: 4.0032 - lr = 0.00213641 \n",
            " 18/390 [>.............................] - ETA: 1:33 - loss: 2.1010 - acc: 0.3924 - LRFinder: val_loss: 3.9587 - lr = 0.00215744 \n",
            " 19/390 [>.............................] - ETA: 1:31 - loss: 2.1044 - acc: 0.3923 - LRFinder: val_loss: 3.9112 - lr = 0.00217846 \n",
            " 20/390 [>.............................] - ETA: 1:29 - loss: 2.1016 - acc: 0.3930 - LRFinder: val_loss: 3.9956 - lr = 0.00219949 \n",
            " 21/390 [>.............................] - ETA: 1:27 - loss: 2.1048 - acc: 0.3899 - LRFinder: val_loss: 3.7931 - lr = 0.00222051 \n",
            " 22/390 [>.............................] - ETA: 1:25 - loss: 2.1027 - acc: 0.3899 - LRFinder: val_loss: 3.9017 - lr = 0.00224154 \n",
            " 23/390 [>.............................] - ETA: 1:23 - loss: 2.1042 - acc: 0.3882 - LRFinder: val_loss: 3.7058 - lr = 0.00226256 \n",
            " 24/390 [>.............................] - ETA: 1:22 - loss: 2.1010 - acc: 0.3910 - LRFinder: val_loss: 3.7837 - lr = 0.00228359 \n",
            " 25/390 [>.............................] - ETA: 1:20 - loss: 2.0996 - acc: 0.3925 - LRFinder: val_loss: 3.7861 - lr = 0.00230462 \n",
            " 26/390 [=>............................] - ETA: 1:19 - loss: 2.1069 - acc: 0.3912 - LRFinder: val_loss: 3.8053 - lr = 0.00232564 \n",
            " 27/390 [=>............................] - ETA: 1:18 - loss: 2.1080 - acc: 0.3903 - LRFinder: val_loss: 3.9736 - lr = 0.00234667 \n",
            " 28/390 [=>............................] - ETA: 1:17 - loss: 2.1050 - acc: 0.3906 - LRFinder: val_loss: 3.7338 - lr = 0.00236769 \n",
            " 29/390 [=>............................] - ETA: 1:16 - loss: 2.1017 - acc: 0.3917 - LRFinder: val_loss: 3.8209 - lr = 0.00238872 \n",
            " 30/390 [=>............................] - ETA: 1:15 - loss: 2.0956 - acc: 0.3948 - LRFinder: val_loss: 3.7185 - lr = 0.00240974 \n",
            " 31/390 [=>............................] - ETA: 1:14 - loss: 2.0978 - acc: 0.3949 - LRFinder: val_loss: 3.7240 - lr = 0.00243077 \n",
            " 32/390 [=>............................] - ETA: 1:13 - loss: 2.1003 - acc: 0.3943 - LRFinder: val_loss: 3.8377 - lr = 0.00245179 \n",
            " 33/390 [=>............................] - ETA: 1:12 - loss: 2.0985 - acc: 0.3949 - LRFinder: val_loss: 3.6698 - lr = 0.00247282 \n",
            " 34/390 [=>............................] - ETA: 1:11 - loss: 2.0998 - acc: 0.3957 - LRFinder: val_loss: 3.7808 - lr = 0.00249385 \n",
            " 35/390 [=>............................] - ETA: 1:10 - loss: 2.0979 - acc: 0.3978 - LRFinder: val_loss: 3.7651 - lr = 0.00251487 \n",
            " 36/390 [=>............................] - ETA: 1:10 - loss: 2.0950 - acc: 0.4006 - LRFinder: val_loss: 3.6741 - lr = 0.00253590 \n",
            " 37/390 [=>............................] - ETA: 1:09 - loss: 2.0923 - acc: 0.4020 - LRFinder: val_loss: 3.7102 - lr = 0.00255692 \n",
            " 38/390 [=>............................] - ETA: 1:08 - loss: 2.0930 - acc: 0.4005 - LRFinder: val_loss: 3.7345 - lr = 0.00257795 \n",
            " 39/390 [==>...........................] - ETA: 1:07 - loss: 2.0934 - acc: 0.4006 - LRFinder: val_loss: 3.6217 - lr = 0.00259897 \n",
            " 40/390 [==>...........................] - ETA: 1:07 - loss: 2.0903 - acc: 0.4012 - LRFinder: val_loss: 3.7817 - lr = 0.00262000 \n",
            " 41/390 [==>...........................] - ETA: 1:06 - loss: 2.0897 - acc: 0.4022 - LRFinder: val_loss: 3.8724 - lr = 0.00264103 \n",
            " 42/390 [==>...........................] - ETA: 1:06 - loss: 2.0895 - acc: 0.4018 - LRFinder: val_loss: 4.0618 - lr = 0.00266205 \n",
            " 43/390 [==>...........................] - ETA: 1:05 - loss: 2.0902 - acc: 0.4019 - LRFinder: val_loss: 4.0067 - lr = 0.00268308 \n",
            " 44/390 [==>...........................] - ETA: 1:05 - loss: 2.0903 - acc: 0.4022 - LRFinder: val_loss: 3.9500 - lr = 0.00270410 \n",
            " 45/390 [==>...........................] - ETA: 1:04 - loss: 2.0886 - acc: 0.4028 - LRFinder: val_loss: 3.8089 - lr = 0.00272513 \n",
            " 46/390 [==>...........................] - ETA: 1:03 - loss: 2.0866 - acc: 0.4040 - LRFinder: val_loss: 3.7793 - lr = 0.00274615 \n",
            " 47/390 [==>...........................] - ETA: 1:03 - loss: 2.0881 - acc: 0.4033 - LRFinder: val_loss: 3.7602 - lr = 0.00276718 \n",
            " 48/390 [==>...........................] - ETA: 1:02 - loss: 2.0892 - acc: 0.4030 - LRFinder: val_loss: 3.8023 - lr = 0.00278821 \n",
            " 49/390 [==>...........................] - ETA: 1:02 - loss: 2.0904 - acc: 0.4023 - LRFinder: val_loss: 3.7372 - lr = 0.00280923 \n",
            " 50/390 [==>...........................] - ETA: 1:02 - loss: 2.0890 - acc: 0.4028 - LRFinder: val_loss: 3.8751 - lr = 0.00283026 \n",
            " 51/390 [==>...........................] - ETA: 1:01 - loss: 2.0917 - acc: 0.4026 - LRFinder: val_loss: 3.7466 - lr = 0.00285128 \n",
            " 52/390 [===>..........................] - ETA: 1:01 - loss: 2.0916 - acc: 0.4020 - LRFinder: val_loss: 3.9497 - lr = 0.00287231 \n",
            " 53/390 [===>..........................] - ETA: 1:00 - loss: 2.0929 - acc: 0.4006 - LRFinder: val_loss: 3.7760 - lr = 0.00289333 \n",
            " 54/390 [===>..........................] - ETA: 1:00 - loss: 2.0914 - acc: 0.4019 - LRFinder: val_loss: 3.7465 - lr = 0.00291436 \n",
            " 55/390 [===>..........................] - ETA: 59s - loss: 2.0893 - acc: 0.4031  - LRFinder: val_loss: 3.6688 - lr = 0.00293538 \n",
            " 56/390 [===>..........................] - ETA: 59s - loss: 2.0894 - acc: 0.4023 - LRFinder: val_loss: 3.6511 - lr = 0.00295641 \n",
            " 57/390 [===>..........................] - ETA: 59s - loss: 2.0903 - acc: 0.4025 - LRFinder: val_loss: 3.8215 - lr = 0.00297744 \n",
            " 58/390 [===>..........................] - ETA: 58s - loss: 2.0893 - acc: 0.4026 - LRFinder: val_loss: 3.6812 - lr = 0.00299846 \n",
            " 59/390 [===>..........................] - ETA: 58s - loss: 2.0888 - acc: 0.4029 - LRFinder: val_loss: 3.7355 - lr = 0.00301949 \n",
            " 60/390 [===>..........................] - ETA: 58s - loss: 2.0889 - acc: 0.4023 - LRFinder: val_loss: 3.6924 - lr = 0.00304051 \n",
            " 61/390 [===>..........................] - ETA: 57s - loss: 2.0887 - acc: 0.4013 - LRFinder: val_loss: 3.7395 - lr = 0.00306154 \n",
            " 62/390 [===>..........................] - ETA: 57s - loss: 2.0894 - acc: 0.4010 - LRFinder: val_loss: 3.7954 - lr = 0.00308256 \n",
            " 63/390 [===>..........................] - ETA: 57s - loss: 2.0898 - acc: 0.4014 - LRFinder: val_loss: 3.6799 - lr = 0.00310359 \n",
            " 64/390 [===>..........................] - ETA: 56s - loss: 2.0902 - acc: 0.4019 - LRFinder: val_loss: 3.7069 - lr = 0.00312462 \n",
            " 65/390 [====>.........................] - ETA: 56s - loss: 2.0888 - acc: 0.4028 - LRFinder: val_loss: 3.7737 - lr = 0.00314564 \n",
            " 66/390 [====>.........................] - ETA: 56s - loss: 2.0897 - acc: 0.4029 - LRFinder: val_loss: 3.8798 - lr = 0.00316667 \n",
            " 67/390 [====>.........................] - ETA: 55s - loss: 2.0896 - acc: 0.4044 - LRFinder: val_loss: 3.7293 - lr = 0.00318769 \n",
            " 68/390 [====>.........................] - ETA: 55s - loss: 2.0905 - acc: 0.4040 - LRFinder: val_loss: 3.7651 - lr = 0.00320872 \n",
            " 69/390 [====>.........................] - ETA: 55s - loss: 2.0879 - acc: 0.4049 - LRFinder: val_loss: 3.8517 - lr = 0.00322974 \n",
            " 70/390 [====>.........................] - ETA: 54s - loss: 2.0874 - acc: 0.4050 - LRFinder: val_loss: 3.9606 - lr = 0.00325077 \n",
            " 71/390 [====>.........................] - ETA: 54s - loss: 2.0885 - acc: 0.4046 - LRFinder: val_loss: 3.8813 - lr = 0.00327179 \n",
            " 72/390 [====>.........................] - ETA: 54s - loss: 2.0883 - acc: 0.4045 - LRFinder: val_loss: 3.9321 - lr = 0.00329282 \n",
            " 73/390 [====>.........................] - ETA: 53s - loss: 2.0872 - acc: 0.4054 - LRFinder: val_loss: 3.8251 - lr = 0.00331385 \n",
            " 74/390 [====>.........................] - ETA: 53s - loss: 2.0856 - acc: 0.4058 - LRFinder: val_loss: 3.9397 - lr = 0.00333487 \n",
            " 75/390 [====>.........................] - ETA: 53s - loss: 2.0845 - acc: 0.4067 - LRFinder: val_loss: 3.8372 - lr = 0.00335590 \n",
            " 76/390 [====>.........................] - ETA: 53s - loss: 2.0845 - acc: 0.4059 - LRFinder: val_loss: 3.7775 - lr = 0.00337692 \n",
            " 77/390 [====>.........................] - ETA: 52s - loss: 2.0841 - acc: 0.4059 - LRFinder: val_loss: 3.7959 - lr = 0.00339795 \n",
            " 78/390 [=====>........................] - ETA: 52s - loss: 2.0838 - acc: 0.4060 - LRFinder: val_loss: 3.7281 - lr = 0.00341897 \n",
            " 79/390 [=====>........................] - ETA: 52s - loss: 2.0838 - acc: 0.4060 - LRFinder: val_loss: 3.8570 - lr = 0.00344000 \n",
            " 80/390 [=====>........................] - ETA: 52s - loss: 2.0832 - acc: 0.4065 - LRFinder: val_loss: 3.8197 - lr = 0.00346103 \n",
            " 81/390 [=====>........................] - ETA: 51s - loss: 2.0823 - acc: 0.4069 - LRFinder: val_loss: 3.9329 - lr = 0.00348205 \n",
            " 82/390 [=====>........................] - ETA: 51s - loss: 2.0807 - acc: 0.4075 - LRFinder: val_loss: 3.6592 - lr = 0.00350308 \n",
            " 83/390 [=====>........................] - ETA: 51s - loss: 2.0798 - acc: 0.4084 - LRFinder: val_loss: 3.7934 - lr = 0.00352410 \n",
            " 84/390 [=====>........................] - ETA: 51s - loss: 2.0808 - acc: 0.4088 - LRFinder: val_loss: 3.7501 - lr = 0.00354513 \n",
            " 85/390 [=====>........................] - ETA: 50s - loss: 2.0803 - acc: 0.4090 - LRFinder: val_loss: 3.6788 - lr = 0.00356615 \n",
            " 86/390 [=====>........................] - ETA: 50s - loss: 2.0789 - acc: 0.4093 - LRFinder: val_loss: 3.8188 - lr = 0.00358718 \n",
            " 87/390 [=====>........................] - ETA: 50s - loss: 2.0786 - acc: 0.4089 - LRFinder: val_loss: 3.8187 - lr = 0.00360821 \n",
            " 88/390 [=====>........................] - ETA: 50s - loss: 2.0778 - acc: 0.4090 - LRFinder: val_loss: 3.6927 - lr = 0.00362923 \n",
            " 89/390 [=====>........................] - ETA: 49s - loss: 2.0772 - acc: 0.4089 - LRFinder: val_loss: 3.8270 - lr = 0.00365026 \n",
            " 90/390 [=====>........................] - ETA: 49s - loss: 2.0749 - acc: 0.4102 - LRFinder: val_loss: 3.9193 - lr = 0.00367128 \n",
            " 91/390 [======>.......................] - ETA: 49s - loss: 2.0721 - acc: 0.4118 - LRFinder: val_loss: 4.0174 - lr = 0.00369231 \n",
            " 92/390 [======>.......................] - ETA: 49s - loss: 2.0720 - acc: 0.4121 - LRFinder: val_loss: 3.9265 - lr = 0.00371333 \n",
            " 93/390 [======>.......................] - ETA: 48s - loss: 2.0710 - acc: 0.4126 - LRFinder: val_loss: 3.9902 - lr = 0.00373436 \n",
            " 94/390 [======>.......................] - ETA: 48s - loss: 2.0705 - acc: 0.4126 - LRFinder: val_loss: 3.9942 - lr = 0.00375538 \n",
            " 95/390 [======>.......................] - ETA: 48s - loss: 2.0709 - acc: 0.4125 - LRFinder: val_loss: 3.7762 - lr = 0.00377641 \n",
            " 96/390 [======>.......................] - ETA: 48s - loss: 2.0709 - acc: 0.4125 - LRFinder: val_loss: 3.8677 - lr = 0.00379744 \n",
            " 97/390 [======>.......................] - ETA: 48s - loss: 2.0702 - acc: 0.4128 - LRFinder: val_loss: 3.8936 - lr = 0.00381846 \n",
            " 98/390 [======>.......................] - ETA: 47s - loss: 2.0703 - acc: 0.4132 - LRFinder: val_loss: 3.9283 - lr = 0.00383949 \n",
            " 99/390 [======>.......................] - ETA: 47s - loss: 2.0710 - acc: 0.4130 - LRFinder: val_loss: 3.9324 - lr = 0.00386051 \n",
            "100/390 [======>.......................] - ETA: 47s - loss: 2.0695 - acc: 0.4137 - LRFinder: val_loss: 3.9372 - lr = 0.00388154 \n",
            "101/390 [======>.......................] - ETA: 47s - loss: 2.0679 - acc: 0.4148 - LRFinder: val_loss: 3.9565 - lr = 0.00390256 \n",
            "102/390 [======>.......................] - ETA: 46s - loss: 2.0677 - acc: 0.4145 - LRFinder: val_loss: 3.8830 - lr = 0.00392359 \n",
            "103/390 [======>.......................] - ETA: 46s - loss: 2.0670 - acc: 0.4149 - LRFinder: val_loss: 3.8912 - lr = 0.00394462 \n",
            "104/390 [=======>......................] - ETA: 46s - loss: 2.0670 - acc: 0.4144 - LRFinder: val_loss: 3.8438 - lr = 0.00396564 \n",
            "105/390 [=======>......................] - ETA: 46s - loss: 2.0662 - acc: 0.4151 - LRFinder: val_loss: 3.9007 - lr = 0.00398667 \n",
            "106/390 [=======>......................] - ETA: 46s - loss: 2.0662 - acc: 0.4149 - LRFinder: val_loss: 4.0684 - lr = 0.00400769 \n",
            "107/390 [=======>......................] - ETA: 45s - loss: 2.0664 - acc: 0.4149 - LRFinder: val_loss: 3.8704 - lr = 0.00402872 \n",
            "108/390 [=======>......................] - ETA: 45s - loss: 2.0675 - acc: 0.4145 - LRFinder: val_loss: 3.8649 - lr = 0.00404974 \n",
            "109/390 [=======>......................] - ETA: 45s - loss: 2.0672 - acc: 0.4148 - LRFinder: val_loss: 3.8285 - lr = 0.00407077 \n",
            "110/390 [=======>......................] - ETA: 45s - loss: 2.0676 - acc: 0.4150 - LRFinder: val_loss: 4.0169 - lr = 0.00409179 \n",
            "111/390 [=======>......................] - ETA: 45s - loss: 2.0665 - acc: 0.4158 - LRFinder: val_loss: 4.0526 - lr = 0.00411282 \n",
            "112/390 [=======>......................] - ETA: 44s - loss: 2.0668 - acc: 0.4160 - LRFinder: val_loss: 3.8640 - lr = 0.00413385 \n",
            "113/390 [=======>......................] - ETA: 44s - loss: 2.0674 - acc: 0.4159 - LRFinder: val_loss: 4.0111 - lr = 0.00415487 \n",
            "114/390 [=======>......................] - ETA: 44s - loss: 2.0682 - acc: 0.4156 - LRFinder: val_loss: 3.7454 - lr = 0.00417590 \n",
            "115/390 [=======>......................] - ETA: 44s - loss: 2.0673 - acc: 0.4158 - LRFinder: val_loss: 3.7250 - lr = 0.00419692 \n",
            "116/390 [=======>......................] - ETA: 44s - loss: 2.0673 - acc: 0.4157 - LRFinder: val_loss: 3.8908 - lr = 0.00421795 \n",
            "117/390 [========>.....................] - ETA: 43s - loss: 2.0686 - acc: 0.4153 - LRFinder: val_loss: 3.8909 - lr = 0.00423897 \n",
            "118/390 [========>.....................] - ETA: 43s - loss: 2.0681 - acc: 0.4152 - LRFinder: val_loss: 3.8472 - lr = 0.00426000 \n",
            "119/390 [========>.....................] - ETA: 43s - loss: 2.0678 - acc: 0.4152 - LRFinder: val_loss: 3.7643 - lr = 0.00428103 \n",
            "120/390 [========>.....................] - ETA: 43s - loss: 2.0677 - acc: 0.4152 - LRFinder: val_loss: 3.9628 - lr = 0.00430205 \n",
            "121/390 [========>.....................] - ETA: 43s - loss: 2.0668 - acc: 0.4157 - LRFinder: val_loss: 3.7490 - lr = 0.00432308 \n",
            "122/390 [========>.....................] - ETA: 42s - loss: 2.0656 - acc: 0.4163 - LRFinder: val_loss: 3.9622 - lr = 0.00434410 \n",
            "123/390 [========>.....................] - ETA: 42s - loss: 2.0649 - acc: 0.4169 - LRFinder: val_loss: 3.8777 - lr = 0.00436513 \n",
            "124/390 [========>.....................] - ETA: 42s - loss: 2.0632 - acc: 0.4173 - LRFinder: val_loss: 3.7961 - lr = 0.00438615 \n",
            "125/390 [========>.....................] - ETA: 42s - loss: 2.0618 - acc: 0.4183 - LRFinder: val_loss: 3.8966 - lr = 0.00440718 \n",
            "126/390 [========>.....................] - ETA: 42s - loss: 2.0615 - acc: 0.4182 - LRFinder: val_loss: 3.9483 - lr = 0.00442821 \n",
            "127/390 [========>.....................] - ETA: 41s - loss: 2.0610 - acc: 0.4184 - LRFinder: val_loss: 3.7385 - lr = 0.00444923 \n",
            "128/390 [========>.....................] - ETA: 41s - loss: 2.0609 - acc: 0.4184 - LRFinder: val_loss: 3.6681 - lr = 0.00447026 \n",
            "129/390 [========>.....................] - ETA: 41s - loss: 2.0609 - acc: 0.4185 - LRFinder: val_loss: 3.7831 - lr = 0.00449128 \n",
            "130/390 [=========>....................] - ETA: 41s - loss: 2.0611 - acc: 0.4188 - LRFinder: val_loss: 3.7461 - lr = 0.00451231 \n",
            "131/390 [=========>....................] - ETA: 41s - loss: 2.0604 - acc: 0.4189 - LRFinder: val_loss: 3.9204 - lr = 0.00453333 \n",
            "132/390 [=========>....................] - ETA: 41s - loss: 2.0604 - acc: 0.4189 - LRFinder: val_loss: 3.9257 - lr = 0.00455436 \n",
            "133/390 [=========>....................] - ETA: 40s - loss: 2.0609 - acc: 0.4186 - LRFinder: val_loss: 3.6350 - lr = 0.00457538 \n",
            "134/390 [=========>....................] - ETA: 40s - loss: 2.0612 - acc: 0.4185 - LRFinder: val_loss: 3.7732 - lr = 0.00459641 \n",
            "135/390 [=========>....................] - ETA: 40s - loss: 2.0618 - acc: 0.4183 - LRFinder: val_loss: 3.9609 - lr = 0.00461744 \n",
            "136/390 [=========>....................] - ETA: 40s - loss: 2.0625 - acc: 0.4177 - LRFinder: val_loss: 3.8643 - lr = 0.00463846 \n",
            "137/390 [=========>....................] - ETA: 40s - loss: 2.0624 - acc: 0.4179 - LRFinder: val_loss: 3.7284 - lr = 0.00465949 \n",
            "138/390 [=========>....................] - ETA: 39s - loss: 2.0625 - acc: 0.4178 - LRFinder: val_loss: 3.7843 - lr = 0.00468051 \n",
            "139/390 [=========>....................] - ETA: 39s - loss: 2.0628 - acc: 0.4175 - LRFinder: val_loss: 3.8892 - lr = 0.00470154 \n",
            "140/390 [=========>....................] - ETA: 39s - loss: 2.0639 - acc: 0.4174 - LRFinder: val_loss: 3.7606 - lr = 0.00472256 \n",
            "141/390 [=========>....................] - ETA: 39s - loss: 2.0650 - acc: 0.4166 - LRFinder: val_loss: 3.6318 - lr = 0.00474359 \n",
            "142/390 [=========>....................] - ETA: 39s - loss: 2.0649 - acc: 0.4164 - LRFinder: val_loss: 3.6171 - lr = 0.00476462 \n",
            "143/390 [==========>...................] - ETA: 39s - loss: 2.0646 - acc: 0.4165 - LRFinder: val_loss: 3.7365 - lr = 0.00478564 \n",
            "144/390 [==========>...................] - ETA: 38s - loss: 2.0651 - acc: 0.4166 - LRFinder: val_loss: 3.7277 - lr = 0.00480667 \n",
            "145/390 [==========>...................] - ETA: 38s - loss: 2.0648 - acc: 0.4166 - LRFinder: val_loss: 3.8008 - lr = 0.00482769 \n",
            "146/390 [==========>...................] - ETA: 38s - loss: 2.0643 - acc: 0.4172 - LRFinder: val_loss: 3.8445 - lr = 0.00484872 \n",
            "147/390 [==========>...................] - ETA: 38s - loss: 2.0641 - acc: 0.4174 - LRFinder: val_loss: 3.6871 - lr = 0.00486974 \n",
            "148/390 [==========>...................] - ETA: 38s - loss: 2.0642 - acc: 0.4174 - LRFinder: val_loss: 3.8680 - lr = 0.00489077 \n",
            "149/390 [==========>...................] - ETA: 37s - loss: 2.0641 - acc: 0.4177 - LRFinder: val_loss: 3.9811 - lr = 0.00491179 \n",
            "150/390 [==========>...................] - ETA: 37s - loss: 2.0629 - acc: 0.4181 - LRFinder: val_loss: 3.9480 - lr = 0.00493282 \n",
            "151/390 [==========>...................] - ETA: 37s - loss: 2.0631 - acc: 0.4180 - LRFinder: val_loss: 3.9878 - lr = 0.00495385 \n",
            "152/390 [==========>...................] - ETA: 37s - loss: 2.0637 - acc: 0.4181 - LRFinder: val_loss: 3.9756 - lr = 0.00497487 \n",
            "153/390 [==========>...................] - ETA: 37s - loss: 2.0645 - acc: 0.4179 - LRFinder: val_loss: 3.9030 - lr = 0.00499590 \n",
            "154/390 [==========>...................] - ETA: 37s - loss: 2.0639 - acc: 0.4181 - LRFinder: val_loss: 3.7226 - lr = 0.00501692 \n",
            "155/390 [==========>...................] - ETA: 36s - loss: 2.0640 - acc: 0.4182 - LRFinder: val_loss: 3.6452 - lr = 0.00503795 \n",
            "156/390 [===========>..................] - ETA: 36s - loss: 2.0640 - acc: 0.4185 - LRFinder: val_loss: 3.6710 - lr = 0.00505897 \n",
            "157/390 [===========>..................] - ETA: 36s - loss: 2.0636 - acc: 0.4186 - LRFinder: val_loss: 3.9211 - lr = 0.00508000 \n",
            "158/390 [===========>..................] - ETA: 36s - loss: 2.0632 - acc: 0.4191 - LRFinder: val_loss: 3.6370 - lr = 0.00510103 \n",
            "159/390 [===========>..................] - ETA: 36s - loss: 2.0622 - acc: 0.4196 - LRFinder: val_loss: 3.8223 - lr = 0.00512205 \n",
            "160/390 [===========>..................] - ETA: 36s - loss: 2.0613 - acc: 0.4200 - LRFinder: val_loss: 3.8842 - lr = 0.00514308 \n",
            "161/390 [===========>..................] - ETA: 35s - loss: 2.0604 - acc: 0.4210 - LRFinder: val_loss: 3.7866 - lr = 0.00516410 \n",
            "162/390 [===========>..................] - ETA: 35s - loss: 2.0595 - acc: 0.4215 - LRFinder: val_loss: 3.8086 - lr = 0.00518513 \n",
            "163/390 [===========>..................] - ETA: 35s - loss: 2.0602 - acc: 0.4210 - LRFinder: val_loss: 3.6842 - lr = 0.00520615 \n",
            "164/390 [===========>..................] - ETA: 35s - loss: 2.0595 - acc: 0.4211 - LRFinder: val_loss: 3.9366 - lr = 0.00522718 \n",
            "165/390 [===========>..................] - ETA: 35s - loss: 2.0590 - acc: 0.4213 - LRFinder: val_loss: 3.8489 - lr = 0.00524821 \n",
            "166/390 [===========>..................] - ETA: 35s - loss: 2.0591 - acc: 0.4211 - LRFinder: val_loss: 4.0087 - lr = 0.00526923 \n",
            "167/390 [===========>..................] - ETA: 34s - loss: 2.0592 - acc: 0.4208 - LRFinder: val_loss: 3.9176 - lr = 0.00529026 \n",
            "168/390 [===========>..................] - ETA: 34s - loss: 2.0593 - acc: 0.4209 - LRFinder: val_loss: 3.8819 - lr = 0.00531128 \n",
            "169/390 [============>.................] - ETA: 34s - loss: 2.0602 - acc: 0.4203 - LRFinder: val_loss: 3.8122 - lr = 0.00533231 \n",
            "170/390 [============>.................] - ETA: 34s - loss: 2.0598 - acc: 0.4205 - LRFinder: val_loss: 3.9161 - lr = 0.00535333 \n",
            "171/390 [============>.................] - ETA: 34s - loss: 2.0608 - acc: 0.4201 - LRFinder: val_loss: 3.7796 - lr = 0.00537436 \n",
            "172/390 [============>.................] - ETA: 33s - loss: 2.0602 - acc: 0.4203 - LRFinder: val_loss: 3.8260 - lr = 0.00539538 \n",
            "173/390 [============>.................] - ETA: 33s - loss: 2.0603 - acc: 0.4202 - LRFinder: val_loss: 3.7451 - lr = 0.00541641 \n",
            "174/390 [============>.................] - ETA: 33s - loss: 2.0595 - acc: 0.4203 - LRFinder: val_loss: 3.6534 - lr = 0.00543744 \n",
            "175/390 [============>.................] - ETA: 33s - loss: 2.0595 - acc: 0.4202 - LRFinder: val_loss: 3.6691 - lr = 0.00545846 \n",
            "176/390 [============>.................] - ETA: 33s - loss: 2.0601 - acc: 0.4200 - LRFinder: val_loss: 3.7987 - lr = 0.00547949 \n",
            "177/390 [============>.................] - ETA: 33s - loss: 2.0595 - acc: 0.4200 - LRFinder: val_loss: 3.9132 - lr = 0.00550051 \n",
            "178/390 [============>.................] - ETA: 32s - loss: 2.0596 - acc: 0.4201 - LRFinder: val_loss: 3.7592 - lr = 0.00552154 \n",
            "179/390 [============>.................] - ETA: 32s - loss: 2.0594 - acc: 0.4207 - LRFinder: val_loss: 3.7833 - lr = 0.00554256 \n",
            "180/390 [============>.................] - ETA: 32s - loss: 2.0592 - acc: 0.4210 - LRFinder: val_loss: 3.6010 - lr = 0.00556359 \n",
            "181/390 [============>.................] - ETA: 32s - loss: 2.0591 - acc: 0.4212 - LRFinder: val_loss: 3.8316 - lr = 0.00558462 \n",
            "182/390 [=============>................] - ETA: 32s - loss: 2.0593 - acc: 0.4212 - LRFinder: val_loss: 3.8673 - lr = 0.00560564 \n",
            "183/390 [=============>................] - ETA: 32s - loss: 2.0596 - acc: 0.4210 - LRFinder: val_loss: 4.0731 - lr = 0.00562667 \n",
            "184/390 [=============>................] - ETA: 31s - loss: 2.0594 - acc: 0.4207 - LRFinder: val_loss: 4.0090 - lr = 0.00564769 \n",
            "185/390 [=============>................] - ETA: 31s - loss: 2.0596 - acc: 0.4209 - LRFinder: val_loss: 3.9011 - lr = 0.00566872 \n",
            "186/390 [=============>................] - ETA: 31s - loss: 2.0592 - acc: 0.4212 - LRFinder: val_loss: 3.8872 - lr = 0.00568974 \n",
            "187/390 [=============>................] - ETA: 31s - loss: 2.0594 - acc: 0.4212 - LRFinder: val_loss: 3.7566 - lr = 0.00571077 \n",
            "188/390 [=============>................] - ETA: 31s - loss: 2.0590 - acc: 0.4213 - LRFinder: val_loss: 3.7897 - lr = 0.00573179 \n",
            "189/390 [=============>................] - ETA: 31s - loss: 2.0588 - acc: 0.4213 - LRFinder: val_loss: 3.8342 - lr = 0.00575282 \n",
            "190/390 [=============>................] - ETA: 30s - loss: 2.0585 - acc: 0.4214 - LRFinder: val_loss: 3.8637 - lr = 0.00577385 \n",
            "191/390 [=============>................] - ETA: 30s - loss: 2.0588 - acc: 0.4210 - LRFinder: val_loss: 3.8139 - lr = 0.00579487 \n",
            "192/390 [=============>................] - ETA: 30s - loss: 2.0585 - acc: 0.4211 - LRFinder: val_loss: 3.8021 - lr = 0.00581590 \n",
            "193/390 [=============>................] - ETA: 30s - loss: 2.0582 - acc: 0.4207 - LRFinder: val_loss: 3.7200 - lr = 0.00583692 \n",
            "194/390 [=============>................] - ETA: 30s - loss: 2.0587 - acc: 0.4205 - LRFinder: val_loss: 3.7624 - lr = 0.00585795 \n",
            "195/390 [==============>...............] - ETA: 30s - loss: 2.0585 - acc: 0.4206 - LRFinder: val_loss: 3.8005 - lr = 0.00587897 \n",
            "196/390 [==============>...............] - ETA: 29s - loss: 2.0587 - acc: 0.4206 - LRFinder: val_loss: 3.6014 - lr = 0.00590000 \n",
            "197/390 [==============>...............] - ETA: 29s - loss: 2.0584 - acc: 0.4209 - LRFinder: val_loss: 3.7342 - lr = 0.00592103 \n",
            "198/390 [==============>...............] - ETA: 29s - loss: 2.0584 - acc: 0.4210 - LRFinder: val_loss: 3.6233 - lr = 0.00594205 \n",
            "199/390 [==============>...............] - ETA: 29s - loss: 2.0582 - acc: 0.4211 - LRFinder: val_loss: 3.6287 - lr = 0.00596308 \n",
            "200/390 [==============>...............] - ETA: 29s - loss: 2.0591 - acc: 0.4205 - LRFinder: val_loss: 3.6223 - lr = 0.00598410 \n",
            "201/390 [==============>...............] - ETA: 29s - loss: 2.0590 - acc: 0.4205 - LRFinder: val_loss: 3.7387 - lr = 0.00600513 \n",
            "202/390 [==============>...............] - ETA: 29s - loss: 2.0586 - acc: 0.4204 - LRFinder: val_loss: 3.8420 - lr = 0.00602615 \n",
            "203/390 [==============>...............] - ETA: 28s - loss: 2.0585 - acc: 0.4206 - LRFinder: val_loss: 3.7598 - lr = 0.00604718 \n",
            "204/390 [==============>...............] - ETA: 28s - loss: 2.0582 - acc: 0.4206 - LRFinder: val_loss: 3.6281 - lr = 0.00606821 \n",
            "205/390 [==============>...............] - ETA: 28s - loss: 2.0570 - acc: 0.4210 - LRFinder: val_loss: 3.6548 - lr = 0.00608923 \n",
            "206/390 [==============>...............] - ETA: 28s - loss: 2.0566 - acc: 0.4210 - LRFinder: val_loss: 3.5978 - lr = 0.00611026 \n",
            "207/390 [==============>...............] - ETA: 28s - loss: 2.0563 - acc: 0.4213 - LRFinder: val_loss: 3.7005 - lr = 0.00613128 \n",
            "208/390 [===============>..............] - ETA: 28s - loss: 2.0559 - acc: 0.4213 - LRFinder: val_loss: 3.6139 - lr = 0.00615231 \n",
            "209/390 [===============>..............] - ETA: 27s - loss: 2.0563 - acc: 0.4211 - LRFinder: val_loss: 3.6438 - lr = 0.00617333 \n",
            "210/390 [===============>..............] - ETA: 27s - loss: 2.0562 - acc: 0.4212 - LRFinder: val_loss: 3.6540 - lr = 0.00619436 \n",
            "211/390 [===============>..............] - ETA: 27s - loss: 2.0554 - acc: 0.4217 - LRFinder: val_loss: 3.6443 - lr = 0.00621538 \n",
            "212/390 [===============>..............] - ETA: 27s - loss: 2.0555 - acc: 0.4214 - LRFinder: val_loss: 3.7002 - lr = 0.00623641 \n",
            "213/390 [===============>..............] - ETA: 27s - loss: 2.0551 - acc: 0.4216 - LRFinder: val_loss: 3.7381 - lr = 0.00625744 \n",
            "214/390 [===============>..............] - ETA: 27s - loss: 2.0553 - acc: 0.4216 - LRFinder: val_loss: 3.7679 - lr = 0.00627846 \n",
            "215/390 [===============>..............] - ETA: 26s - loss: 2.0545 - acc: 0.4217 - LRFinder: val_loss: 3.7726 - lr = 0.00629949 \n",
            "216/390 [===============>..............] - ETA: 26s - loss: 2.0538 - acc: 0.4222 - LRFinder: val_loss: 3.7687 - lr = 0.00632051 \n",
            "217/390 [===============>..............] - ETA: 26s - loss: 2.0539 - acc: 0.4218 - LRFinder: val_loss: 3.8634 - lr = 0.00634154 \n",
            "218/390 [===============>..............] - ETA: 26s - loss: 2.0541 - acc: 0.4218 - LRFinder: val_loss: 3.8411 - lr = 0.00636256 \n",
            "219/390 [===============>..............] - ETA: 26s - loss: 2.0532 - acc: 0.4223 - LRFinder: val_loss: 3.8291 - lr = 0.00638359 \n",
            "220/390 [===============>..............] - ETA: 26s - loss: 2.0530 - acc: 0.4224 - LRFinder: val_loss: 4.0195 - lr = 0.00640462 \n",
            "221/390 [================>.............] - ETA: 25s - loss: 2.0527 - acc: 0.4226 - LRFinder: val_loss: 3.7960 - lr = 0.00642564 \n",
            "222/390 [================>.............] - ETA: 25s - loss: 2.0522 - acc: 0.4227 - LRFinder: val_loss: 4.0126 - lr = 0.00644667 \n",
            "223/390 [================>.............] - ETA: 25s - loss: 2.0528 - acc: 0.4224 - LRFinder: val_loss: 3.7182 - lr = 0.00646769 \n",
            "224/390 [================>.............] - ETA: 25s - loss: 2.0527 - acc: 0.4226 - LRFinder: val_loss: 3.8414 - lr = 0.00648872 \n",
            "225/390 [================>.............] - ETA: 25s - loss: 2.0522 - acc: 0.4224 - LRFinder: val_loss: 3.8415 - lr = 0.00650974 \n",
            "226/390 [================>.............] - ETA: 25s - loss: 2.0513 - acc: 0.4227 - LRFinder: val_loss: 3.7560 - lr = 0.00653077 \n",
            "227/390 [================>.............] - ETA: 25s - loss: 2.0512 - acc: 0.4228 - LRFinder: val_loss: 3.7572 - lr = 0.00655179 \n",
            "228/390 [================>.............] - ETA: 24s - loss: 2.0514 - acc: 0.4227 - LRFinder: val_loss: 3.7067 - lr = 0.00657282 \n",
            "229/390 [================>.............] - ETA: 24s - loss: 2.0510 - acc: 0.4232 - LRFinder: val_loss: 3.7314 - lr = 0.00659385 \n",
            "230/390 [================>.............] - ETA: 24s - loss: 2.0513 - acc: 0.4229 - LRFinder: val_loss: 3.6593 - lr = 0.00661487 \n",
            "231/390 [================>.............] - ETA: 24s - loss: 2.0508 - acc: 0.4232 - LRFinder: val_loss: 3.7808 - lr = 0.00663590 \n",
            "232/390 [================>.............] - ETA: 24s - loss: 2.0505 - acc: 0.4232 - LRFinder: val_loss: 3.7047 - lr = 0.00665692 \n",
            "233/390 [================>.............] - ETA: 24s - loss: 2.0501 - acc: 0.4234 - LRFinder: val_loss: 3.7250 - lr = 0.00667795 \n",
            "234/390 [=================>............] - ETA: 23s - loss: 2.0501 - acc: 0.4233 - LRFinder: val_loss: 4.0228 - lr = 0.00669897 \n",
            "235/390 [=================>............] - ETA: 23s - loss: 2.0500 - acc: 0.4233 - LRFinder: val_loss: 3.9088 - lr = 0.00672000 \n",
            "236/390 [=================>............] - ETA: 23s - loss: 2.0496 - acc: 0.4233 - LRFinder: val_loss: 3.7816 - lr = 0.00674103 \n",
            "237/390 [=================>............] - ETA: 23s - loss: 2.0495 - acc: 0.4234 - LRFinder: val_loss: 3.7491 - lr = 0.00676205 \n",
            "238/390 [=================>............] - ETA: 23s - loss: 2.0498 - acc: 0.4235 - LRFinder: val_loss: 3.8449 - lr = 0.00678308 \n",
            "239/390 [=================>............] - ETA: 23s - loss: 2.0498 - acc: 0.4237 - LRFinder: val_loss: 3.8791 - lr = 0.00680410 \n",
            "240/390 [=================>............] - ETA: 22s - loss: 2.0497 - acc: 0.4239 - LRFinder: val_loss: 3.7696 - lr = 0.00682513 \n",
            "241/390 [=================>............] - ETA: 22s - loss: 2.0498 - acc: 0.4238 - LRFinder: val_loss: 3.9016 - lr = 0.00684615 \n",
            "242/390 [=================>............] - ETA: 22s - loss: 2.0498 - acc: 0.4237 - LRFinder: val_loss: 3.8086 - lr = 0.00686718 \n",
            "243/390 [=================>............] - ETA: 22s - loss: 2.0500 - acc: 0.4235 - LRFinder: val_loss: 3.9415 - lr = 0.00688821 \n",
            "244/390 [=================>............] - ETA: 22s - loss: 2.0507 - acc: 0.4230 - LRFinder: val_loss: 3.8682 - lr = 0.00690923 \n",
            "245/390 [=================>............] - ETA: 22s - loss: 2.0506 - acc: 0.4233 - LRFinder: val_loss: 3.8317 - lr = 0.00693026 \n",
            "246/390 [=================>............] - ETA: 22s - loss: 2.0500 - acc: 0.4238 - LRFinder: val_loss: 3.8766 - lr = 0.00695128 \n",
            "247/390 [==================>...........] - ETA: 21s - loss: 2.0497 - acc: 0.4240 - LRFinder: val_loss: 3.8287 - lr = 0.00697231 \n",
            "248/390 [==================>...........] - ETA: 21s - loss: 2.0494 - acc: 0.4242 - LRFinder: val_loss: 3.7850 - lr = 0.00699333 \n",
            "249/390 [==================>...........] - ETA: 21s - loss: 2.0494 - acc: 0.4242 - LRFinder: val_loss: 3.7777 - lr = 0.00701436 \n",
            "250/390 [==================>...........] - ETA: 21s - loss: 2.0495 - acc: 0.4242 - LRFinder: val_loss: 3.8627 - lr = 0.00703538 \n",
            "251/390 [==================>...........] - ETA: 21s - loss: 2.0497 - acc: 0.4242 - LRFinder: val_loss: 3.6736 - lr = 0.00705641 \n",
            "252/390 [==================>...........] - ETA: 21s - loss: 2.0491 - acc: 0.4243 - LRFinder: val_loss: 3.5678 - lr = 0.00707744 \n",
            "253/390 [==================>...........] - ETA: 20s - loss: 2.0492 - acc: 0.4242 - LRFinder: val_loss: 3.5943 - lr = 0.00709846 \n",
            "254/390 [==================>...........] - ETA: 20s - loss: 2.0489 - acc: 0.4245 - LRFinder: val_loss: 3.4514 - lr = 0.00711949 \n",
            "255/390 [==================>...........] - ETA: 20s - loss: 2.0485 - acc: 0.4249 - LRFinder: val_loss: 3.6584 - lr = 0.00714051 \n",
            "256/390 [==================>...........] - ETA: 20s - loss: 2.0485 - acc: 0.4248 - LRFinder: val_loss: 3.7012 - lr = 0.00716154 \n",
            "257/390 [==================>...........] - ETA: 20s - loss: 2.0481 - acc: 0.4250 - LRFinder: val_loss: 3.6908 - lr = 0.00718256 \n",
            "258/390 [==================>...........] - ETA: 20s - loss: 2.0479 - acc: 0.4251 - LRFinder: val_loss: 3.5815 - lr = 0.00720359 \n",
            "259/390 [==================>...........] - ETA: 19s - loss: 2.0477 - acc: 0.4253 - LRFinder: val_loss: 3.7074 - lr = 0.00722462 \n",
            "260/390 [===================>..........] - ETA: 19s - loss: 2.0474 - acc: 0.4254 - LRFinder: val_loss: 3.6008 - lr = 0.00724564 \n",
            "261/390 [===================>..........] - ETA: 19s - loss: 2.0473 - acc: 0.4254 - LRFinder: val_loss: 3.5829 - lr = 0.00726667 \n",
            "262/390 [===================>..........] - ETA: 19s - loss: 2.0467 - acc: 0.4257 - LRFinder: val_loss: 3.4726 - lr = 0.00728769 \n",
            "263/390 [===================>..........] - ETA: 19s - loss: 2.0465 - acc: 0.4256 - LRFinder: val_loss: 3.6213 - lr = 0.00730872 \n",
            "264/390 [===================>..........] - ETA: 19s - loss: 2.0460 - acc: 0.4258 - LRFinder: val_loss: 3.4579 - lr = 0.00732974 \n",
            "265/390 [===================>..........] - ETA: 19s - loss: 2.0459 - acc: 0.4261 - LRFinder: val_loss: 3.5011 - lr = 0.00735077 \n",
            "266/390 [===================>..........] - ETA: 18s - loss: 2.0464 - acc: 0.4261 - LRFinder: val_loss: 3.4485 - lr = 0.00737179 \n",
            "267/390 [===================>..........] - ETA: 18s - loss: 2.0458 - acc: 0.4265 - LRFinder: val_loss: 3.5573 - lr = 0.00739282 \n",
            "268/390 [===================>..........] - ETA: 18s - loss: 2.0452 - acc: 0.4266 - LRFinder: val_loss: 3.6012 - lr = 0.00741385 \n",
            "269/390 [===================>..........] - ETA: 18s - loss: 2.0456 - acc: 0.4264 - LRFinder: val_loss: 3.4358 - lr = 0.00743487 \n",
            "270/390 [===================>..........] - ETA: 18s - loss: 2.0451 - acc: 0.4267 - LRFinder: val_loss: 3.5069 - lr = 0.00745590 \n",
            "271/390 [===================>..........] - ETA: 18s - loss: 2.0449 - acc: 0.4266 - LRFinder: val_loss: 3.3581 - lr = 0.00747692 \n",
            "272/390 [===================>..........] - ETA: 17s - loss: 2.0445 - acc: 0.4267 - LRFinder: val_loss: 3.3529 - lr = 0.00749795 \n",
            "273/390 [====================>.........] - ETA: 17s - loss: 2.0442 - acc: 0.4268 - LRFinder: val_loss: 3.2796 - lr = 0.00751897 \n",
            "274/390 [====================>.........] - ETA: 17s - loss: 2.0433 - acc: 0.4271 - LRFinder: val_loss: 3.2847 - lr = 0.00754000 \n",
            "275/390 [====================>.........] - ETA: 17s - loss: 2.0432 - acc: 0.4272 - LRFinder: val_loss: 3.3640 - lr = 0.00756103 \n",
            "276/390 [====================>.........] - ETA: 17s - loss: 2.0431 - acc: 0.4272 - LRFinder: val_loss: 3.4047 - lr = 0.00758205 \n",
            "277/390 [====================>.........] - ETA: 17s - loss: 2.0424 - acc: 0.4275 - LRFinder: val_loss: 3.5807 - lr = 0.00760308 \n",
            "278/390 [====================>.........] - ETA: 17s - loss: 2.0419 - acc: 0.4275 - LRFinder: val_loss: 3.4078 - lr = 0.00762410 \n",
            "279/390 [====================>.........] - ETA: 16s - loss: 2.0411 - acc: 0.4276 - LRFinder: val_loss: 3.4168 - lr = 0.00764513 \n",
            "280/390 [====================>.........] - ETA: 16s - loss: 2.0408 - acc: 0.4277 - LRFinder: val_loss: 3.4016 - lr = 0.00766615 \n",
            "281/390 [====================>.........] - ETA: 16s - loss: 2.0404 - acc: 0.4278 - LRFinder: val_loss: 3.3323 - lr = 0.00768718 \n",
            "282/390 [====================>.........] - ETA: 16s - loss: 2.0405 - acc: 0.4277 - LRFinder: val_loss: 3.2765 - lr = 0.00770821 \n",
            "283/390 [====================>.........] - ETA: 16s - loss: 2.0398 - acc: 0.4277 - LRFinder: val_loss: 3.2640 - lr = 0.00772923 \n",
            "284/390 [====================>.........] - ETA: 16s - loss: 2.0398 - acc: 0.4276 - LRFinder: val_loss: 3.4029 - lr = 0.00775026 \n",
            "285/390 [====================>.........] - ETA: 15s - loss: 2.0396 - acc: 0.4276 - LRFinder: val_loss: 3.4375 - lr = 0.00777128 \n",
            "286/390 [=====================>........] - ETA: 15s - loss: 2.0396 - acc: 0.4276 - LRFinder: val_loss: 3.5888 - lr = 0.00779231 \n",
            "287/390 [=====================>........] - ETA: 15s - loss: 2.0394 - acc: 0.4278 - LRFinder: val_loss: 3.6559 - lr = 0.00781333 \n",
            "288/390 [=====================>........] - ETA: 15s - loss: 2.0392 - acc: 0.4278 - LRFinder: val_loss: 3.6866 - lr = 0.00783436 \n",
            "289/390 [=====================>........] - ETA: 15s - loss: 2.0388 - acc: 0.4279 - LRFinder: val_loss: 3.4015 - lr = 0.00785538 \n",
            "290/390 [=====================>........] - ETA: 15s - loss: 2.0386 - acc: 0.4281 - LRFinder: val_loss: 3.4303 - lr = 0.00787641 \n",
            "291/390 [=====================>........] - ETA: 15s - loss: 2.0387 - acc: 0.4280 - LRFinder: val_loss: 3.3842 - lr = 0.00789744 \n",
            "292/390 [=====================>........] - ETA: 14s - loss: 2.0380 - acc: 0.4283 - LRFinder: val_loss: 3.3307 - lr = 0.00791846 \n",
            "293/390 [=====================>........] - ETA: 14s - loss: 2.0378 - acc: 0.4284 - LRFinder: val_loss: 3.4392 - lr = 0.00793949 \n",
            "294/390 [=====================>........] - ETA: 14s - loss: 2.0380 - acc: 0.4284 - LRFinder: val_loss: 3.4636 - lr = 0.00796051 \n",
            "295/390 [=====================>........] - ETA: 14s - loss: 2.0379 - acc: 0.4284 - LRFinder: val_loss: 3.4068 - lr = 0.00798154 \n",
            "296/390 [=====================>........] - ETA: 14s - loss: 2.0378 - acc: 0.4287 - LRFinder: val_loss: 3.5470 - lr = 0.00800256 \n",
            "297/390 [=====================>........] - ETA: 14s - loss: 2.0377 - acc: 0.4287 - LRFinder: val_loss: 3.4008 - lr = 0.00802359 \n",
            "298/390 [=====================>........] - ETA: 13s - loss: 2.0376 - acc: 0.4287 - LRFinder: val_loss: 3.3901 - lr = 0.00804462 \n",
            "299/390 [======================>.......] - ETA: 13s - loss: 2.0373 - acc: 0.4287 - LRFinder: val_loss: 3.4486 - lr = 0.00806564 \n",
            "300/390 [======================>.......] - ETA: 13s - loss: 2.0369 - acc: 0.4288 - LRFinder: val_loss: 3.5780 - lr = 0.00808667 \n",
            "301/390 [======================>.......] - ETA: 13s - loss: 2.0362 - acc: 0.4291 - LRFinder: val_loss: 3.5759 - lr = 0.00810769 \n",
            "302/390 [======================>.......] - ETA: 13s - loss: 2.0356 - acc: 0.4294 - LRFinder: val_loss: 3.6987 - lr = 0.00812872 \n",
            "303/390 [======================>.......] - ETA: 13s - loss: 2.0357 - acc: 0.4294 - LRFinder: val_loss: 3.4526 - lr = 0.00814974 \n",
            "304/390 [======================>.......] - ETA: 13s - loss: 2.0358 - acc: 0.4293 - LRFinder: val_loss: 3.4436 - lr = 0.00817077 \n",
            "305/390 [======================>.......] - ETA: 12s - loss: 2.0360 - acc: 0.4293 - LRFinder: val_loss: 3.3131 - lr = 0.00819179 \n",
            "306/390 [======================>.......] - ETA: 12s - loss: 2.0356 - acc: 0.4294 - LRFinder: val_loss: 3.3502 - lr = 0.00821282 \n",
            "307/390 [======================>.......] - ETA: 12s - loss: 2.0351 - acc: 0.4297 - LRFinder: val_loss: 3.5314 - lr = 0.00823385 \n",
            "308/390 [======================>.......] - ETA: 12s - loss: 2.0350 - acc: 0.4297 - LRFinder: val_loss: 3.3537 - lr = 0.00825487 \n",
            "309/390 [======================>.......] - ETA: 12s - loss: 2.0349 - acc: 0.4299 - LRFinder: val_loss: 3.2855 - lr = 0.00827590 \n",
            "310/390 [======================>.......] - ETA: 12s - loss: 2.0342 - acc: 0.4303 - LRFinder: val_loss: 3.2503 - lr = 0.00829692 \n",
            "311/390 [======================>.......] - ETA: 11s - loss: 2.0339 - acc: 0.4305 - LRFinder: val_loss: 3.1743 - lr = 0.00831795 \n",
            "312/390 [=======================>......] - ETA: 11s - loss: 2.0336 - acc: 0.4305 - LRFinder: val_loss: 3.1881 - lr = 0.00833897 \n",
            "313/390 [=======================>......] - ETA: 11s - loss: 2.0333 - acc: 0.4306 - LRFinder: val_loss: 3.1689 - lr = 0.00836000 \n",
            "314/390 [=======================>......] - ETA: 11s - loss: 2.0334 - acc: 0.4305 - LRFinder: val_loss: 3.1860 - lr = 0.00838103 \n",
            "315/390 [=======================>......] - ETA: 11s - loss: 2.0332 - acc: 0.4305 - LRFinder: val_loss: 3.1178 - lr = 0.00840205 \n",
            "316/390 [=======================>......] - ETA: 11s - loss: 2.0329 - acc: 0.4306 - LRFinder: val_loss: 3.1455 - lr = 0.00842308 \n",
            "317/390 [=======================>......] - ETA: 11s - loss: 2.0327 - acc: 0.4307 - LRFinder: val_loss: 3.0779 - lr = 0.00844410 \n",
            "318/390 [=======================>......] - ETA: 10s - loss: 2.0326 - acc: 0.4308 - LRFinder: val_loss: 3.1998 - lr = 0.00846513 \n",
            "319/390 [=======================>......] - ETA: 10s - loss: 2.0323 - acc: 0.4309 - LRFinder: val_loss: 3.3385 - lr = 0.00848615 \n",
            "320/390 [=======================>......] - ETA: 10s - loss: 2.0322 - acc: 0.4309 - LRFinder: val_loss: 3.3841 - lr = 0.00850718 \n",
            "321/390 [=======================>......] - ETA: 10s - loss: 2.0325 - acc: 0.4308 - LRFinder: val_loss: 3.3159 - lr = 0.00852821 \n",
            "322/390 [=======================>......] - ETA: 10s - loss: 2.0320 - acc: 0.4311 - LRFinder: val_loss: 3.3616 - lr = 0.00854923 \n",
            "323/390 [=======================>......] - ETA: 10s - loss: 2.0316 - acc: 0.4311 - LRFinder: val_loss: 3.1481 - lr = 0.00857026 \n",
            "324/390 [=======================>......] - ETA: 9s - loss: 2.0310 - acc: 0.4314  - LRFinder: val_loss: 3.2211 - lr = 0.00859128 \n",
            "325/390 [========================>.....] - ETA: 9s - loss: 2.0306 - acc: 0.4314 - LRFinder: val_loss: 3.1824 - lr = 0.00861231 \n",
            "326/390 [========================>.....] - ETA: 9s - loss: 2.0302 - acc: 0.4315 - LRFinder: val_loss: 3.4439 - lr = 0.00863333 \n",
            "327/390 [========================>.....] - ETA: 9s - loss: 2.0301 - acc: 0.4315 - LRFinder: val_loss: 3.4340 - lr = 0.00865436 \n",
            "328/390 [========================>.....] - ETA: 9s - loss: 2.0297 - acc: 0.4317 - LRFinder: val_loss: 3.4538 - lr = 0.00867538 \n",
            "329/390 [========================>.....] - ETA: 9s - loss: 2.0302 - acc: 0.4315 - LRFinder: val_loss: 3.4431 - lr = 0.00869641 \n",
            "330/390 [========================>.....] - ETA: 9s - loss: 2.0304 - acc: 0.4313 - LRFinder: val_loss: 3.5136 - lr = 0.00871744 \n",
            "331/390 [========================>.....] - ETA: 8s - loss: 2.0301 - acc: 0.4314 - LRFinder: val_loss: 3.3655 - lr = 0.00873846 \n",
            "332/390 [========================>.....] - ETA: 8s - loss: 2.0302 - acc: 0.4316 - LRFinder: val_loss: 3.3935 - lr = 0.00875949 \n",
            "333/390 [========================>.....] - ETA: 8s - loss: 2.0296 - acc: 0.4318 - LRFinder: val_loss: 3.3246 - lr = 0.00878051 \n",
            "334/390 [========================>.....] - ETA: 8s - loss: 2.0295 - acc: 0.4319 - LRFinder: val_loss: 3.4771 - lr = 0.00880154 \n",
            "335/390 [========================>.....] - ETA: 8s - loss: 2.0291 - acc: 0.4321 - LRFinder: val_loss: 3.3757 - lr = 0.00882256 \n",
            "336/390 [========================>.....] - ETA: 8s - loss: 2.0289 - acc: 0.4322 - LRFinder: val_loss: 3.2966 - lr = 0.00884359 \n",
            "337/390 [========================>.....] - ETA: 7s - loss: 2.0290 - acc: 0.4321 - LRFinder: val_loss: 3.3131 - lr = 0.00886462 \n",
            "338/390 [=========================>....] - ETA: 7s - loss: 2.0289 - acc: 0.4318 - LRFinder: val_loss: 3.3399 - lr = 0.00888564 \n",
            "339/390 [=========================>....] - ETA: 7s - loss: 2.0289 - acc: 0.4317 - LRFinder: val_loss: 3.4409 - lr = 0.00890667 \n",
            "340/390 [=========================>....] - ETA: 7s - loss: 2.0290 - acc: 0.4317 - LRFinder: val_loss: 3.2876 - lr = 0.00892769 \n",
            "341/390 [=========================>....] - ETA: 7s - loss: 2.0287 - acc: 0.4319 - LRFinder: val_loss: 3.2771 - lr = 0.00894872 \n",
            "342/390 [=========================>....] - ETA: 7s - loss: 2.0284 - acc: 0.4319 - LRFinder: val_loss: 3.2616 - lr = 0.00896974 \n",
            "343/390 [=========================>....] - ETA: 7s - loss: 2.0282 - acc: 0.4321 - LRFinder: val_loss: 3.4060 - lr = 0.00899077 \n",
            "344/390 [=========================>....] - ETA: 6s - loss: 2.0274 - acc: 0.4324 - LRFinder: val_loss: 3.4398 - lr = 0.00901179 \n",
            "345/390 [=========================>....] - ETA: 6s - loss: 2.0273 - acc: 0.4326 - LRFinder: val_loss: 3.4884 - lr = 0.00903282 \n",
            "346/390 [=========================>....] - ETA: 6s - loss: 2.0268 - acc: 0.4328 - LRFinder: val_loss: 3.5368 - lr = 0.00905385 \n",
            "347/390 [=========================>....] - ETA: 6s - loss: 2.0266 - acc: 0.4328 - LRFinder: val_loss: 3.6304 - lr = 0.00907487 \n",
            "348/390 [=========================>....] - ETA: 6s - loss: 2.0264 - acc: 0.4330 - LRFinder: val_loss: 3.7700 - lr = 0.00909590 \n",
            "349/390 [=========================>....] - ETA: 6s - loss: 2.0263 - acc: 0.4330 - LRFinder: val_loss: 3.6822 - lr = 0.00911692 \n",
            "350/390 [=========================>....] - ETA: 6s - loss: 2.0261 - acc: 0.4330 - LRFinder: val_loss: 3.7006 - lr = 0.00913795 \n",
            "351/390 [==========================>...] - ETA: 5s - loss: 2.0265 - acc: 0.4329 - LRFinder: val_loss: 3.4562 - lr = 0.00915897 \n",
            "352/390 [==========================>...] - ETA: 5s - loss: 2.0263 - acc: 0.4331 - LRFinder: val_loss: 3.7396 - lr = 0.00918000 \n",
            "353/390 [==========================>...] - ETA: 5s - loss: 2.0260 - acc: 0.4332 - LRFinder: val_loss: 3.7299 - lr = 0.00920103 \n",
            "354/390 [==========================>...] - ETA: 5s - loss: 2.0258 - acc: 0.4332 - LRFinder: val_loss: 3.7404 - lr = 0.00922205 \n",
            "355/390 [==========================>...] - ETA: 5s - loss: 2.0255 - acc: 0.4333 - LRFinder: val_loss: 3.6790 - lr = 0.00924308 \n",
            "356/390 [==========================>...] - ETA: 5s - loss: 2.0253 - acc: 0.4335 - LRFinder: val_loss: 3.9627 - lr = 0.00926410 \n",
            "357/390 [==========================>...] - ETA: 4s - loss: 2.0246 - acc: 0.4337 - LRFinder: val_loss: 4.2227 - lr = 0.00928513 \n",
            "358/390 [==========================>...] - ETA: 4s - loss: 2.0245 - acc: 0.4339 - LRFinder: val_loss: 3.9399 - lr = 0.00930615 \n",
            "359/390 [==========================>...] - ETA: 4s - loss: 2.0243 - acc: 0.4339 - LRFinder: val_loss: 3.9094 - lr = 0.00932718 \n",
            "360/390 [==========================>...] - ETA: 4s - loss: 2.0242 - acc: 0.4340 - LRFinder: val_loss: 3.8890 - lr = 0.00934821 \n",
            "361/390 [==========================>...] - ETA: 4s - loss: 2.0241 - acc: 0.4342 - LRFinder: val_loss: 3.7882 - lr = 0.00936923 \n",
            "362/390 [==========================>...] - ETA: 4s - loss: 2.0240 - acc: 0.4343 - LRFinder: val_loss: 3.7755 - lr = 0.00939026 \n",
            "363/390 [==========================>...] - ETA: 4s - loss: 2.0239 - acc: 0.4343 - LRFinder: val_loss: 3.7552 - lr = 0.00941128 \n",
            "364/390 [===========================>..] - ETA: 3s - loss: 2.0240 - acc: 0.4343 - LRFinder: val_loss: 3.6837 - lr = 0.00943231 \n",
            "365/390 [===========================>..] - ETA: 3s - loss: 2.0236 - acc: 0.4345 - LRFinder: val_loss: 3.5875 - lr = 0.00945333 \n",
            "366/390 [===========================>..] - ETA: 3s - loss: 2.0233 - acc: 0.4346 - LRFinder: val_loss: 3.6286 - lr = 0.00947436 \n",
            "367/390 [===========================>..] - ETA: 3s - loss: 2.0231 - acc: 0.4346 - LRFinder: val_loss: 3.6934 - lr = 0.00949538 \n",
            "368/390 [===========================>..] - ETA: 3s - loss: 2.0227 - acc: 0.4347 - LRFinder: val_loss: 3.4761 - lr = 0.00951641 \n",
            "369/390 [===========================>..] - ETA: 3s - loss: 2.0223 - acc: 0.4350 - LRFinder: val_loss: 3.6139 - lr = 0.00953744 \n",
            "370/390 [===========================>..] - ETA: 3s - loss: 2.0221 - acc: 0.4351 - LRFinder: val_loss: 3.6013 - lr = 0.00955846 \n",
            "371/390 [===========================>..] - ETA: 2s - loss: 2.0222 - acc: 0.4350 - LRFinder: val_loss: 3.7194 - lr = 0.00957949 \n",
            "372/390 [===========================>..] - ETA: 2s - loss: 2.0220 - acc: 0.4351 - LRFinder: val_loss: 3.7508 - lr = 0.00960051 \n",
            "373/390 [===========================>..] - ETA: 2s - loss: 2.0219 - acc: 0.4350 - LRFinder: val_loss: 3.6289 - lr = 0.00962154 \n",
            "374/390 [===========================>..] - ETA: 2s - loss: 2.0214 - acc: 0.4352 - LRFinder: val_loss: 3.3928 - lr = 0.00964256 \n",
            "375/390 [===========================>..] - ETA: 2s - loss: 2.0211 - acc: 0.4351 - LRFinder: val_loss: 3.6699 - lr = 0.00966359 \n",
            "376/390 [===========================>..] - ETA: 2s - loss: 2.0209 - acc: 0.4352 - LRFinder: val_loss: 3.6538 - lr = 0.00968462 \n",
            "377/390 [============================>.] - ETA: 1s - loss: 2.0205 - acc: 0.4353 - LRFinder: val_loss: 3.7180 - lr = 0.00970564 \n",
            "378/390 [============================>.] - ETA: 1s - loss: 2.0203 - acc: 0.4354 - LRFinder: val_loss: 3.6467 - lr = 0.00972667 \n",
            "379/390 [============================>.] - ETA: 1s - loss: 2.0200 - acc: 0.4354 - LRFinder: val_loss: 3.6553 - lr = 0.00974769 \n",
            "380/390 [============================>.] - ETA: 1s - loss: 2.0199 - acc: 0.4355 - LRFinder: val_loss: 3.5869 - lr = 0.00976872 \n",
            "381/390 [============================>.] - ETA: 1s - loss: 2.0198 - acc: 0.4356 - LRFinder: val_loss: 3.6298 - lr = 0.00978974 \n",
            "382/390 [============================>.] - ETA: 1s - loss: 2.0194 - acc: 0.4358 - LRFinder: val_loss: 3.4675 - lr = 0.00981077 \n",
            "383/390 [============================>.] - ETA: 1s - loss: 2.0193 - acc: 0.4359 - LRFinder: val_loss: 3.3857 - lr = 0.00983179 \n",
            "384/390 [============================>.] - ETA: 0s - loss: 2.0188 - acc: 0.4361 - LRFinder: val_loss: 3.5778 - lr = 0.00985282 \n",
            "385/390 [============================>.] - ETA: 0s - loss: 2.0185 - acc: 0.4362 - LRFinder: val_loss: 3.3614 - lr = 0.00987385 \n",
            "386/390 [============================>.] - ETA: 0s - loss: 2.0187 - acc: 0.4361 - LRFinder: val_loss: 3.4379 - lr = 0.00989487 \n",
            "387/390 [============================>.] - ETA: 0s - loss: 2.0185 - acc: 0.4363 - LRFinder: val_loss: 3.3160 - lr = 0.00991590 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 2.0184 - acc: 0.4362 - LRFinder: val_loss: 3.4289 - lr = 0.00993692 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 2.0180 - acc: 0.4363 - LRFinder: val_loss: 3.3124 - lr = 0.00995795 \n",
            " - LRFinder: val_loss: 3.3862 - lr = 0.00997897 \n",
            "390/390 [==============================] - 60s 153ms/step - loss: 2.0176 - acc: 0.4364 - val_loss: 3.4503 - val_acc: 0.1627\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {weights/momentum/momentum-0.9/}\n",
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 11:11 - loss: 1.9959 - acc: 0.4844 - LRFinder: val_loss: 3.4511 - lr = 0.00180000 \n",
            "  2/390 [..............................] - ETA: 7:31 - loss: 1.9605 - acc: 0.4727  - LRFinder: val_loss: 3.5351 - lr = 0.00182103 \n",
            "  3/390 [..............................] - ETA: 5:22 - loss: 1.9541 - acc: 0.4714 - LRFinder: val_loss: 3.3660 - lr = 0.00184205 \n",
            "  4/390 [..............................] - ETA: 4:14 - loss: 1.9371 - acc: 0.4766 - LRFinder: val_loss: 3.5764 - lr = 0.00186308 \n",
            "  5/390 [..............................] - ETA: 3:34 - loss: 1.9741 - acc: 0.4562 - LRFinder: val_loss: 3.5753 - lr = 0.00188410 \n",
            "  6/390 [..............................] - ETA: 3:07 - loss: 1.9696 - acc: 0.4505 - LRFinder: val_loss: 3.5342 - lr = 0.00190513 \n",
            "  7/390 [..............................] - ETA: 2:47 - loss: 1.9728 - acc: 0.4520 - LRFinder: val_loss: 3.4711 - lr = 0.00192615 \n",
            "  8/390 [..............................] - ETA: 2:33 - loss: 1.9600 - acc: 0.4639 - LRFinder: val_loss: 3.6007 - lr = 0.00194718 \n",
            "  9/390 [..............................] - ETA: 2:21 - loss: 1.9478 - acc: 0.4740 - LRFinder: val_loss: 3.6985 - lr = 0.00196821 \n",
            " 10/390 [..............................] - ETA: 2:12 - loss: 1.9362 - acc: 0.4719 - LRFinder: val_loss: 3.6742 - lr = 0.00198923 \n",
            " 11/390 [..............................] - ETA: 2:05 - loss: 1.9398 - acc: 0.4688 - LRFinder: val_loss: 3.7608 - lr = 0.00201026 \n",
            " 12/390 [..............................] - ETA: 1:59 - loss: 1.9332 - acc: 0.4701 - LRFinder: val_loss: 3.6672 - lr = 0.00203128 \n",
            " 13/390 [>.............................] - ETA: 1:53 - loss: 1.9362 - acc: 0.4651 - LRFinder: val_loss: 3.7002 - lr = 0.00205231 \n",
            " 14/390 [>.............................] - ETA: 1:49 - loss: 1.9283 - acc: 0.4676 - LRFinder: val_loss: 3.6077 - lr = 0.00207333 \n",
            " 15/390 [>.............................] - ETA: 1:45 - loss: 1.9306 - acc: 0.4693 - LRFinder: val_loss: 3.6626 - lr = 0.00209436 \n",
            " 16/390 [>.............................] - ETA: 1:41 - loss: 1.9248 - acc: 0.4722 - LRFinder: val_loss: 3.7427 - lr = 0.00211538 \n",
            " 17/390 [>.............................] - ETA: 1:38 - loss: 1.9182 - acc: 0.4747 - LRFinder: val_loss: 3.5657 - lr = 0.00213641 \n",
            " 18/390 [>.............................] - ETA: 1:35 - loss: 1.9136 - acc: 0.4757 - LRFinder: val_loss: 3.6428 - lr = 0.00215744 \n",
            " 19/390 [>.............................] - ETA: 1:33 - loss: 1.9160 - acc: 0.4729 - LRFinder: val_loss: 3.6677 - lr = 0.00217846 \n",
            " 20/390 [>.............................] - ETA: 1:31 - loss: 1.9132 - acc: 0.4734 - LRFinder: val_loss: 3.7062 - lr = 0.00219949 \n",
            " 21/390 [>.............................] - ETA: 1:29 - loss: 1.9114 - acc: 0.4732 - LRFinder: val_loss: 3.6520 - lr = 0.00222051 \n",
            " 22/390 [>.............................] - ETA: 1:27 - loss: 1.9150 - acc: 0.4734 - LRFinder: val_loss: 3.6855 - lr = 0.00224154 \n",
            " 23/390 [>.............................] - ETA: 1:25 - loss: 1.9138 - acc: 0.4759 - LRFinder: val_loss: 3.7086 - lr = 0.00226256 \n",
            " 24/390 [>.............................] - ETA: 1:23 - loss: 1.9109 - acc: 0.4775 - LRFinder: val_loss: 3.7129 - lr = 0.00228359 \n",
            " 25/390 [>.............................] - ETA: 1:22 - loss: 1.9062 - acc: 0.4797 - LRFinder: val_loss: 3.6579 - lr = 0.00230462 \n",
            " 26/390 [=>............................] - ETA: 1:20 - loss: 1.9046 - acc: 0.4814 - LRFinder: val_loss: 3.7488 - lr = 0.00232564 \n",
            " 27/390 [=>............................] - ETA: 1:19 - loss: 1.9000 - acc: 0.4832 - LRFinder: val_loss: 3.7593 - lr = 0.00234667 \n",
            " 28/390 [=>............................] - ETA: 1:18 - loss: 1.8962 - acc: 0.4858 - LRFinder: val_loss: 3.7646 - lr = 0.00236769 \n",
            " 29/390 [=>............................] - ETA: 1:17 - loss: 1.8933 - acc: 0.4865 - LRFinder: val_loss: 3.8829 - lr = 0.00238872 \n",
            " 30/390 [=>............................] - ETA: 1:16 - loss: 1.8921 - acc: 0.4870 - LRFinder: val_loss: 3.8627 - lr = 0.00240974 \n",
            " 31/390 [=>............................] - ETA: 1:15 - loss: 1.8946 - acc: 0.4836 - LRFinder: val_loss: 3.8432 - lr = 0.00243077 \n",
            " 32/390 [=>............................] - ETA: 1:14 - loss: 1.8953 - acc: 0.4829 - LRFinder: val_loss: 3.6850 - lr = 0.00245179 \n",
            " 33/390 [=>............................] - ETA: 1:13 - loss: 1.8944 - acc: 0.4822 - LRFinder: val_loss: 3.8245 - lr = 0.00247282 \n",
            " 34/390 [=>............................] - ETA: 1:12 - loss: 1.8919 - acc: 0.4837 - LRFinder: val_loss: 3.7674 - lr = 0.00249385 \n",
            " 35/390 [=>............................] - ETA: 1:11 - loss: 1.8909 - acc: 0.4842 - LRFinder: val_loss: 3.8277 - lr = 0.00251487 \n",
            " 36/390 [=>............................] - ETA: 1:11 - loss: 1.8873 - acc: 0.4848 - LRFinder: val_loss: 3.8081 - lr = 0.00253590 \n",
            " 37/390 [=>............................] - ETA: 1:10 - loss: 1.8900 - acc: 0.4818 - LRFinder: val_loss: 3.7911 - lr = 0.00255692 \n",
            " 38/390 [=>............................] - ETA: 1:09 - loss: 1.8909 - acc: 0.4799 - LRFinder: val_loss: 3.7360 - lr = 0.00257795 \n",
            " 39/390 [==>...........................] - ETA: 1:08 - loss: 1.8927 - acc: 0.4768 - LRFinder: val_loss: 3.8143 - lr = 0.00259897 \n",
            " 40/390 [==>...........................] - ETA: 1:08 - loss: 1.8932 - acc: 0.4766 - LRFinder: val_loss: 3.5583 - lr = 0.00262000 \n",
            " 41/390 [==>...........................] - ETA: 1:07 - loss: 1.8900 - acc: 0.4788 - LRFinder: val_loss: 3.6467 - lr = 0.00264103 \n",
            " 42/390 [==>...........................] - ETA: 1:07 - loss: 1.8908 - acc: 0.4788 - LRFinder: val_loss: 3.7379 - lr = 0.00266205 \n",
            " 43/390 [==>...........................] - ETA: 1:06 - loss: 1.8903 - acc: 0.4786 - LRFinder: val_loss: 3.7001 - lr = 0.00268308 \n",
            " 44/390 [==>...........................] - ETA: 1:05 - loss: 1.8930 - acc: 0.4783 - LRFinder: val_loss: 3.8723 - lr = 0.00270410 \n",
            " 45/390 [==>...........................] - ETA: 1:05 - loss: 1.8951 - acc: 0.4786 - LRFinder: val_loss: 3.8695 - lr = 0.00272513 \n",
            " 46/390 [==>...........................] - ETA: 1:04 - loss: 1.8971 - acc: 0.4781 - LRFinder: val_loss: 3.7614 - lr = 0.00274615 \n",
            " 47/390 [==>...........................] - ETA: 1:04 - loss: 1.8971 - acc: 0.4791 - LRFinder: val_loss: 3.9789 - lr = 0.00276718 \n",
            " 48/390 [==>...........................] - ETA: 1:03 - loss: 1.8961 - acc: 0.4805 - LRFinder: val_loss: 3.8298 - lr = 0.00278821 \n",
            " 49/390 [==>...........................] - ETA: 1:03 - loss: 1.8953 - acc: 0.4817 - LRFinder: val_loss: 3.8324 - lr = 0.00280923 \n",
            " 50/390 [==>...........................] - ETA: 1:02 - loss: 1.8939 - acc: 0.4813 - LRFinder: val_loss: 3.8737 - lr = 0.00283026 \n",
            " 51/390 [==>...........................] - ETA: 1:02 - loss: 1.8957 - acc: 0.4798 - LRFinder: val_loss: 3.7860 - lr = 0.00285128 \n",
            " 52/390 [===>..........................] - ETA: 1:01 - loss: 1.8929 - acc: 0.4790 - LRFinder: val_loss: 3.8089 - lr = 0.00287231 \n",
            " 53/390 [===>..........................] - ETA: 1:01 - loss: 1.8949 - acc: 0.4780 - LRFinder: val_loss: 3.7760 - lr = 0.00289333 \n",
            " 54/390 [===>..........................] - ETA: 1:01 - loss: 1.8947 - acc: 0.4773 - LRFinder: val_loss: 3.7066 - lr = 0.00291436 \n",
            " 55/390 [===>..........................] - ETA: 1:00 - loss: 1.8973 - acc: 0.4761 - LRFinder: val_loss: 3.6421 - lr = 0.00293538 \n",
            " 56/390 [===>..........................] - ETA: 1:00 - loss: 1.8989 - acc: 0.4764 - LRFinder: val_loss: 3.5962 - lr = 0.00295641 \n",
            " 57/390 [===>..........................] - ETA: 59s - loss: 1.9004 - acc: 0.4746  - LRFinder: val_loss: 3.6239 - lr = 0.00297744 \n",
            " 58/390 [===>..........................] - ETA: 59s - loss: 1.8998 - acc: 0.4755 - LRFinder: val_loss: 3.6815 - lr = 0.00299846 \n",
            " 59/390 [===>..........................] - ETA: 59s - loss: 1.8993 - acc: 0.4759 - LRFinder: val_loss: 3.5919 - lr = 0.00301949 \n",
            " 60/390 [===>..........................] - ETA: 58s - loss: 1.8976 - acc: 0.4767 - LRFinder: val_loss: 3.5147 - lr = 0.00304051 \n",
            " 61/390 [===>..........................] - ETA: 58s - loss: 1.9003 - acc: 0.4764 - LRFinder: val_loss: 3.3906 - lr = 0.00306154 \n",
            " 62/390 [===>..........................] - ETA: 57s - loss: 1.8987 - acc: 0.4776 - LRFinder: val_loss: 3.3933 - lr = 0.00308256 \n",
            " 63/390 [===>..........................] - ETA: 57s - loss: 1.8977 - acc: 0.4788 - LRFinder: val_loss: 3.4495 - lr = 0.00310359 \n",
            " 64/390 [===>..........................] - ETA: 57s - loss: 1.8970 - acc: 0.4786 - LRFinder: val_loss: 3.3807 - lr = 0.00312462 \n",
            " 65/390 [====>.........................] - ETA: 56s - loss: 1.8943 - acc: 0.4803 - LRFinder: val_loss: 3.5663 - lr = 0.00314564 \n",
            " 66/390 [====>.........................] - ETA: 56s - loss: 1.8934 - acc: 0.4806 - LRFinder: val_loss: 3.4511 - lr = 0.00316667 \n",
            " 67/390 [====>.........................] - ETA: 56s - loss: 1.8944 - acc: 0.4795 - LRFinder: val_loss: 3.5096 - lr = 0.00318769 \n",
            " 68/390 [====>.........................] - ETA: 55s - loss: 1.8931 - acc: 0.4804 - LRFinder: val_loss: 3.6996 - lr = 0.00320872 \n",
            " 69/390 [====>.........................] - ETA: 55s - loss: 1.8931 - acc: 0.4803 - LRFinder: val_loss: 3.6532 - lr = 0.00322974 \n",
            " 70/390 [====>.........................] - ETA: 55s - loss: 1.8951 - acc: 0.4796 - LRFinder: val_loss: 3.6751 - lr = 0.00325077 \n",
            " 71/390 [====>.........................] - ETA: 54s - loss: 1.8954 - acc: 0.4784 - LRFinder: val_loss: 3.6961 - lr = 0.00327179 \n",
            " 72/390 [====>.........................] - ETA: 54s - loss: 1.8972 - acc: 0.4783 - LRFinder: val_loss: 3.6753 - lr = 0.00329282 \n",
            " 73/390 [====>.........................] - ETA: 54s - loss: 1.8972 - acc: 0.4787 - LRFinder: val_loss: 3.5490 - lr = 0.00331385 \n",
            " 74/390 [====>.........................] - ETA: 54s - loss: 1.8954 - acc: 0.4793 - LRFinder: val_loss: 3.5307 - lr = 0.00333487 \n",
            " 75/390 [====>.........................] - ETA: 53s - loss: 1.8930 - acc: 0.4809 - LRFinder: val_loss: 3.5673 - lr = 0.00335590 \n",
            " 76/390 [====>.........................] - ETA: 53s - loss: 1.8921 - acc: 0.4805 - LRFinder: val_loss: 3.4272 - lr = 0.00337692 \n",
            " 77/390 [====>.........................] - ETA: 53s - loss: 1.8937 - acc: 0.4796 - LRFinder: val_loss: 3.5591 - lr = 0.00339795 \n",
            " 78/390 [=====>........................] - ETA: 52s - loss: 1.8945 - acc: 0.4800 - LRFinder: val_loss: 3.4585 - lr = 0.00341897 \n",
            " 79/390 [=====>........................] - ETA: 52s - loss: 1.8942 - acc: 0.4801 - LRFinder: val_loss: 3.5523 - lr = 0.00344000 \n",
            " 80/390 [=====>........................] - ETA: 52s - loss: 1.8935 - acc: 0.4802 - LRFinder: val_loss: 3.6550 - lr = 0.00346103 \n",
            " 81/390 [=====>........................] - ETA: 52s - loss: 1.8933 - acc: 0.4806 - LRFinder: val_loss: 3.5993 - lr = 0.00348205 \n",
            " 82/390 [=====>........................] - ETA: 51s - loss: 1.8925 - acc: 0.4802 - LRFinder: val_loss: 3.7298 - lr = 0.00350308 \n",
            " 83/390 [=====>........................] - ETA: 51s - loss: 1.8924 - acc: 0.4808 - LRFinder: val_loss: 3.5989 - lr = 0.00352410 \n",
            " 84/390 [=====>........................] - ETA: 51s - loss: 1.8928 - acc: 0.4807 - LRFinder: val_loss: 3.6303 - lr = 0.00354513 \n",
            " 85/390 [=====>........................] - ETA: 51s - loss: 1.8935 - acc: 0.4805 - LRFinder: val_loss: 3.6234 - lr = 0.00356615 \n",
            " 86/390 [=====>........................] - ETA: 50s - loss: 1.8929 - acc: 0.4817 - LRFinder: val_loss: 3.6216 - lr = 0.00358718 \n",
            " 87/390 [=====>........................] - ETA: 50s - loss: 1.8920 - acc: 0.4820 - LRFinder: val_loss: 3.6665 - lr = 0.00360821 \n",
            " 88/390 [=====>........................] - ETA: 50s - loss: 1.8908 - acc: 0.4821 - LRFinder: val_loss: 3.5300 - lr = 0.00362923 \n",
            " 89/390 [=====>........................] - ETA: 50s - loss: 1.8903 - acc: 0.4828 - LRFinder: val_loss: 3.6069 - lr = 0.00365026 \n",
            " 90/390 [=====>........................] - ETA: 49s - loss: 1.8886 - acc: 0.4833 - LRFinder: val_loss: 3.5891 - lr = 0.00367128 \n",
            " 91/390 [======>.......................] - ETA: 49s - loss: 1.8889 - acc: 0.4832 - LRFinder: val_loss: 3.4646 - lr = 0.00369231 \n",
            " 92/390 [======>.......................] - ETA: 49s - loss: 1.8908 - acc: 0.4828 - LRFinder: val_loss: 3.3126 - lr = 0.00371333 \n",
            " 93/390 [======>.......................] - ETA: 49s - loss: 1.8892 - acc: 0.4836 - LRFinder: val_loss: 3.5610 - lr = 0.00373436 \n",
            " 94/390 [======>.......................] - ETA: 48s - loss: 1.8887 - acc: 0.4840 - LRFinder: val_loss: 3.4395 - lr = 0.00375538 \n",
            " 95/390 [======>.......................] - ETA: 48s - loss: 1.8876 - acc: 0.4845 - LRFinder: val_loss: 3.5372 - lr = 0.00377641 \n",
            " 96/390 [======>.......................] - ETA: 48s - loss: 1.8863 - acc: 0.4849 - LRFinder: val_loss: 3.5920 - lr = 0.00379744 \n",
            " 97/390 [======>.......................] - ETA: 48s - loss: 1.8879 - acc: 0.4845 - LRFinder: val_loss: 3.5490 - lr = 0.00381846 \n",
            " 98/390 [======>.......................] - ETA: 48s - loss: 1.8873 - acc: 0.4847 - LRFinder: val_loss: 3.5517 - lr = 0.00383949 \n",
            " 99/390 [======>.......................] - ETA: 47s - loss: 1.8873 - acc: 0.4853 - LRFinder: val_loss: 3.5803 - lr = 0.00386051 \n",
            "100/390 [======>.......................] - ETA: 47s - loss: 1.8881 - acc: 0.4848 - LRFinder: val_loss: 3.6153 - lr = 0.00388154 \n",
            "101/390 [======>.......................] - ETA: 47s - loss: 1.8876 - acc: 0.4857 - LRFinder: val_loss: 3.6019 - lr = 0.00390256 \n",
            "102/390 [======>.......................] - ETA: 47s - loss: 1.8881 - acc: 0.4854 - LRFinder: val_loss: 3.5931 - lr = 0.00392359 \n",
            "103/390 [======>.......................] - ETA: 46s - loss: 1.8875 - acc: 0.4858 - LRFinder: val_loss: 3.6822 - lr = 0.00394462 \n",
            "104/390 [=======>......................] - ETA: 46s - loss: 1.8870 - acc: 0.4857 - LRFinder: val_loss: 3.5522 - lr = 0.00396564 \n",
            "105/390 [=======>......................] - ETA: 46s - loss: 1.8876 - acc: 0.4851 - LRFinder: val_loss: 3.7104 - lr = 0.00398667 \n",
            "106/390 [=======>......................] - ETA: 46s - loss: 1.8885 - acc: 0.4845 - LRFinder: val_loss: 3.6860 - lr = 0.00400769 \n",
            "107/390 [=======>......................] - ETA: 46s - loss: 1.8886 - acc: 0.4844 - LRFinder: val_loss: 3.8317 - lr = 0.00402872 \n",
            "108/390 [=======>......................] - ETA: 45s - loss: 1.8886 - acc: 0.4839 - LRFinder: val_loss: 3.6719 - lr = 0.00404974 \n",
            "109/390 [=======>......................] - ETA: 45s - loss: 1.8879 - acc: 0.4845 - LRFinder: val_loss: 3.7279 - lr = 0.00407077 \n",
            "110/390 [=======>......................] - ETA: 45s - loss: 1.8860 - acc: 0.4850 - LRFinder: val_loss: 3.6058 - lr = 0.00409179 \n",
            "111/390 [=======>......................] - ETA: 45s - loss: 1.8858 - acc: 0.4852 - LRFinder: val_loss: 3.7043 - lr = 0.00411282 \n",
            "112/390 [=======>......................] - ETA: 45s - loss: 1.8856 - acc: 0.4848 - LRFinder: val_loss: 3.6045 - lr = 0.00413385 \n",
            "113/390 [=======>......................] - ETA: 44s - loss: 1.8871 - acc: 0.4840 - LRFinder: val_loss: 3.6403 - lr = 0.00415487 \n",
            "114/390 [=======>......................] - ETA: 44s - loss: 1.8862 - acc: 0.4842 - LRFinder: val_loss: 3.5377 - lr = 0.00417590 \n",
            "115/390 [=======>......................] - ETA: 44s - loss: 1.8856 - acc: 0.4844 - LRFinder: val_loss: 3.5307 - lr = 0.00419692 \n",
            "116/390 [=======>......................] - ETA: 44s - loss: 1.8854 - acc: 0.4843 - LRFinder: val_loss: 3.5910 - lr = 0.00421795 \n",
            "117/390 [========>.....................] - ETA: 43s - loss: 1.8855 - acc: 0.4842 - LRFinder: val_loss: 3.3788 - lr = 0.00423897 \n",
            "118/390 [========>.....................] - ETA: 43s - loss: 1.8847 - acc: 0.4843 - LRFinder: val_loss: 3.4920 - lr = 0.00426000 \n",
            "119/390 [========>.....................] - ETA: 43s - loss: 1.8862 - acc: 0.4835 - LRFinder: val_loss: 3.4382 - lr = 0.00428103 \n",
            "120/390 [========>.....................] - ETA: 43s - loss: 1.8863 - acc: 0.4835 - LRFinder: val_loss: 3.4882 - lr = 0.00430205 \n",
            "121/390 [========>.....................] - ETA: 43s - loss: 1.8845 - acc: 0.4842 - LRFinder: val_loss: 3.6613 - lr = 0.00432308 \n",
            "122/390 [========>.....................] - ETA: 42s - loss: 1.8839 - acc: 0.4838 - LRFinder: val_loss: 3.7599 - lr = 0.00434410 \n",
            "123/390 [========>.....................] - ETA: 42s - loss: 1.8845 - acc: 0.4832 - LRFinder: val_loss: 3.7318 - lr = 0.00436513 \n",
            "124/390 [========>.....................] - ETA: 42s - loss: 1.8841 - acc: 0.4833 - LRFinder: val_loss: 3.6924 - lr = 0.00438615 \n",
            "125/390 [========>.....................] - ETA: 42s - loss: 1.8852 - acc: 0.4831 - LRFinder: val_loss: 3.4479 - lr = 0.00440718 \n",
            "126/390 [========>.....................] - ETA: 42s - loss: 1.8852 - acc: 0.4833 - LRFinder: val_loss: 3.6274 - lr = 0.00442821 \n",
            "127/390 [========>.....................] - ETA: 42s - loss: 1.8853 - acc: 0.4835 - LRFinder: val_loss: 3.7139 - lr = 0.00444923 \n",
            "128/390 [========>.....................] - ETA: 41s - loss: 1.8848 - acc: 0.4835 - LRFinder: val_loss: 3.6058 - lr = 0.00447026 \n",
            "129/390 [========>.....................] - ETA: 41s - loss: 1.8855 - acc: 0.4835 - LRFinder: val_loss: 3.5867 - lr = 0.00449128 \n",
            "130/390 [=========>....................] - ETA: 41s - loss: 1.8854 - acc: 0.4834 - LRFinder: val_loss: 3.5098 - lr = 0.00451231 \n",
            "131/390 [=========>....................] - ETA: 41s - loss: 1.8847 - acc: 0.4836 - LRFinder: val_loss: 3.3929 - lr = 0.00453333 \n",
            "132/390 [=========>....................] - ETA: 41s - loss: 1.8850 - acc: 0.4834 - LRFinder: val_loss: 3.2364 - lr = 0.00455436 \n",
            "133/390 [=========>....................] - ETA: 40s - loss: 1.8846 - acc: 0.4836 - LRFinder: val_loss: 3.2497 - lr = 0.00457538 \n",
            "134/390 [=========>....................] - ETA: 40s - loss: 1.8848 - acc: 0.4836 - LRFinder: val_loss: 3.3071 - lr = 0.00459641 \n",
            "135/390 [=========>....................] - ETA: 40s - loss: 1.8855 - acc: 0.4834 - LRFinder: val_loss: 3.4008 - lr = 0.00461744 \n",
            "136/390 [=========>....................] - ETA: 40s - loss: 1.8846 - acc: 0.4839 - LRFinder: val_loss: 3.3983 - lr = 0.00463846 \n",
            "137/390 [=========>....................] - ETA: 40s - loss: 1.8837 - acc: 0.4840 - LRFinder: val_loss: 3.3056 - lr = 0.00465949 \n",
            "138/390 [=========>....................] - ETA: 39s - loss: 1.8842 - acc: 0.4840 - LRFinder: val_loss: 3.3625 - lr = 0.00468051 \n",
            "139/390 [=========>....................] - ETA: 39s - loss: 1.8857 - acc: 0.4836 - LRFinder: val_loss: 3.5487 - lr = 0.00470154 \n",
            "140/390 [=========>....................] - ETA: 39s - loss: 1.8846 - acc: 0.4836 - LRFinder: val_loss: 3.5888 - lr = 0.00472256 \n",
            "141/390 [=========>....................] - ETA: 39s - loss: 1.8845 - acc: 0.4840 - LRFinder: val_loss: 3.5598 - lr = 0.00474359 \n",
            "142/390 [=========>....................] - ETA: 39s - loss: 1.8846 - acc: 0.4840 - LRFinder: val_loss: 3.4644 - lr = 0.00476462 \n",
            "143/390 [==========>...................] - ETA: 39s - loss: 1.8845 - acc: 0.4842 - LRFinder: val_loss: 3.4432 - lr = 0.00478564 \n",
            "144/390 [==========>...................] - ETA: 38s - loss: 1.8843 - acc: 0.4840 - LRFinder: val_loss: 3.3706 - lr = 0.00480667 \n",
            "145/390 [==========>...................] - ETA: 38s - loss: 1.8840 - acc: 0.4841 - LRFinder: val_loss: 3.2919 - lr = 0.00482769 \n",
            "146/390 [==========>...................] - ETA: 38s - loss: 1.8836 - acc: 0.4843 - LRFinder: val_loss: 3.2837 - lr = 0.00484872 \n",
            "147/390 [==========>...................] - ETA: 38s - loss: 1.8837 - acc: 0.4844 - LRFinder: val_loss: 3.3765 - lr = 0.00486974 \n",
            "148/390 [==========>...................] - ETA: 38s - loss: 1.8839 - acc: 0.4846 - LRFinder: val_loss: 3.2390 - lr = 0.00489077 \n",
            "149/390 [==========>...................] - ETA: 37s - loss: 1.8832 - acc: 0.4848 - LRFinder: val_loss: 3.3469 - lr = 0.00491179 \n",
            "150/390 [==========>...................] - ETA: 37s - loss: 1.8824 - acc: 0.4852 - LRFinder: val_loss: 3.3564 - lr = 0.00493282 \n",
            "151/390 [==========>...................] - ETA: 37s - loss: 1.8821 - acc: 0.4854 - LRFinder: val_loss: 3.3794 - lr = 0.00495385 \n",
            "152/390 [==========>...................] - ETA: 37s - loss: 1.8815 - acc: 0.4852 - LRFinder: val_loss: 3.5371 - lr = 0.00497487 \n",
            "153/390 [==========>...................] - ETA: 37s - loss: 1.8809 - acc: 0.4857 - LRFinder: val_loss: 3.5177 - lr = 0.00499590 \n",
            "154/390 [==========>...................] - ETA: 37s - loss: 1.8806 - acc: 0.4855 - LRFinder: val_loss: 3.5327 - lr = 0.00501692 \n",
            "155/390 [==========>...................] - ETA: 36s - loss: 1.8799 - acc: 0.4859 - LRFinder: val_loss: 3.5491 - lr = 0.00503795 \n",
            "156/390 [===========>..................] - ETA: 36s - loss: 1.8800 - acc: 0.4857 - LRFinder: val_loss: 3.5712 - lr = 0.00505897 \n",
            "157/390 [===========>..................] - ETA: 36s - loss: 1.8791 - acc: 0.4858 - LRFinder: val_loss: 3.4455 - lr = 0.00508000 \n",
            "158/390 [===========>..................] - ETA: 36s - loss: 1.8790 - acc: 0.4858 - LRFinder: val_loss: 3.3815 - lr = 0.00510103 \n",
            "159/390 [===========>..................] - ETA: 36s - loss: 1.8789 - acc: 0.4859 - LRFinder: val_loss: 3.3990 - lr = 0.00512205 \n",
            "160/390 [===========>..................] - ETA: 36s - loss: 1.8796 - acc: 0.4857 - LRFinder: val_loss: 3.5697 - lr = 0.00514308 \n",
            "161/390 [===========>..................] - ETA: 35s - loss: 1.8802 - acc: 0.4855 - LRFinder: val_loss: 3.4573 - lr = 0.00516410 \n",
            "162/390 [===========>..................] - ETA: 35s - loss: 1.8792 - acc: 0.4859 - LRFinder: val_loss: 3.4681 - lr = 0.00518513 \n",
            "163/390 [===========>..................] - ETA: 35s - loss: 1.8788 - acc: 0.4861 - LRFinder: val_loss: 3.4647 - lr = 0.00520615 \n",
            "164/390 [===========>..................] - ETA: 35s - loss: 1.8789 - acc: 0.4864 - LRFinder: val_loss: 3.6379 - lr = 0.00522718 \n",
            "165/390 [===========>..................] - ETA: 35s - loss: 1.8787 - acc: 0.4866 - LRFinder: val_loss: 3.5998 - lr = 0.00524821 \n",
            "166/390 [===========>..................] - ETA: 34s - loss: 1.8783 - acc: 0.4868 - LRFinder: val_loss: 3.5797 - lr = 0.00526923 \n",
            "167/390 [===========>..................] - ETA: 34s - loss: 1.8777 - acc: 0.4869 - LRFinder: val_loss: 3.4338 - lr = 0.00529026 \n",
            "168/390 [===========>..................] - ETA: 34s - loss: 1.8775 - acc: 0.4870 - LRFinder: val_loss: 3.5147 - lr = 0.00531128 \n",
            "169/390 [============>.................] - ETA: 34s - loss: 1.8775 - acc: 0.4870 - LRFinder: val_loss: 3.4655 - lr = 0.00533231 \n",
            "170/390 [============>.................] - ETA: 34s - loss: 1.8771 - acc: 0.4872 - LRFinder: val_loss: 3.3982 - lr = 0.00535333 \n",
            "171/390 [============>.................] - ETA: 34s - loss: 1.8781 - acc: 0.4869 - LRFinder: val_loss: 3.6294 - lr = 0.00537436 \n",
            "172/390 [============>.................] - ETA: 33s - loss: 1.8779 - acc: 0.4875 - LRFinder: val_loss: 3.5981 - lr = 0.00539538 \n",
            "173/390 [============>.................] - ETA: 33s - loss: 1.8783 - acc: 0.4873 - LRFinder: val_loss: 3.4816 - lr = 0.00541641 \n",
            "174/390 [============>.................] - ETA: 33s - loss: 1.8777 - acc: 0.4874 - LRFinder: val_loss: 3.5691 - lr = 0.00543744 \n",
            "175/390 [============>.................] - ETA: 33s - loss: 1.8774 - acc: 0.4878 - LRFinder: val_loss: 3.5592 - lr = 0.00545846 \n",
            "176/390 [============>.................] - ETA: 33s - loss: 1.8782 - acc: 0.4877 - LRFinder: val_loss: 3.4689 - lr = 0.00547949 \n",
            "177/390 [============>.................] - ETA: 33s - loss: 1.8781 - acc: 0.4877 - LRFinder: val_loss: 3.5617 - lr = 0.00550051 \n",
            "178/390 [============>.................] - ETA: 32s - loss: 1.8779 - acc: 0.4874 - LRFinder: val_loss: 3.8301 - lr = 0.00552154 \n",
            "179/390 [============>.................] - ETA: 32s - loss: 1.8777 - acc: 0.4875 - LRFinder: val_loss: 3.6992 - lr = 0.00554256 \n",
            "180/390 [============>.................] - ETA: 32s - loss: 1.8773 - acc: 0.4872 - LRFinder: val_loss: 3.8385 - lr = 0.00556359 \n",
            "181/390 [============>.................] - ETA: 32s - loss: 1.8776 - acc: 0.4870 - LRFinder: val_loss: 3.8497 - lr = 0.00558462 \n",
            "182/390 [=============>................] - ETA: 32s - loss: 1.8780 - acc: 0.4869 - LRFinder: val_loss: 3.6570 - lr = 0.00560564 \n",
            "183/390 [=============>................] - ETA: 32s - loss: 1.8780 - acc: 0.4871 - LRFinder: val_loss: 3.6026 - lr = 0.00562667 \n",
            "184/390 [=============>................] - ETA: 31s - loss: 1.8781 - acc: 0.4873 - LRFinder: val_loss: 3.6180 - lr = 0.00564769 \n",
            "185/390 [=============>................] - ETA: 31s - loss: 1.8776 - acc: 0.4876 - LRFinder: val_loss: 3.7899 - lr = 0.00566872 \n",
            "186/390 [=============>................] - ETA: 31s - loss: 1.8774 - acc: 0.4877 - LRFinder: val_loss: 3.7311 - lr = 0.00568974 \n",
            "187/390 [=============>................] - ETA: 31s - loss: 1.8770 - acc: 0.4877 - LRFinder: val_loss: 3.9408 - lr = 0.00571077 \n",
            "188/390 [=============>................] - ETA: 31s - loss: 1.8769 - acc: 0.4879 - LRFinder: val_loss: 3.7657 - lr = 0.00573179 \n",
            "189/390 [=============>................] - ETA: 31s - loss: 1.8775 - acc: 0.4878 - LRFinder: val_loss: 3.8597 - lr = 0.00575282 \n",
            "190/390 [=============>................] - ETA: 30s - loss: 1.8774 - acc: 0.4878 - LRFinder: val_loss: 3.7773 - lr = 0.00577385 \n",
            "191/390 [=============>................] - ETA: 30s - loss: 1.8773 - acc: 0.4878 - LRFinder: val_loss: 3.7906 - lr = 0.00579487 \n",
            "192/390 [=============>................] - ETA: 30s - loss: 1.8770 - acc: 0.4879 - LRFinder: val_loss: 4.0044 - lr = 0.00581590 \n",
            "193/390 [=============>................] - ETA: 30s - loss: 1.8762 - acc: 0.4883 - LRFinder: val_loss: 3.8809 - lr = 0.00583692 \n",
            "194/390 [=============>................] - ETA: 30s - loss: 1.8770 - acc: 0.4880 - LRFinder: val_loss: 3.7997 - lr = 0.00585795 \n",
            "195/390 [==============>...............] - ETA: 30s - loss: 1.8765 - acc: 0.4882 - LRFinder: val_loss: 3.8680 - lr = 0.00587897 \n",
            "196/390 [==============>...............] - ETA: 29s - loss: 1.8758 - acc: 0.4885 - LRFinder: val_loss: 4.0631 - lr = 0.00590000 \n",
            "197/390 [==============>...............] - ETA: 29s - loss: 1.8759 - acc: 0.4886 - LRFinder: val_loss: 3.9536 - lr = 0.00592103 \n",
            "198/390 [==============>...............] - ETA: 29s - loss: 1.8760 - acc: 0.4885 - LRFinder: val_loss: 4.0090 - lr = 0.00594205 \n",
            "199/390 [==============>...............] - ETA: 29s - loss: 1.8761 - acc: 0.4882 - LRFinder: val_loss: 4.0026 - lr = 0.00596308 \n",
            "200/390 [==============>...............] - ETA: 29s - loss: 1.8764 - acc: 0.4879 - LRFinder: val_loss: 4.1330 - lr = 0.00598410 \n",
            "201/390 [==============>...............] - ETA: 29s - loss: 1.8766 - acc: 0.4874 - LRFinder: val_loss: 3.9468 - lr = 0.00600513 \n",
            "202/390 [==============>...............] - ETA: 28s - loss: 1.8756 - acc: 0.4877 - LRFinder: val_loss: 4.0606 - lr = 0.00602615 \n",
            "203/390 [==============>...............] - ETA: 28s - loss: 1.8748 - acc: 0.4878 - LRFinder: val_loss: 4.2720 - lr = 0.00604718 \n",
            "204/390 [==============>...............] - ETA: 28s - loss: 1.8741 - acc: 0.4881 - LRFinder: val_loss: 4.2813 - lr = 0.00606821 \n",
            "205/390 [==============>...............] - ETA: 28s - loss: 1.8745 - acc: 0.4877 - LRFinder: val_loss: 3.9593 - lr = 0.00608923 \n",
            "206/390 [==============>...............] - ETA: 28s - loss: 1.8745 - acc: 0.4878 - LRFinder: val_loss: 4.1110 - lr = 0.00611026 \n",
            "207/390 [==============>...............] - ETA: 28s - loss: 1.8743 - acc: 0.4880 - LRFinder: val_loss: 4.0646 - lr = 0.00613128 \n",
            "208/390 [===============>..............] - ETA: 28s - loss: 1.8746 - acc: 0.4881 - LRFinder: val_loss: 3.8777 - lr = 0.00615231 \n",
            "209/390 [===============>..............] - ETA: 27s - loss: 1.8744 - acc: 0.4882 - LRFinder: val_loss: 4.0485 - lr = 0.00617333 \n",
            "210/390 [===============>..............] - ETA: 27s - loss: 1.8741 - acc: 0.4881 - LRFinder: val_loss: 4.0831 - lr = 0.00619436 \n",
            "211/390 [===============>..............] - ETA: 27s - loss: 1.8736 - acc: 0.4886 - LRFinder: val_loss: 3.9729 - lr = 0.00621538 \n",
            "212/390 [===============>..............] - ETA: 27s - loss: 1.8735 - acc: 0.4886 - LRFinder: val_loss: 4.0102 - lr = 0.00623641 \n",
            "213/390 [===============>..............] - ETA: 27s - loss: 1.8731 - acc: 0.4888 - LRFinder: val_loss: 4.2196 - lr = 0.00625744 \n",
            "214/390 [===============>..............] - ETA: 27s - loss: 1.8733 - acc: 0.4885 - LRFinder: val_loss: 4.2199 - lr = 0.00627846 \n",
            "215/390 [===============>..............] - ETA: 26s - loss: 1.8734 - acc: 0.4884 - LRFinder: val_loss: 3.9880 - lr = 0.00629949 \n",
            "216/390 [===============>..............] - ETA: 26s - loss: 1.8731 - acc: 0.4885 - LRFinder: val_loss: 4.0768 - lr = 0.00632051 \n",
            "217/390 [===============>..............] - ETA: 26s - loss: 1.8726 - acc: 0.4888 - LRFinder: val_loss: 4.1308 - lr = 0.00634154 \n",
            "218/390 [===============>..............] - ETA: 26s - loss: 1.8722 - acc: 0.4889 - LRFinder: val_loss: 4.1208 - lr = 0.00636256 \n",
            "219/390 [===============>..............] - ETA: 26s - loss: 1.8716 - acc: 0.4889 - LRFinder: val_loss: 4.2045 - lr = 0.00638359 \n",
            "220/390 [===============>..............] - ETA: 26s - loss: 1.8712 - acc: 0.4893 - LRFinder: val_loss: 4.0914 - lr = 0.00640462 \n",
            "221/390 [================>.............] - ETA: 25s - loss: 1.8715 - acc: 0.4891 - LRFinder: val_loss: 4.0531 - lr = 0.00642564 \n",
            "222/390 [================>.............] - ETA: 25s - loss: 1.8714 - acc: 0.4892 - LRFinder: val_loss: 4.2125 - lr = 0.00644667 \n",
            "223/390 [================>.............] - ETA: 25s - loss: 1.8706 - acc: 0.4894 - LRFinder: val_loss: 4.1393 - lr = 0.00646769 \n",
            "224/390 [================>.............] - ETA: 25s - loss: 1.8706 - acc: 0.4894 - LRFinder: val_loss: 4.2790 - lr = 0.00648872 \n",
            "225/390 [================>.............] - ETA: 25s - loss: 1.8710 - acc: 0.4892 - LRFinder: val_loss: 4.2447 - lr = 0.00650974 \n",
            "226/390 [================>.............] - ETA: 25s - loss: 1.8713 - acc: 0.4889 - LRFinder: val_loss: 4.2855 - lr = 0.00653077 \n",
            "227/390 [================>.............] - ETA: 24s - loss: 1.8704 - acc: 0.4892 - LRFinder: val_loss: 4.0589 - lr = 0.00655179 \n",
            "228/390 [================>.............] - ETA: 24s - loss: 1.8704 - acc: 0.4891 - LRFinder: val_loss: 3.9285 - lr = 0.00657282 \n",
            "229/390 [================>.............] - ETA: 24s - loss: 1.8699 - acc: 0.4893 - LRFinder: val_loss: 4.0350 - lr = 0.00659385 \n",
            "230/390 [================>.............] - ETA: 24s - loss: 1.8693 - acc: 0.4896 - LRFinder: val_loss: 3.8823 - lr = 0.00661487 \n",
            "231/390 [================>.............] - ETA: 24s - loss: 1.8689 - acc: 0.4898 - LRFinder: val_loss: 4.0808 - lr = 0.00663590 \n",
            "232/390 [================>.............] - ETA: 24s - loss: 1.8682 - acc: 0.4900 - LRFinder: val_loss: 3.9463 - lr = 0.00665692 \n",
            "233/390 [================>.............] - ETA: 24s - loss: 1.8681 - acc: 0.4902 - LRFinder: val_loss: 3.7205 - lr = 0.00667795 \n",
            "234/390 [=================>............] - ETA: 23s - loss: 1.8690 - acc: 0.4898 - LRFinder: val_loss: 3.7420 - lr = 0.00669897 \n",
            "235/390 [=================>............] - ETA: 23s - loss: 1.8688 - acc: 0.4899 - LRFinder: val_loss: 3.6873 - lr = 0.00672000 \n",
            "236/390 [=================>............] - ETA: 23s - loss: 1.8677 - acc: 0.4905 - LRFinder: val_loss: 3.6938 - lr = 0.00674103 \n",
            "237/390 [=================>............] - ETA: 23s - loss: 1.8685 - acc: 0.4902 - LRFinder: val_loss: 3.8072 - lr = 0.00676205 \n",
            "238/390 [=================>............] - ETA: 23s - loss: 1.8687 - acc: 0.4901 - LRFinder: val_loss: 3.6414 - lr = 0.00678308 \n",
            "239/390 [=================>............] - ETA: 23s - loss: 1.8683 - acc: 0.4901 - LRFinder: val_loss: 3.5884 - lr = 0.00680410 \n",
            "240/390 [=================>............] - ETA: 22s - loss: 1.8681 - acc: 0.4902 - LRFinder: val_loss: 3.6736 - lr = 0.00682513 \n",
            "241/390 [=================>............] - ETA: 22s - loss: 1.8682 - acc: 0.4902 - LRFinder: val_loss: 3.8347 - lr = 0.00684615 \n",
            "242/390 [=================>............] - ETA: 22s - loss: 1.8686 - acc: 0.4902 - LRFinder: val_loss: 3.8891 - lr = 0.00686718 \n",
            "243/390 [=================>............] - ETA: 22s - loss: 1.8689 - acc: 0.4900 - LRFinder: val_loss: 3.7183 - lr = 0.00688821 \n",
            "244/390 [=================>............] - ETA: 22s - loss: 1.8692 - acc: 0.4902 - LRFinder: val_loss: 3.8443 - lr = 0.00690923 \n",
            "245/390 [=================>............] - ETA: 22s - loss: 1.8698 - acc: 0.4903 - LRFinder: val_loss: 4.0931 - lr = 0.00693026 \n",
            "246/390 [=================>............] - ETA: 21s - loss: 1.8693 - acc: 0.4904 - LRFinder: val_loss: 3.7404 - lr = 0.00695128 \n",
            "247/390 [==================>...........] - ETA: 21s - loss: 1.8692 - acc: 0.4905 - LRFinder: val_loss: 4.0174 - lr = 0.00697231 \n",
            "248/390 [==================>...........] - ETA: 21s - loss: 1.8685 - acc: 0.4909 - LRFinder: val_loss: 4.1295 - lr = 0.00699333 \n",
            "249/390 [==================>...........] - ETA: 21s - loss: 1.8683 - acc: 0.4910 - LRFinder: val_loss: 4.1803 - lr = 0.00701436 \n",
            "250/390 [==================>...........] - ETA: 21s - loss: 1.8679 - acc: 0.4914 - LRFinder: val_loss: 4.1863 - lr = 0.00703538 \n",
            "251/390 [==================>...........] - ETA: 21s - loss: 1.8678 - acc: 0.4913 - LRFinder: val_loss: 3.9439 - lr = 0.00705641 \n",
            "252/390 [==================>...........] - ETA: 21s - loss: 1.8680 - acc: 0.4911 - LRFinder: val_loss: 4.0386 - lr = 0.00707744 \n",
            "253/390 [==================>...........] - ETA: 20s - loss: 1.8682 - acc: 0.4908 - LRFinder: val_loss: 3.9765 - lr = 0.00709846 \n",
            "254/390 [==================>...........] - ETA: 20s - loss: 1.8680 - acc: 0.4909 - LRFinder: val_loss: 3.9962 - lr = 0.00711949 \n",
            "255/390 [==================>...........] - ETA: 20s - loss: 1.8677 - acc: 0.4910 - LRFinder: val_loss: 3.9841 - lr = 0.00714051 \n",
            "256/390 [==================>...........] - ETA: 20s - loss: 1.8680 - acc: 0.4907 - LRFinder: val_loss: 4.0155 - lr = 0.00716154 \n",
            "257/390 [==================>...........] - ETA: 20s - loss: 1.8675 - acc: 0.4908 - LRFinder: val_loss: 4.1242 - lr = 0.00718256 \n",
            "258/390 [==================>...........] - ETA: 20s - loss: 1.8671 - acc: 0.4909 - LRFinder: val_loss: 4.1678 - lr = 0.00720359 \n",
            "259/390 [==================>...........] - ETA: 19s - loss: 1.8665 - acc: 0.4913 - LRFinder: val_loss: 4.1226 - lr = 0.00722462 \n",
            "260/390 [===================>..........] - ETA: 19s - loss: 1.8665 - acc: 0.4915 - LRFinder: val_loss: 3.9562 - lr = 0.00724564 \n",
            "261/390 [===================>..........] - ETA: 19s - loss: 1.8657 - acc: 0.4919 - LRFinder: val_loss: 3.9393 - lr = 0.00726667 \n",
            "262/390 [===================>..........] - ETA: 19s - loss: 1.8652 - acc: 0.4919 - LRFinder: val_loss: 3.8023 - lr = 0.00728769 \n",
            "263/390 [===================>..........] - ETA: 19s - loss: 1.8654 - acc: 0.4919 - LRFinder: val_loss: 3.6067 - lr = 0.00730872 \n",
            "264/390 [===================>..........] - ETA: 19s - loss: 1.8651 - acc: 0.4920 - LRFinder: val_loss: 3.4935 - lr = 0.00732974 \n",
            "265/390 [===================>..........] - ETA: 18s - loss: 1.8646 - acc: 0.4920 - LRFinder: val_loss: 3.2719 - lr = 0.00735077 \n",
            "266/390 [===================>..........] - ETA: 18s - loss: 1.8643 - acc: 0.4922 - LRFinder: val_loss: 3.3818 - lr = 0.00737179 \n",
            "267/390 [===================>..........] - ETA: 18s - loss: 1.8639 - acc: 0.4926 - LRFinder: val_loss: 3.6460 - lr = 0.00739282 \n",
            "268/390 [===================>..........] - ETA: 18s - loss: 1.8636 - acc: 0.4927 - LRFinder: val_loss: 3.5743 - lr = 0.00741385 \n",
            "269/390 [===================>..........] - ETA: 18s - loss: 1.8634 - acc: 0.4927 - LRFinder: val_loss: 3.6038 - lr = 0.00743487 \n",
            "270/390 [===================>..........] - ETA: 18s - loss: 1.8637 - acc: 0.4927 - LRFinder: val_loss: 3.7176 - lr = 0.00745590 \n",
            "271/390 [===================>..........] - ETA: 18s - loss: 1.8636 - acc: 0.4928 - LRFinder: val_loss: 3.8130 - lr = 0.00747692 \n",
            "272/390 [===================>..........] - ETA: 17s - loss: 1.8630 - acc: 0.4929 - LRFinder: val_loss: 3.8285 - lr = 0.00749795 \n",
            "273/390 [====================>.........] - ETA: 17s - loss: 1.8629 - acc: 0.4929 - LRFinder: val_loss: 3.6521 - lr = 0.00751897 \n",
            "274/390 [====================>.........] - ETA: 17s - loss: 1.8632 - acc: 0.4928 - LRFinder: val_loss: 3.7286 - lr = 0.00754000 \n",
            "275/390 [====================>.........] - ETA: 17s - loss: 1.8629 - acc: 0.4928 - LRFinder: val_loss: 3.6022 - lr = 0.00756103 \n",
            "276/390 [====================>.........] - ETA: 17s - loss: 1.8628 - acc: 0.4930 - LRFinder: val_loss: 3.6423 - lr = 0.00758205 \n",
            "277/390 [====================>.........] - ETA: 17s - loss: 1.8632 - acc: 0.4928 - LRFinder: val_loss: 3.6938 - lr = 0.00760308 \n",
            "278/390 [====================>.........] - ETA: 16s - loss: 1.8625 - acc: 0.4931 - LRFinder: val_loss: 3.6603 - lr = 0.00762410 \n",
            "279/390 [====================>.........] - ETA: 16s - loss: 1.8623 - acc: 0.4933 - LRFinder: val_loss: 3.7339 - lr = 0.00764513 \n",
            "280/390 [====================>.........] - ETA: 16s - loss: 1.8622 - acc: 0.4932 - LRFinder: val_loss: 3.7561 - lr = 0.00766615 \n",
            "281/390 [====================>.........] - ETA: 16s - loss: 1.8621 - acc: 0.4932 - LRFinder: val_loss: 3.7571 - lr = 0.00768718 \n",
            "282/390 [====================>.........] - ETA: 16s - loss: 1.8624 - acc: 0.4932 - LRFinder: val_loss: 3.8126 - lr = 0.00770821 \n",
            "283/390 [====================>.........] - ETA: 16s - loss: 1.8624 - acc: 0.4932 - LRFinder: val_loss: 3.8600 - lr = 0.00772923 \n",
            "284/390 [====================>.........] - ETA: 16s - loss: 1.8622 - acc: 0.4933 - LRFinder: val_loss: 3.7455 - lr = 0.00775026 \n",
            "285/390 [====================>.........] - ETA: 15s - loss: 1.8622 - acc: 0.4933 - LRFinder: val_loss: 3.7784 - lr = 0.00777128 \n",
            "286/390 [=====================>........] - ETA: 15s - loss: 1.8620 - acc: 0.4933 - LRFinder: val_loss: 3.7471 - lr = 0.00779231 \n",
            "287/390 [=====================>........] - ETA: 15s - loss: 1.8621 - acc: 0.4932 - LRFinder: val_loss: 3.7617 - lr = 0.00781333 \n",
            "288/390 [=====================>........] - ETA: 15s - loss: 1.8619 - acc: 0.4933 - LRFinder: val_loss: 3.8615 - lr = 0.00783436 \n",
            "289/390 [=====================>........] - ETA: 15s - loss: 1.8615 - acc: 0.4933 - LRFinder: val_loss: 3.8597 - lr = 0.00785538 \n",
            "290/390 [=====================>........] - ETA: 15s - loss: 1.8616 - acc: 0.4931 - LRFinder: val_loss: 3.9081 - lr = 0.00787641 \n",
            "291/390 [=====================>........] - ETA: 14s - loss: 1.8607 - acc: 0.4933 - LRFinder: val_loss: 3.9240 - lr = 0.00789744 \n",
            "292/390 [=====================>........] - ETA: 14s - loss: 1.8603 - acc: 0.4934 - LRFinder: val_loss: 3.7038 - lr = 0.00791846 \n",
            "293/390 [=====================>........] - ETA: 14s - loss: 1.8604 - acc: 0.4935 - LRFinder: val_loss: 3.8120 - lr = 0.00793949 \n",
            "294/390 [=====================>........] - ETA: 14s - loss: 1.8601 - acc: 0.4937 - LRFinder: val_loss: 3.6978 - lr = 0.00796051 \n",
            "295/390 [=====================>........] - ETA: 14s - loss: 1.8604 - acc: 0.4936 - LRFinder: val_loss: 3.6892 - lr = 0.00798154 \n",
            "296/390 [=====================>........] - ETA: 14s - loss: 1.8604 - acc: 0.4935 - LRFinder: val_loss: 3.5441 - lr = 0.00800256 \n",
            "297/390 [=====================>........] - ETA: 14s - loss: 1.8601 - acc: 0.4936 - LRFinder: val_loss: 3.5327 - lr = 0.00802359 \n",
            "298/390 [=====================>........] - ETA: 13s - loss: 1.8598 - acc: 0.4939 - LRFinder: val_loss: 3.6084 - lr = 0.00804462 \n",
            "299/390 [======================>.......] - ETA: 13s - loss: 1.8597 - acc: 0.4937 - LRFinder: val_loss: 3.5964 - lr = 0.00806564 \n",
            "300/390 [======================>.......] - ETA: 13s - loss: 1.8599 - acc: 0.4935 - LRFinder: val_loss: 3.6937 - lr = 0.00808667 \n",
            "301/390 [======================>.......] - ETA: 13s - loss: 1.8597 - acc: 0.4937 - LRFinder: val_loss: 3.6556 - lr = 0.00810769 \n",
            "302/390 [======================>.......] - ETA: 13s - loss: 1.8595 - acc: 0.4937 - LRFinder: val_loss: 3.7784 - lr = 0.00812872 \n",
            "303/390 [======================>.......] - ETA: 13s - loss: 1.8591 - acc: 0.4938 - LRFinder: val_loss: 3.7180 - lr = 0.00814974 \n",
            "304/390 [======================>.......] - ETA: 12s - loss: 1.8584 - acc: 0.4939 - LRFinder: val_loss: 3.6302 - lr = 0.00817077 \n",
            "305/390 [======================>.......] - ETA: 12s - loss: 1.8581 - acc: 0.4941 - LRFinder: val_loss: 3.7338 - lr = 0.00819179 \n",
            "306/390 [======================>.......] - ETA: 12s - loss: 1.8577 - acc: 0.4943 - LRFinder: val_loss: 3.7182 - lr = 0.00821282 \n",
            "307/390 [======================>.......] - ETA: 12s - loss: 1.8576 - acc: 0.4944 - LRFinder: val_loss: 3.8677 - lr = 0.00823385 \n",
            "308/390 [======================>.......] - ETA: 12s - loss: 1.8574 - acc: 0.4945 - LRFinder: val_loss: 3.8229 - lr = 0.00825487 \n",
            "309/390 [======================>.......] - ETA: 12s - loss: 1.8568 - acc: 0.4946 - LRFinder: val_loss: 3.7510 - lr = 0.00827590 \n",
            "310/390 [======================>.......] - ETA: 12s - loss: 1.8566 - acc: 0.4948 - LRFinder: val_loss: 3.8149 - lr = 0.00829692 \n",
            "311/390 [======================>.......] - ETA: 11s - loss: 1.8567 - acc: 0.4947 - LRFinder: val_loss: 3.8078 - lr = 0.00831795 \n",
            "312/390 [=======================>......] - ETA: 11s - loss: 1.8568 - acc: 0.4947 - LRFinder: val_loss: 3.8499 - lr = 0.00833897 \n",
            "313/390 [=======================>......] - ETA: 11s - loss: 1.8566 - acc: 0.4948 - LRFinder: val_loss: 3.9397 - lr = 0.00836000 \n",
            "314/390 [=======================>......] - ETA: 11s - loss: 1.8566 - acc: 0.4948 - LRFinder: val_loss: 3.6812 - lr = 0.00838103 \n",
            "315/390 [=======================>......] - ETA: 11s - loss: 1.8562 - acc: 0.4951 - LRFinder: val_loss: 3.9058 - lr = 0.00840205 \n",
            "316/390 [=======================>......] - ETA: 11s - loss: 1.8563 - acc: 0.4951 - LRFinder: val_loss: 4.0794 - lr = 0.00842308 \n",
            "317/390 [=======================>......] - ETA: 11s - loss: 1.8562 - acc: 0.4950 - LRFinder: val_loss: 4.0803 - lr = 0.00844410 \n",
            "318/390 [=======================>......] - ETA: 10s - loss: 1.8562 - acc: 0.4952 - LRFinder: val_loss: 4.1447 - lr = 0.00846513 \n",
            "319/390 [=======================>......] - ETA: 10s - loss: 1.8565 - acc: 0.4951 - LRFinder: val_loss: 3.9699 - lr = 0.00848615 \n",
            "320/390 [=======================>......] - ETA: 10s - loss: 1.8562 - acc: 0.4951 - LRFinder: val_loss: 3.8987 - lr = 0.00850718 \n",
            "321/390 [=======================>......] - ETA: 10s - loss: 1.8561 - acc: 0.4952 - LRFinder: val_loss: 4.0310 - lr = 0.00852821 \n",
            "322/390 [=======================>......] - ETA: 10s - loss: 1.8555 - acc: 0.4954 - LRFinder: val_loss: 3.9496 - lr = 0.00854923 \n",
            "323/390 [=======================>......] - ETA: 10s - loss: 1.8555 - acc: 0.4954 - LRFinder: val_loss: 4.2175 - lr = 0.00857026 \n",
            "324/390 [=======================>......] - ETA: 9s - loss: 1.8554 - acc: 0.4955  - LRFinder: val_loss: 4.0207 - lr = 0.00859128 \n",
            "325/390 [========================>.....] - ETA: 9s - loss: 1.8552 - acc: 0.4955 - LRFinder: val_loss: 4.1681 - lr = 0.00861231 \n",
            "326/390 [========================>.....] - ETA: 9s - loss: 1.8552 - acc: 0.4956 - LRFinder: val_loss: 4.0526 - lr = 0.00863333 \n",
            "327/390 [========================>.....] - ETA: 9s - loss: 1.8551 - acc: 0.4955 - LRFinder: val_loss: 3.8557 - lr = 0.00865436 \n",
            "328/390 [========================>.....] - ETA: 9s - loss: 1.8550 - acc: 0.4958 - LRFinder: val_loss: 3.8964 - lr = 0.00867538 \n",
            "329/390 [========================>.....] - ETA: 9s - loss: 1.8549 - acc: 0.4959 - LRFinder: val_loss: 3.9962 - lr = 0.00869641 \n",
            "330/390 [========================>.....] - ETA: 9s - loss: 1.8544 - acc: 0.4961 - LRFinder: val_loss: 3.9207 - lr = 0.00871744 \n",
            "331/390 [========================>.....] - ETA: 8s - loss: 1.8543 - acc: 0.4959 - LRFinder: val_loss: 4.1064 - lr = 0.00873846 \n",
            "332/390 [========================>.....] - ETA: 8s - loss: 1.8541 - acc: 0.4959 - LRFinder: val_loss: 4.2001 - lr = 0.00875949 \n",
            "333/390 [========================>.....] - ETA: 8s - loss: 1.8542 - acc: 0.4958 - LRFinder: val_loss: 4.1081 - lr = 0.00878051 \n",
            "334/390 [========================>.....] - ETA: 8s - loss: 1.8539 - acc: 0.4959 - LRFinder: val_loss: 3.8628 - lr = 0.00880154 \n",
            "335/390 [========================>.....] - ETA: 8s - loss: 1.8538 - acc: 0.4961 - LRFinder: val_loss: 4.0273 - lr = 0.00882256 \n",
            "336/390 [========================>.....] - ETA: 8s - loss: 1.8540 - acc: 0.4958 - LRFinder: val_loss: 3.8283 - lr = 0.00884359 \n",
            "337/390 [========================>.....] - ETA: 7s - loss: 1.8539 - acc: 0.4957 - LRFinder: val_loss: 4.0525 - lr = 0.00886462 \n",
            "338/390 [=========================>....] - ETA: 7s - loss: 1.8540 - acc: 0.4957 - LRFinder: val_loss: 4.0253 - lr = 0.00888564 \n",
            "339/390 [=========================>....] - ETA: 7s - loss: 1.8538 - acc: 0.4956 - LRFinder: val_loss: 3.8726 - lr = 0.00890667 \n",
            "340/390 [=========================>....] - ETA: 7s - loss: 1.8535 - acc: 0.4958 - LRFinder: val_loss: 3.6598 - lr = 0.00892769 \n",
            "341/390 [=========================>....] - ETA: 7s - loss: 1.8538 - acc: 0.4956 - LRFinder: val_loss: 3.7910 - lr = 0.00894872 \n",
            "342/390 [=========================>....] - ETA: 7s - loss: 1.8530 - acc: 0.4960 - LRFinder: val_loss: 3.7634 - lr = 0.00896974 \n",
            "343/390 [=========================>....] - ETA: 7s - loss: 1.8527 - acc: 0.4961 - LRFinder: val_loss: 3.7758 - lr = 0.00899077 \n",
            "344/390 [=========================>....] - ETA: 6s - loss: 1.8523 - acc: 0.4961 - LRFinder: val_loss: 3.7114 - lr = 0.00901179 \n",
            "345/390 [=========================>....] - ETA: 6s - loss: 1.8524 - acc: 0.4960 - LRFinder: val_loss: 3.8514 - lr = 0.00903282 \n",
            "346/390 [=========================>....] - ETA: 6s - loss: 1.8523 - acc: 0.4962 - LRFinder: val_loss: 3.8282 - lr = 0.00905385 \n",
            "347/390 [=========================>....] - ETA: 6s - loss: 1.8525 - acc: 0.4962 - LRFinder: val_loss: 3.6230 - lr = 0.00907487 \n",
            "348/390 [=========================>....] - ETA: 6s - loss: 1.8522 - acc: 0.4962 - LRFinder: val_loss: 3.4874 - lr = 0.00909590 \n",
            "349/390 [=========================>....] - ETA: 6s - loss: 1.8517 - acc: 0.4964 - LRFinder: val_loss: 3.6434 - lr = 0.00911692 \n",
            "350/390 [=========================>....] - ETA: 6s - loss: 1.8513 - acc: 0.4966 - LRFinder: val_loss: 3.5166 - lr = 0.00913795 \n",
            "351/390 [==========================>...] - ETA: 5s - loss: 1.8508 - acc: 0.4967 - LRFinder: val_loss: 3.7103 - lr = 0.00915897 \n",
            "352/390 [==========================>...] - ETA: 5s - loss: 1.8507 - acc: 0.4967 - LRFinder: val_loss: 3.5985 - lr = 0.00918000 \n",
            "353/390 [==========================>...] - ETA: 5s - loss: 1.8504 - acc: 0.4969 - LRFinder: val_loss: 3.6163 - lr = 0.00920103 \n",
            "354/390 [==========================>...] - ETA: 5s - loss: 1.8502 - acc: 0.4969 - LRFinder: val_loss: 3.5306 - lr = 0.00922205 \n",
            "355/390 [==========================>...] - ETA: 5s - loss: 1.8501 - acc: 0.4970 - LRFinder: val_loss: 3.3717 - lr = 0.00924308 \n",
            "356/390 [==========================>...] - ETA: 5s - loss: 1.8494 - acc: 0.4973 - LRFinder: val_loss: 3.4998 - lr = 0.00926410 \n",
            "357/390 [==========================>...] - ETA: 4s - loss: 1.8493 - acc: 0.4975 - LRFinder: val_loss: 3.4912 - lr = 0.00928513 \n",
            "358/390 [==========================>...] - ETA: 4s - loss: 1.8491 - acc: 0.4976 - LRFinder: val_loss: 3.4597 - lr = 0.00930615 \n",
            "359/390 [==========================>...] - ETA: 4s - loss: 1.8493 - acc: 0.4975 - LRFinder: val_loss: 3.6910 - lr = 0.00932718 \n",
            "360/390 [==========================>...] - ETA: 4s - loss: 1.8489 - acc: 0.4976 - LRFinder: val_loss: 3.5680 - lr = 0.00934821 \n",
            "361/390 [==========================>...] - ETA: 4s - loss: 1.8486 - acc: 0.4977 - LRFinder: val_loss: 3.4854 - lr = 0.00936923 \n",
            "362/390 [==========================>...] - ETA: 4s - loss: 1.8481 - acc: 0.4979 - LRFinder: val_loss: 3.5729 - lr = 0.00939026 \n",
            "363/390 [==========================>...] - ETA: 4s - loss: 1.8481 - acc: 0.4978 - LRFinder: val_loss: 3.5298 - lr = 0.00941128 \n",
            "364/390 [===========================>..] - ETA: 3s - loss: 1.8480 - acc: 0.4978 - LRFinder: val_loss: 3.3757 - lr = 0.00943231 \n",
            "365/390 [===========================>..] - ETA: 3s - loss: 1.8476 - acc: 0.4979 - LRFinder: val_loss: 3.4060 - lr = 0.00945333 \n",
            "366/390 [===========================>..] - ETA: 3s - loss: 1.8476 - acc: 0.4979 - LRFinder: val_loss: 3.4884 - lr = 0.00947436 \n",
            "367/390 [===========================>..] - ETA: 3s - loss: 1.8475 - acc: 0.4979 - LRFinder: val_loss: 3.5672 - lr = 0.00949538 \n",
            "368/390 [===========================>..] - ETA: 3s - loss: 1.8474 - acc: 0.4979 - LRFinder: val_loss: 3.7038 - lr = 0.00951641 \n",
            "369/390 [===========================>..] - ETA: 3s - loss: 1.8473 - acc: 0.4980 - LRFinder: val_loss: 3.9069 - lr = 0.00953744 \n",
            "370/390 [===========================>..] - ETA: 2s - loss: 1.8468 - acc: 0.4981 - LRFinder: val_loss: 4.0890 - lr = 0.00955846 \n",
            "371/390 [===========================>..] - ETA: 2s - loss: 1.8472 - acc: 0.4981 - LRFinder: val_loss: 3.8981 - lr = 0.00957949 \n",
            "372/390 [===========================>..] - ETA: 2s - loss: 1.8465 - acc: 0.4982 - LRFinder: val_loss: 3.8663 - lr = 0.00960051 \n",
            "373/390 [===========================>..] - ETA: 2s - loss: 1.8461 - acc: 0.4983 - LRFinder: val_loss: 3.6991 - lr = 0.00962154 \n",
            "374/390 [===========================>..] - ETA: 2s - loss: 1.8458 - acc: 0.4984 - LRFinder: val_loss: 3.9071 - lr = 0.00964256 \n",
            "375/390 [===========================>..] - ETA: 2s - loss: 1.8453 - acc: 0.4984 - LRFinder: val_loss: 3.8719 - lr = 0.00966359 \n",
            "376/390 [===========================>..] - ETA: 2s - loss: 1.8451 - acc: 0.4985 - LRFinder: val_loss: 4.0464 - lr = 0.00968462 \n",
            "377/390 [============================>.] - ETA: 1s - loss: 1.8449 - acc: 0.4986 - LRFinder: val_loss: 4.0438 - lr = 0.00970564 \n",
            "378/390 [============================>.] - ETA: 1s - loss: 1.8450 - acc: 0.4985 - LRFinder: val_loss: 3.9590 - lr = 0.00972667 \n",
            "379/390 [============================>.] - ETA: 1s - loss: 1.8444 - acc: 0.4987 - LRFinder: val_loss: 3.8951 - lr = 0.00974769 \n",
            "380/390 [============================>.] - ETA: 1s - loss: 1.8444 - acc: 0.4987 - LRFinder: val_loss: 3.9152 - lr = 0.00976872 \n",
            "381/390 [============================>.] - ETA: 1s - loss: 1.8441 - acc: 0.4987 - LRFinder: val_loss: 3.7109 - lr = 0.00978974 \n",
            "382/390 [============================>.] - ETA: 1s - loss: 1.8442 - acc: 0.4988 - LRFinder: val_loss: 3.6669 - lr = 0.00981077 \n",
            "383/390 [============================>.] - ETA: 1s - loss: 1.8439 - acc: 0.4988 - LRFinder: val_loss: 3.6697 - lr = 0.00983179 \n",
            "384/390 [============================>.] - ETA: 0s - loss: 1.8436 - acc: 0.4989 - LRFinder: val_loss: 3.5184 - lr = 0.00985282 \n",
            "385/390 [============================>.] - ETA: 0s - loss: 1.8429 - acc: 0.4991 - LRFinder: val_loss: 3.4607 - lr = 0.00987385 \n",
            "386/390 [============================>.] - ETA: 0s - loss: 1.8430 - acc: 0.4993 - LRFinder: val_loss: 3.4986 - lr = 0.00989487 \n",
            "387/390 [============================>.] - ETA: 0s - loss: 1.8431 - acc: 0.4993 - LRFinder: val_loss: 3.4355 - lr = 0.00991590 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 1.8429 - acc: 0.4994 - LRFinder: val_loss: 3.4201 - lr = 0.00993692 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.8428 - acc: 0.4995 - LRFinder: val_loss: 3.3467 - lr = 0.00995795 \n",
            " - LRFinder: val_loss: 3.5408 - lr = 0.00997897 \n",
            "390/390 [==============================] - 60s 153ms/step - loss: 1.8424 - acc: 0.4995 - val_loss: 3.5104 - val_acc: 0.1741\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {weights/momentum/momentum-0.95/}\n",
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 11:21 - loss: 1.8377 - acc: 0.4766 - LRFinder: val_loss: 3.5196 - lr = 0.00180000 \n",
            "  2/390 [..............................] - ETA: 7:49 - loss: 1.7483 - acc: 0.5273  - LRFinder: val_loss: 3.4652 - lr = 0.00182103 \n",
            "  3/390 [..............................] - ETA: 5:34 - loss: 1.7571 - acc: 0.5156 - LRFinder: val_loss: 3.4648 - lr = 0.00184205 \n",
            "  4/390 [..............................] - ETA: 4:24 - loss: 1.7197 - acc: 0.5391 - LRFinder: val_loss: 3.4064 - lr = 0.00186308 \n",
            "  5/390 [..............................] - ETA: 3:41 - loss: 1.7528 - acc: 0.5266 - LRFinder: val_loss: 3.4872 - lr = 0.00188410 \n",
            "  6/390 [..............................] - ETA: 3:13 - loss: 1.7383 - acc: 0.5286 - LRFinder: val_loss: 3.5031 - lr = 0.00190513 \n",
            "  7/390 [..............................] - ETA: 2:53 - loss: 1.7196 - acc: 0.5368 - LRFinder: val_loss: 3.3387 - lr = 0.00192615 \n",
            "  8/390 [..............................] - ETA: 2:38 - loss: 1.7239 - acc: 0.5303 - LRFinder: val_loss: 3.4749 - lr = 0.00194718 \n",
            "  9/390 [..............................] - ETA: 2:26 - loss: 1.7210 - acc: 0.5304 - LRFinder: val_loss: 3.5399 - lr = 0.00196821 \n",
            " 10/390 [..............................] - ETA: 2:17 - loss: 1.7181 - acc: 0.5344 - LRFinder: val_loss: 3.5823 - lr = 0.00198923 \n",
            " 11/390 [..............................] - ETA: 2:09 - loss: 1.7197 - acc: 0.5327 - LRFinder: val_loss: 3.5142 - lr = 0.00201026 \n",
            " 12/390 [..............................] - ETA: 2:02 - loss: 1.7128 - acc: 0.5339 - LRFinder: val_loss: 3.5308 - lr = 0.00203128 \n",
            " 13/390 [>.............................] - ETA: 1:57 - loss: 1.7197 - acc: 0.5319 - LRFinder: val_loss: 3.5011 - lr = 0.00205231 \n",
            " 14/390 [>.............................] - ETA: 1:52 - loss: 1.7217 - acc: 0.5318 - LRFinder: val_loss: 3.4822 - lr = 0.00207333 \n",
            " 15/390 [>.............................] - ETA: 1:48 - loss: 1.7335 - acc: 0.5297 - LRFinder: val_loss: 3.4325 - lr = 0.00209436 \n",
            " 16/390 [>.............................] - ETA: 1:44 - loss: 1.7248 - acc: 0.5317 - LRFinder: val_loss: 3.4949 - lr = 0.00211538 \n",
            " 17/390 [>.............................] - ETA: 1:41 - loss: 1.7230 - acc: 0.5340 - LRFinder: val_loss: 3.5473 - lr = 0.00213641 \n",
            " 18/390 [>.............................] - ETA: 1:38 - loss: 1.7240 - acc: 0.5373 - LRFinder: val_loss: 3.5631 - lr = 0.00215744 \n",
            " 19/390 [>.............................] - ETA: 1:35 - loss: 1.7345 - acc: 0.5325 - LRFinder: val_loss: 3.5621 - lr = 0.00217846 \n",
            " 20/390 [>.............................] - ETA: 1:33 - loss: 1.7320 - acc: 0.5316 - LRFinder: val_loss: 3.6341 - lr = 0.00219949 \n",
            " 21/390 [>.............................] - ETA: 1:31 - loss: 1.7377 - acc: 0.5316 - LRFinder: val_loss: 3.6797 - lr = 0.00222051 \n",
            " 22/390 [>.............................] - ETA: 1:29 - loss: 1.7394 - acc: 0.5320 - LRFinder: val_loss: 3.5568 - lr = 0.00224154 \n",
            " 23/390 [>.............................] - ETA: 1:27 - loss: 1.7313 - acc: 0.5326 - LRFinder: val_loss: 3.7077 - lr = 0.00226256 \n",
            " 24/390 [>.............................] - ETA: 1:25 - loss: 1.7359 - acc: 0.5342 - LRFinder: val_loss: 3.6042 - lr = 0.00228359 \n",
            " 25/390 [>.............................] - ETA: 1:24 - loss: 1.7432 - acc: 0.5316 - LRFinder: val_loss: 3.6693 - lr = 0.00230462 \n",
            " 26/390 [=>............................] - ETA: 1:22 - loss: 1.7476 - acc: 0.5322 - LRFinder: val_loss: 3.6253 - lr = 0.00232564 \n",
            " 27/390 [=>............................] - ETA: 1:21 - loss: 1.7477 - acc: 0.5310 - LRFinder: val_loss: 3.6954 - lr = 0.00234667 \n",
            " 28/390 [=>............................] - ETA: 1:20 - loss: 1.7453 - acc: 0.5329 - LRFinder: val_loss: 3.4714 - lr = 0.00236769 \n",
            " 29/390 [=>............................] - ETA: 1:18 - loss: 1.7420 - acc: 0.5334 - LRFinder: val_loss: 3.7143 - lr = 0.00238872 \n",
            " 30/390 [=>............................] - ETA: 1:17 - loss: 1.7379 - acc: 0.5341 - LRFinder: val_loss: 3.5210 - lr = 0.00240974 \n",
            " 31/390 [=>............................] - ETA: 1:16 - loss: 1.7451 - acc: 0.5318 - LRFinder: val_loss: 3.6432 - lr = 0.00243077 \n",
            " 32/390 [=>............................] - ETA: 1:15 - loss: 1.7406 - acc: 0.5352 - LRFinder: val_loss: 3.6036 - lr = 0.00245179 \n",
            " 33/390 [=>............................] - ETA: 1:14 - loss: 1.7387 - acc: 0.5365 - LRFinder: val_loss: 3.5494 - lr = 0.00247282 \n",
            " 34/390 [=>............................] - ETA: 1:13 - loss: 1.7346 - acc: 0.5375 - LRFinder: val_loss: 3.6011 - lr = 0.00249385 \n",
            " 35/390 [=>............................] - ETA: 1:13 - loss: 1.7348 - acc: 0.5382 - LRFinder: val_loss: 3.5546 - lr = 0.00251487 \n",
            " 36/390 [=>............................] - ETA: 1:12 - loss: 1.7360 - acc: 0.5365 - LRFinder: val_loss: 3.6345 - lr = 0.00253590 \n",
            " 37/390 [=>............................] - ETA: 1:11 - loss: 1.7352 - acc: 0.5382 - LRFinder: val_loss: 3.6225 - lr = 0.00255692 \n",
            " 38/390 [=>............................] - ETA: 1:10 - loss: 1.7349 - acc: 0.5389 - LRFinder: val_loss: 3.6231 - lr = 0.00257795 \n",
            " 39/390 [==>...........................] - ETA: 1:10 - loss: 1.7331 - acc: 0.5379 - LRFinder: val_loss: 3.5581 - lr = 0.00259897 \n",
            " 40/390 [==>...........................] - ETA: 1:09 - loss: 1.7275 - acc: 0.5400 - LRFinder: val_loss: 3.5617 - lr = 0.00262000 \n",
            " 41/390 [==>...........................] - ETA: 1:08 - loss: 1.7289 - acc: 0.5400 - LRFinder: val_loss: 3.6366 - lr = 0.00264103 \n",
            " 42/390 [==>...........................] - ETA: 1:08 - loss: 1.7321 - acc: 0.5394 - LRFinder: val_loss: 3.6548 - lr = 0.00266205 \n",
            " 43/390 [==>...........................] - ETA: 1:07 - loss: 1.7306 - acc: 0.5400 - LRFinder: val_loss: 3.5572 - lr = 0.00268308 \n",
            " 44/390 [==>...........................] - ETA: 1:06 - loss: 1.7360 - acc: 0.5389 - LRFinder: val_loss: 3.7401 - lr = 0.00270410 \n",
            " 45/390 [==>...........................] - ETA: 1:06 - loss: 1.7368 - acc: 0.5399 - LRFinder: val_loss: 3.7899 - lr = 0.00272513 \n",
            " 46/390 [==>...........................] - ETA: 1:05 - loss: 1.7391 - acc: 0.5397 - LRFinder: val_loss: 3.6175 - lr = 0.00274615 \n",
            " 47/390 [==>...........................] - ETA: 1:05 - loss: 1.7371 - acc: 0.5406 - LRFinder: val_loss: 3.7296 - lr = 0.00276718 \n",
            " 48/390 [==>...........................] - ETA: 1:04 - loss: 1.7389 - acc: 0.5384 - LRFinder: val_loss: 3.6713 - lr = 0.00278821 \n",
            " 49/390 [==>...........................] - ETA: 1:04 - loss: 1.7391 - acc: 0.5376 - LRFinder: val_loss: 3.8356 - lr = 0.00280923 \n",
            " 50/390 [==>...........................] - ETA: 1:03 - loss: 1.7383 - acc: 0.5373 - LRFinder: val_loss: 3.8798 - lr = 0.00283026 \n",
            " 51/390 [==>...........................] - ETA: 1:03 - loss: 1.7389 - acc: 0.5389 - LRFinder: val_loss: 3.5978 - lr = 0.00285128 \n",
            " 52/390 [===>..........................] - ETA: 1:02 - loss: 1.7363 - acc: 0.5400 - LRFinder: val_loss: 3.7880 - lr = 0.00287231 \n",
            " 53/390 [===>..........................] - ETA: 1:02 - loss: 1.7351 - acc: 0.5399 - LRFinder: val_loss: 3.6614 - lr = 0.00289333 \n",
            " 54/390 [===>..........................] - ETA: 1:01 - loss: 1.7358 - acc: 0.5392 - LRFinder: val_loss: 3.7718 - lr = 0.00291436 \n",
            " 55/390 [===>..........................] - ETA: 1:01 - loss: 1.7366 - acc: 0.5391 - LRFinder: val_loss: 3.7382 - lr = 0.00293538 \n",
            " 56/390 [===>..........................] - ETA: 1:01 - loss: 1.7374 - acc: 0.5389 - LRFinder: val_loss: 3.8332 - lr = 0.00295641 \n",
            " 57/390 [===>..........................] - ETA: 1:00 - loss: 1.7385 - acc: 0.5382 - LRFinder: val_loss: 3.6979 - lr = 0.00297744 \n",
            " 58/390 [===>..........................] - ETA: 1:00 - loss: 1.7386 - acc: 0.5374 - LRFinder: val_loss: 3.7896 - lr = 0.00299846 \n",
            " 59/390 [===>..........................] - ETA: 59s - loss: 1.7396 - acc: 0.5367  - LRFinder: val_loss: 3.8549 - lr = 0.00301949 \n",
            " 60/390 [===>..........................] - ETA: 59s - loss: 1.7376 - acc: 0.5374 - LRFinder: val_loss: 3.7159 - lr = 0.00304051 \n",
            " 61/390 [===>..........................] - ETA: 59s - loss: 1.7371 - acc: 0.5371 - LRFinder: val_loss: 4.0492 - lr = 0.00306154 \n",
            " 62/390 [===>..........................] - ETA: 58s - loss: 1.7351 - acc: 0.5376 - LRFinder: val_loss: 4.1006 - lr = 0.00308256 \n",
            " 63/390 [===>..........................] - ETA: 58s - loss: 1.7343 - acc: 0.5375 - LRFinder: val_loss: 4.0256 - lr = 0.00310359 \n",
            " 64/390 [===>..........................] - ETA: 58s - loss: 1.7349 - acc: 0.5366 - LRFinder: val_loss: 4.0716 - lr = 0.00312462 \n",
            " 65/390 [====>.........................] - ETA: 57s - loss: 1.7349 - acc: 0.5367 - LRFinder: val_loss: 4.1043 - lr = 0.00314564 \n",
            " 66/390 [====>.........................] - ETA: 57s - loss: 1.7333 - acc: 0.5369 - LRFinder: val_loss: 4.2036 - lr = 0.00316667 \n",
            " 67/390 [====>.........................] - ETA: 57s - loss: 1.7354 - acc: 0.5364 - LRFinder: val_loss: 4.0528 - lr = 0.00318769 \n",
            " 68/390 [====>.........................] - ETA: 56s - loss: 1.7340 - acc: 0.5362 - LRFinder: val_loss: 4.1913 - lr = 0.00320872 \n",
            " 69/390 [====>.........................] - ETA: 56s - loss: 1.7307 - acc: 0.5375 - LRFinder: val_loss: 4.2913 - lr = 0.00322974 \n",
            " 70/390 [====>.........................] - ETA: 56s - loss: 1.7322 - acc: 0.5373 - LRFinder: val_loss: 4.1472 - lr = 0.00325077 \n",
            " 71/390 [====>.........................] - ETA: 55s - loss: 1.7314 - acc: 0.5372 - LRFinder: val_loss: 4.3998 - lr = 0.00327179 \n",
            " 72/390 [====>.........................] - ETA: 55s - loss: 1.7298 - acc: 0.5384 - LRFinder: val_loss: 4.0845 - lr = 0.00329282 \n",
            " 73/390 [====>.........................] - ETA: 55s - loss: 1.7296 - acc: 0.5387 - LRFinder: val_loss: 4.0794 - lr = 0.00331385 \n",
            " 74/390 [====>.........................] - ETA: 54s - loss: 1.7281 - acc: 0.5391 - LRFinder: val_loss: 4.0121 - lr = 0.00333487 \n",
            " 75/390 [====>.........................] - ETA: 54s - loss: 1.7283 - acc: 0.5389 - LRFinder: val_loss: 4.0832 - lr = 0.00335590 \n",
            " 76/390 [====>.........................] - ETA: 54s - loss: 1.7298 - acc: 0.5381 - LRFinder: val_loss: 4.1700 - lr = 0.00337692 \n",
            " 77/390 [====>.........................] - ETA: 53s - loss: 1.7290 - acc: 0.5384 - LRFinder: val_loss: 4.0465 - lr = 0.00339795 \n",
            " 78/390 [=====>........................] - ETA: 53s - loss: 1.7290 - acc: 0.5384 - LRFinder: val_loss: 4.0160 - lr = 0.00341897 \n",
            " 79/390 [=====>........................] - ETA: 53s - loss: 1.7282 - acc: 0.5382 - LRFinder: val_loss: 4.0536 - lr = 0.00344000 \n",
            " 80/390 [=====>........................] - ETA: 53s - loss: 1.7286 - acc: 0.5380 - LRFinder: val_loss: 3.9741 - lr = 0.00346103 \n",
            " 81/390 [=====>........................] - ETA: 52s - loss: 1.7290 - acc: 0.5385 - LRFinder: val_loss: 3.9297 - lr = 0.00348205 \n",
            " 82/390 [=====>........................] - ETA: 52s - loss: 1.7280 - acc: 0.5388 - LRFinder: val_loss: 3.8467 - lr = 0.00350308 \n",
            " 83/390 [=====>........................] - ETA: 52s - loss: 1.7274 - acc: 0.5393 - LRFinder: val_loss: 3.8408 - lr = 0.00352410 \n",
            " 84/390 [=====>........................] - ETA: 51s - loss: 1.7271 - acc: 0.5393 - LRFinder: val_loss: 3.8518 - lr = 0.00354513 \n",
            " 85/390 [=====>........................] - ETA: 51s - loss: 1.7267 - acc: 0.5398 - LRFinder: val_loss: 3.8446 - lr = 0.00356615 \n",
            " 86/390 [=====>........................] - ETA: 51s - loss: 1.7279 - acc: 0.5398 - LRFinder: val_loss: 3.7387 - lr = 0.00358718 \n",
            " 87/390 [=====>........................] - ETA: 51s - loss: 1.7281 - acc: 0.5401 - LRFinder: val_loss: 3.8207 - lr = 0.00360821 \n",
            " 88/390 [=====>........................] - ETA: 50s - loss: 1.7277 - acc: 0.5410 - LRFinder: val_loss: 3.7880 - lr = 0.00362923 \n",
            " 89/390 [=====>........................] - ETA: 50s - loss: 1.7283 - acc: 0.5411 - LRFinder: val_loss: 3.9971 - lr = 0.00365026 \n",
            " 90/390 [=====>........................] - ETA: 50s - loss: 1.7292 - acc: 0.5406 - LRFinder: val_loss: 4.1355 - lr = 0.00367128 \n",
            " 91/390 [======>.......................] - ETA: 50s - loss: 1.7276 - acc: 0.5415 - LRFinder: val_loss: 4.0923 - lr = 0.00369231 \n",
            " 92/390 [======>.......................] - ETA: 49s - loss: 1.7267 - acc: 0.5418 - LRFinder: val_loss: 4.0465 - lr = 0.00371333 \n",
            " 93/390 [======>.......................] - ETA: 49s - loss: 1.7270 - acc: 0.5420 - LRFinder: val_loss: 4.2652 - lr = 0.00373436 \n",
            " 94/390 [======>.......................] - ETA: 49s - loss: 1.7272 - acc: 0.5417 - LRFinder: val_loss: 4.2708 - lr = 0.00375538 \n",
            " 95/390 [======>.......................] - ETA: 49s - loss: 1.7260 - acc: 0.5414 - LRFinder: val_loss: 4.3333 - lr = 0.00377641 \n",
            " 96/390 [======>.......................] - ETA: 49s - loss: 1.7262 - acc: 0.5417 - LRFinder: val_loss: 4.2116 - lr = 0.00379744 \n",
            " 97/390 [======>.......................] - ETA: 48s - loss: 1.7239 - acc: 0.5425 - LRFinder: val_loss: 4.1992 - lr = 0.00381846 \n",
            " 98/390 [======>.......................] - ETA: 48s - loss: 1.7234 - acc: 0.5428 - LRFinder: val_loss: 4.0279 - lr = 0.00383949 \n",
            " 99/390 [======>.......................] - ETA: 48s - loss: 1.7232 - acc: 0.5432 - LRFinder: val_loss: 3.9943 - lr = 0.00386051 \n",
            "100/390 [======>.......................] - ETA: 48s - loss: 1.7228 - acc: 0.5430 - LRFinder: val_loss: 4.1917 - lr = 0.00388154 \n",
            "101/390 [======>.......................] - ETA: 47s - loss: 1.7228 - acc: 0.5429 - LRFinder: val_loss: 4.0042 - lr = 0.00390256 \n",
            "102/390 [======>.......................] - ETA: 47s - loss: 1.7219 - acc: 0.5430 - LRFinder: val_loss: 4.0290 - lr = 0.00392359 \n",
            "103/390 [======>.......................] - ETA: 47s - loss: 1.7226 - acc: 0.5428 - LRFinder: val_loss: 4.2825 - lr = 0.00394462 \n",
            "104/390 [=======>......................] - ETA: 47s - loss: 1.7226 - acc: 0.5430 - LRFinder: val_loss: 4.2031 - lr = 0.00396564 \n",
            "105/390 [=======>......................] - ETA: 46s - loss: 1.7230 - acc: 0.5424 - LRFinder: val_loss: 4.1220 - lr = 0.00398667 \n",
            "106/390 [=======>......................] - ETA: 46s - loss: 1.7221 - acc: 0.5427 - LRFinder: val_loss: 4.2925 - lr = 0.00400769 \n",
            "107/390 [=======>......................] - ETA: 46s - loss: 1.7225 - acc: 0.5424 - LRFinder: val_loss: 4.6614 - lr = 0.00402872 \n",
            "108/390 [=======>......................] - ETA: 46s - loss: 1.7231 - acc: 0.5420 - LRFinder: val_loss: 4.8114 - lr = 0.00404974 \n",
            "109/390 [=======>......................] - ETA: 46s - loss: 1.7236 - acc: 0.5426 - LRFinder: val_loss: 4.6574 - lr = 0.00407077 \n",
            "110/390 [=======>......................] - ETA: 45s - loss: 1.7255 - acc: 0.5418 - LRFinder: val_loss: 4.7326 - lr = 0.00409179 \n",
            "111/390 [=======>......................] - ETA: 45s - loss: 1.7254 - acc: 0.5417 - LRFinder: val_loss: 4.9308 - lr = 0.00411282 \n",
            "112/390 [=======>......................] - ETA: 45s - loss: 1.7262 - acc: 0.5417 - LRFinder: val_loss: 4.8127 - lr = 0.00413385 \n",
            "113/390 [=======>......................] - ETA: 45s - loss: 1.7261 - acc: 0.5416 - LRFinder: val_loss: 4.8549 - lr = 0.00415487 \n",
            "114/390 [=======>......................] - ETA: 45s - loss: 1.7272 - acc: 0.5412 - LRFinder: val_loss: 4.8803 - lr = 0.00417590 \n",
            "115/390 [=======>......................] - ETA: 44s - loss: 1.7283 - acc: 0.5406 - LRFinder: val_loss: 4.8326 - lr = 0.00419692 \n",
            "116/390 [=======>......................] - ETA: 44s - loss: 1.7282 - acc: 0.5401 - LRFinder: val_loss: 4.5689 - lr = 0.00421795 \n",
            "117/390 [========>.....................] - ETA: 44s - loss: 1.7277 - acc: 0.5403 - LRFinder: val_loss: 4.5854 - lr = 0.00423897 \n",
            "118/390 [========>.....................] - ETA: 44s - loss: 1.7290 - acc: 0.5401 - LRFinder: val_loss: 4.5363 - lr = 0.00426000 \n",
            "119/390 [========>.....................] - ETA: 44s - loss: 1.7303 - acc: 0.5397 - LRFinder: val_loss: 4.3265 - lr = 0.00428103 \n",
            "120/390 [========>.....................] - ETA: 43s - loss: 1.7295 - acc: 0.5401 - LRFinder: val_loss: 4.6448 - lr = 0.00430205 \n",
            "121/390 [========>.....................] - ETA: 43s - loss: 1.7291 - acc: 0.5402 - LRFinder: val_loss: 4.3709 - lr = 0.00432308 \n",
            "122/390 [========>.....................] - ETA: 43s - loss: 1.7284 - acc: 0.5405 - LRFinder: val_loss: 4.2772 - lr = 0.00434410 \n",
            "123/390 [========>.....................] - ETA: 43s - loss: 1.7300 - acc: 0.5402 - LRFinder: val_loss: 4.5316 - lr = 0.00436513 \n",
            "124/390 [========>.....................] - ETA: 43s - loss: 1.7317 - acc: 0.5395 - LRFinder: val_loss: 4.6718 - lr = 0.00438615 \n",
            "125/390 [========>.....................] - ETA: 42s - loss: 1.7338 - acc: 0.5389 - LRFinder: val_loss: 4.7645 - lr = 0.00440718 \n",
            "126/390 [========>.....................] - ETA: 42s - loss: 1.7338 - acc: 0.5389 - LRFinder: val_loss: 5.0015 - lr = 0.00442821 \n",
            "127/390 [========>.....................] - ETA: 42s - loss: 1.7328 - acc: 0.5392 - LRFinder: val_loss: 4.9668 - lr = 0.00444923 \n",
            "128/390 [========>.....................] - ETA: 42s - loss: 1.7316 - acc: 0.5400 - LRFinder: val_loss: 4.8510 - lr = 0.00447026 \n",
            "129/390 [========>.....................] - ETA: 42s - loss: 1.7305 - acc: 0.5402 - LRFinder: val_loss: 4.9113 - lr = 0.00449128 \n",
            "130/390 [=========>....................] - ETA: 41s - loss: 1.7296 - acc: 0.5405 - LRFinder: val_loss: 4.7593 - lr = 0.00451231 \n",
            "131/390 [=========>....................] - ETA: 41s - loss: 1.7295 - acc: 0.5407 - LRFinder: val_loss: 4.8536 - lr = 0.00453333 \n",
            "132/390 [=========>....................] - ETA: 41s - loss: 1.7309 - acc: 0.5402 - LRFinder: val_loss: 4.8720 - lr = 0.00455436 \n",
            "133/390 [=========>....................] - ETA: 41s - loss: 1.7300 - acc: 0.5403 - LRFinder: val_loss: 4.9200 - lr = 0.00457538 \n",
            "134/390 [=========>....................] - ETA: 41s - loss: 1.7302 - acc: 0.5401 - LRFinder: val_loss: 5.1739 - lr = 0.00459641 \n",
            "135/390 [=========>....................] - ETA: 40s - loss: 1.7308 - acc: 0.5399 - LRFinder: val_loss: 5.1504 - lr = 0.00461744 \n",
            "136/390 [=========>....................] - ETA: 40s - loss: 1.7321 - acc: 0.5396 - LRFinder: val_loss: 5.0933 - lr = 0.00463846 \n",
            "137/390 [=========>....................] - ETA: 40s - loss: 1.7323 - acc: 0.5395 - LRFinder: val_loss: 5.4013 - lr = 0.00465949 \n",
            "138/390 [=========>....................] - ETA: 40s - loss: 1.7317 - acc: 0.5400 - LRFinder: val_loss: 5.1613 - lr = 0.00468051 \n",
            "139/390 [=========>....................] - ETA: 40s - loss: 1.7309 - acc: 0.5404 - LRFinder: val_loss: 5.1192 - lr = 0.00470154 \n",
            "140/390 [=========>....................] - ETA: 39s - loss: 1.7307 - acc: 0.5406 - LRFinder: val_loss: 5.0705 - lr = 0.00472256 \n",
            "141/390 [=========>....................] - ETA: 39s - loss: 1.7307 - acc: 0.5404 - LRFinder: val_loss: 4.6190 - lr = 0.00474359 \n",
            "142/390 [=========>....................] - ETA: 39s - loss: 1.7307 - acc: 0.5403 - LRFinder: val_loss: 4.5983 - lr = 0.00476462 \n",
            "143/390 [==========>...................] - ETA: 39s - loss: 1.7306 - acc: 0.5402 - LRFinder: val_loss: 4.2827 - lr = 0.00478564 \n",
            "144/390 [==========>...................] - ETA: 39s - loss: 1.7303 - acc: 0.5405 - LRFinder: val_loss: 4.6316 - lr = 0.00480667 \n",
            "145/390 [==========>...................] - ETA: 39s - loss: 1.7311 - acc: 0.5401 - LRFinder: val_loss: 4.6192 - lr = 0.00482769 \n",
            "146/390 [==========>...................] - ETA: 38s - loss: 1.7314 - acc: 0.5400 - LRFinder: val_loss: 4.6400 - lr = 0.00484872 \n",
            "147/390 [==========>...................] - ETA: 38s - loss: 1.7323 - acc: 0.5391 - LRFinder: val_loss: 4.8693 - lr = 0.00486974 \n",
            "148/390 [==========>...................] - ETA: 38s - loss: 1.7330 - acc: 0.5389 - LRFinder: val_loss: 4.8796 - lr = 0.00489077 \n",
            "149/390 [==========>...................] - ETA: 38s - loss: 1.7328 - acc: 0.5389 - LRFinder: val_loss: 4.6588 - lr = 0.00491179 \n",
            "150/390 [==========>...................] - ETA: 38s - loss: 1.7333 - acc: 0.5385 - LRFinder: val_loss: 4.0198 - lr = 0.00493282 \n",
            "151/390 [==========>...................] - ETA: 37s - loss: 1.7337 - acc: 0.5383 - LRFinder: val_loss: 3.9366 - lr = 0.00495385 \n",
            "152/390 [==========>...................] - ETA: 37s - loss: 1.7341 - acc: 0.5382 - LRFinder: val_loss: 3.6624 - lr = 0.00497487 \n",
            "153/390 [==========>...................] - ETA: 37s - loss: 1.7351 - acc: 0.5382 - LRFinder: val_loss: 3.6637 - lr = 0.00499590 \n",
            "154/390 [==========>...................] - ETA: 37s - loss: 1.7354 - acc: 0.5380 - LRFinder: val_loss: 3.3953 - lr = 0.00501692 \n",
            "155/390 [==========>...................] - ETA: 37s - loss: 1.7362 - acc: 0.5380 - LRFinder: val_loss: 3.5046 - lr = 0.00503795 \n",
            "156/390 [===========>..................] - ETA: 37s - loss: 1.7365 - acc: 0.5378 - LRFinder: val_loss: 3.3439 - lr = 0.00505897 \n",
            "157/390 [===========>..................] - ETA: 36s - loss: 1.7372 - acc: 0.5376 - LRFinder: val_loss: 3.3798 - lr = 0.00508000 \n",
            "158/390 [===========>..................] - ETA: 36s - loss: 1.7370 - acc: 0.5378 - LRFinder: val_loss: 3.4304 - lr = 0.00510103 \n",
            "159/390 [===========>..................] - ETA: 36s - loss: 1.7363 - acc: 0.5381 - LRFinder: val_loss: 3.4732 - lr = 0.00512205 \n",
            "160/390 [===========>..................] - ETA: 36s - loss: 1.7372 - acc: 0.5375 - LRFinder: val_loss: 3.3472 - lr = 0.00514308 \n",
            "161/390 [===========>..................] - ETA: 36s - loss: 1.7381 - acc: 0.5373 - LRFinder: val_loss: 3.4107 - lr = 0.00516410 \n",
            "162/390 [===========>..................] - ETA: 35s - loss: 1.7376 - acc: 0.5375 - LRFinder: val_loss: 3.3821 - lr = 0.00518513 \n",
            "163/390 [===========>..................] - ETA: 35s - loss: 1.7378 - acc: 0.5376 - LRFinder: val_loss: 3.3363 - lr = 0.00520615 \n",
            "164/390 [===========>..................] - ETA: 35s - loss: 1.7372 - acc: 0.5380 - LRFinder: val_loss: 3.5875 - lr = 0.00522718 \n",
            "165/390 [===========>..................] - ETA: 35s - loss: 1.7379 - acc: 0.5377 - LRFinder: val_loss: 3.4356 - lr = 0.00524821 \n",
            "166/390 [===========>..................] - ETA: 35s - loss: 1.7387 - acc: 0.5376 - LRFinder: val_loss: 3.3300 - lr = 0.00526923 \n",
            "167/390 [===========>..................] - ETA: 35s - loss: 1.7376 - acc: 0.5383 - LRFinder: val_loss: 3.2634 - lr = 0.00529026 \n",
            "168/390 [===========>..................] - ETA: 34s - loss: 1.7370 - acc: 0.5384 - LRFinder: val_loss: 3.3496 - lr = 0.00531128 \n",
            "169/390 [============>.................] - ETA: 34s - loss: 1.7364 - acc: 0.5386 - LRFinder: val_loss: 3.2056 - lr = 0.00533231 \n",
            "170/390 [============>.................] - ETA: 34s - loss: 1.7354 - acc: 0.5386 - LRFinder: val_loss: 3.3479 - lr = 0.00535333 \n",
            "171/390 [============>.................] - ETA: 34s - loss: 1.7353 - acc: 0.5390 - LRFinder: val_loss: 3.2933 - lr = 0.00537436 \n",
            "172/390 [============>.................] - ETA: 34s - loss: 1.7361 - acc: 0.5384 - LRFinder: val_loss: 3.4811 - lr = 0.00539538 \n",
            "173/390 [============>.................] - ETA: 34s - loss: 1.7353 - acc: 0.5389 - LRFinder: val_loss: 3.3534 - lr = 0.00541641 \n",
            "174/390 [============>.................] - ETA: 33s - loss: 1.7352 - acc: 0.5390 - LRFinder: val_loss: 3.4143 - lr = 0.00543744 \n",
            "175/390 [============>.................] - ETA: 33s - loss: 1.7350 - acc: 0.5391 - LRFinder: val_loss: 3.3242 - lr = 0.00545846 \n",
            "176/390 [============>.................] - ETA: 33s - loss: 1.7348 - acc: 0.5390 - LRFinder: val_loss: 3.3077 - lr = 0.00547949 \n",
            "177/390 [============>.................] - ETA: 33s - loss: 1.7341 - acc: 0.5395 - LRFinder: val_loss: 3.2643 - lr = 0.00550051 \n",
            "178/390 [============>.................] - ETA: 33s - loss: 1.7352 - acc: 0.5392 - LRFinder: val_loss: 3.3092 - lr = 0.00552154 \n",
            "179/390 [============>.................] - ETA: 33s - loss: 1.7356 - acc: 0.5388 - LRFinder: val_loss: 3.2277 - lr = 0.00554256 \n",
            "180/390 [============>.................] - ETA: 32s - loss: 1.7357 - acc: 0.5388 - LRFinder: val_loss: 3.2575 - lr = 0.00556359 \n",
            "181/390 [============>.................] - ETA: 32s - loss: 1.7361 - acc: 0.5385 - LRFinder: val_loss: 3.3974 - lr = 0.00558462 \n",
            "182/390 [=============>................] - ETA: 32s - loss: 1.7365 - acc: 0.5385 - LRFinder: val_loss: 3.3854 - lr = 0.00560564 \n",
            "183/390 [=============>................] - ETA: 32s - loss: 1.7365 - acc: 0.5385 - LRFinder: val_loss: 3.5376 - lr = 0.00562667 \n",
            "184/390 [=============>................] - ETA: 32s - loss: 1.7363 - acc: 0.5386 - LRFinder: val_loss: 3.4729 - lr = 0.00564769 \n",
            "185/390 [=============>................] - ETA: 32s - loss: 1.7362 - acc: 0.5388 - LRFinder: val_loss: 3.4750 - lr = 0.00566872 \n",
            "186/390 [=============>................] - ETA: 31s - loss: 1.7365 - acc: 0.5390 - LRFinder: val_loss: 3.5445 - lr = 0.00568974 \n",
            "187/390 [=============>................] - ETA: 31s - loss: 1.7367 - acc: 0.5387 - LRFinder: val_loss: 3.6594 - lr = 0.00571077 \n",
            "188/390 [=============>................] - ETA: 31s - loss: 1.7370 - acc: 0.5389 - LRFinder: val_loss: 3.6799 - lr = 0.00573179 \n",
            "189/390 [=============>................] - ETA: 31s - loss: 1.7363 - acc: 0.5391 - LRFinder: val_loss: 3.6894 - lr = 0.00575282 \n",
            "190/390 [=============>................] - ETA: 31s - loss: 1.7365 - acc: 0.5394 - LRFinder: val_loss: 3.7422 - lr = 0.00577385 \n",
            "191/390 [=============>................] - ETA: 31s - loss: 1.7369 - acc: 0.5393 - LRFinder: val_loss: 3.6882 - lr = 0.00579487 \n",
            "192/390 [=============>................] - ETA: 30s - loss: 1.7360 - acc: 0.5396 - LRFinder: val_loss: 3.6636 - lr = 0.00581590 \n",
            "193/390 [=============>................] - ETA: 30s - loss: 1.7358 - acc: 0.5399 - LRFinder: val_loss: 3.7261 - lr = 0.00583692 \n",
            "194/390 [=============>................] - ETA: 30s - loss: 1.7359 - acc: 0.5396 - LRFinder: val_loss: 3.7057 - lr = 0.00585795 \n",
            "195/390 [==============>...............] - ETA: 30s - loss: 1.7358 - acc: 0.5395 - LRFinder: val_loss: 3.7998 - lr = 0.00587897 \n",
            "196/390 [==============>...............] - ETA: 30s - loss: 1.7357 - acc: 0.5397 - LRFinder: val_loss: 3.7920 - lr = 0.00590000 \n",
            "197/390 [==============>...............] - ETA: 30s - loss: 1.7349 - acc: 0.5401 - LRFinder: val_loss: 3.6521 - lr = 0.00592103 \n",
            "198/390 [==============>...............] - ETA: 29s - loss: 1.7349 - acc: 0.5402 - LRFinder: val_loss: 3.4896 - lr = 0.00594205 \n",
            "199/390 [==============>...............] - ETA: 29s - loss: 1.7354 - acc: 0.5401 - LRFinder: val_loss: 3.5836 - lr = 0.00596308 \n",
            "200/390 [==============>...............] - ETA: 29s - loss: 1.7361 - acc: 0.5397 - LRFinder: val_loss: 3.6736 - lr = 0.00598410 \n",
            "201/390 [==============>...............] - ETA: 29s - loss: 1.7354 - acc: 0.5398 - LRFinder: val_loss: 3.7476 - lr = 0.00600513 \n",
            "202/390 [==============>...............] - ETA: 29s - loss: 1.7356 - acc: 0.5396 - LRFinder: val_loss: 3.5734 - lr = 0.00602615 \n",
            "203/390 [==============>...............] - ETA: 29s - loss: 1.7350 - acc: 0.5399 - LRFinder: val_loss: 3.7140 - lr = 0.00604718 \n",
            "204/390 [==============>...............] - ETA: 28s - loss: 1.7350 - acc: 0.5398 - LRFinder: val_loss: 3.6410 - lr = 0.00606821 \n",
            "205/390 [==============>...............] - ETA: 28s - loss: 1.7347 - acc: 0.5400 - LRFinder: val_loss: 3.6936 - lr = 0.00608923 \n",
            "206/390 [==============>...............] - ETA: 28s - loss: 1.7347 - acc: 0.5400 - LRFinder: val_loss: 3.7764 - lr = 0.00611026 \n",
            "207/390 [==============>...............] - ETA: 28s - loss: 1.7341 - acc: 0.5402 - LRFinder: val_loss: 3.7153 - lr = 0.00613128 \n",
            "208/390 [===============>..............] - ETA: 28s - loss: 1.7339 - acc: 0.5400 - LRFinder: val_loss: 3.7174 - lr = 0.00615231 \n",
            "209/390 [===============>..............] - ETA: 28s - loss: 1.7332 - acc: 0.5406 - LRFinder: val_loss: 3.9266 - lr = 0.00617333 \n",
            "210/390 [===============>..............] - ETA: 27s - loss: 1.7341 - acc: 0.5402 - LRFinder: val_loss: 4.0234 - lr = 0.00619436 \n",
            "211/390 [===============>..............] - ETA: 27s - loss: 1.7341 - acc: 0.5402 - LRFinder: val_loss: 3.8643 - lr = 0.00621538 \n",
            "212/390 [===============>..............] - ETA: 27s - loss: 1.7337 - acc: 0.5404 - LRFinder: val_loss: 3.8604 - lr = 0.00623641 \n",
            "213/390 [===============>..............] - ETA: 27s - loss: 1.7334 - acc: 0.5405 - LRFinder: val_loss: 3.7710 - lr = 0.00625744 \n",
            "214/390 [===============>..............] - ETA: 27s - loss: 1.7330 - acc: 0.5405 - LRFinder: val_loss: 3.8692 - lr = 0.00627846 \n",
            "215/390 [===============>..............] - ETA: 27s - loss: 1.7331 - acc: 0.5404 - LRFinder: val_loss: 3.7780 - lr = 0.00629949 \n",
            "216/390 [===============>..............] - ETA: 26s - loss: 1.7328 - acc: 0.5405 - LRFinder: val_loss: 3.9141 - lr = 0.00632051 \n",
            "217/390 [===============>..............] - ETA: 26s - loss: 1.7323 - acc: 0.5406 - LRFinder: val_loss: 3.8185 - lr = 0.00634154 \n",
            "218/390 [===============>..............] - ETA: 26s - loss: 1.7322 - acc: 0.5405 - LRFinder: val_loss: 3.8459 - lr = 0.00636256 \n",
            "219/390 [===============>..............] - ETA: 26s - loss: 1.7317 - acc: 0.5407 - LRFinder: val_loss: 3.8229 - lr = 0.00638359 \n",
            "220/390 [===============>..............] - ETA: 26s - loss: 1.7321 - acc: 0.5406 - LRFinder: val_loss: 4.0305 - lr = 0.00640462 \n",
            "221/390 [================>.............] - ETA: 26s - loss: 1.7318 - acc: 0.5407 - LRFinder: val_loss: 4.1469 - lr = 0.00642564 \n",
            "222/390 [================>.............] - ETA: 25s - loss: 1.7316 - acc: 0.5407 - LRFinder: val_loss: 4.1395 - lr = 0.00644667 \n",
            "223/390 [================>.............] - ETA: 25s - loss: 1.7301 - acc: 0.5413 - LRFinder: val_loss: 4.1007 - lr = 0.00646769 \n",
            "224/390 [================>.............] - ETA: 25s - loss: 1.7295 - acc: 0.5416 - LRFinder: val_loss: 4.0276 - lr = 0.00648872 \n",
            "225/390 [================>.............] - ETA: 25s - loss: 1.7288 - acc: 0.5418 - LRFinder: val_loss: 3.9711 - lr = 0.00650974 \n",
            "226/390 [================>.............] - ETA: 25s - loss: 1.7283 - acc: 0.5420 - LRFinder: val_loss: 4.0771 - lr = 0.00653077 \n",
            "227/390 [================>.............] - ETA: 25s - loss: 1.7291 - acc: 0.5417 - LRFinder: val_loss: 4.1662 - lr = 0.00655179 \n",
            "228/390 [================>.............] - ETA: 24s - loss: 1.7289 - acc: 0.5418 - LRFinder: val_loss: 4.0776 - lr = 0.00657282 \n",
            "229/390 [================>.............] - ETA: 24s - loss: 1.7292 - acc: 0.5419 - LRFinder: val_loss: 3.9328 - lr = 0.00659385 \n",
            "230/390 [================>.............] - ETA: 24s - loss: 1.7292 - acc: 0.5421 - LRFinder: val_loss: 3.7908 - lr = 0.00661487 \n",
            "231/390 [================>.............] - ETA: 24s - loss: 1.7289 - acc: 0.5420 - LRFinder: val_loss: 3.8265 - lr = 0.00663590 \n",
            "232/390 [================>.............] - ETA: 24s - loss: 1.7283 - acc: 0.5422 - LRFinder: val_loss: 3.8673 - lr = 0.00665692 \n",
            "233/390 [================>.............] - ETA: 24s - loss: 1.7280 - acc: 0.5421 - LRFinder: val_loss: 3.8171 - lr = 0.00667795 \n",
            "234/390 [=================>............] - ETA: 23s - loss: 1.7276 - acc: 0.5422 - LRFinder: val_loss: 3.9428 - lr = 0.00669897 \n",
            "235/390 [=================>............] - ETA: 23s - loss: 1.7274 - acc: 0.5424 - LRFinder: val_loss: 4.0010 - lr = 0.00672000 \n",
            "236/390 [=================>............] - ETA: 23s - loss: 1.7274 - acc: 0.5424 - LRFinder: val_loss: 3.8979 - lr = 0.00674103 \n",
            "237/390 [=================>............] - ETA: 23s - loss: 1.7276 - acc: 0.5423 - LRFinder: val_loss: 3.9236 - lr = 0.00676205 \n",
            "238/390 [=================>............] - ETA: 23s - loss: 1.7278 - acc: 0.5423 - LRFinder: val_loss: 3.8752 - lr = 0.00678308 \n",
            "239/390 [=================>............] - ETA: 23s - loss: 1.7273 - acc: 0.5424 - LRFinder: val_loss: 3.8222 - lr = 0.00680410 \n",
            "240/390 [=================>............] - ETA: 23s - loss: 1.7274 - acc: 0.5425 - LRFinder: val_loss: 3.9272 - lr = 0.00682513 \n",
            "241/390 [=================>............] - ETA: 22s - loss: 1.7266 - acc: 0.5428 - LRFinder: val_loss: 3.9287 - lr = 0.00684615 \n",
            "242/390 [=================>............] - ETA: 22s - loss: 1.7271 - acc: 0.5426 - LRFinder: val_loss: 3.9902 - lr = 0.00686718 \n",
            "243/390 [=================>............] - ETA: 22s - loss: 1.7271 - acc: 0.5427 - LRFinder: val_loss: 4.0623 - lr = 0.00688821 \n",
            "244/390 [=================>............] - ETA: 22s - loss: 1.7266 - acc: 0.5430 - LRFinder: val_loss: 3.9207 - lr = 0.00690923 \n",
            "245/390 [=================>............] - ETA: 22s - loss: 1.7265 - acc: 0.5432 - LRFinder: val_loss: 3.9000 - lr = 0.00693026 \n",
            "246/390 [=================>............] - ETA: 22s - loss: 1.7258 - acc: 0.5436 - LRFinder: val_loss: 3.9651 - lr = 0.00695128 \n",
            "247/390 [==================>...........] - ETA: 21s - loss: 1.7260 - acc: 0.5436 - LRFinder: val_loss: 4.0645 - lr = 0.00697231 \n",
            "248/390 [==================>...........] - ETA: 21s - loss: 1.7256 - acc: 0.5436 - LRFinder: val_loss: 4.0937 - lr = 0.00699333 \n",
            "249/390 [==================>...........] - ETA: 21s - loss: 1.7257 - acc: 0.5435 - LRFinder: val_loss: 4.2620 - lr = 0.00701436 \n",
            "250/390 [==================>...........] - ETA: 21s - loss: 1.7253 - acc: 0.5437 - LRFinder: val_loss: 4.2151 - lr = 0.00703538 \n",
            "251/390 [==================>...........] - ETA: 21s - loss: 1.7249 - acc: 0.5439 - LRFinder: val_loss: 4.3069 - lr = 0.00705641 \n",
            "252/390 [==================>...........] - ETA: 21s - loss: 1.7244 - acc: 0.5440 - LRFinder: val_loss: 4.1665 - lr = 0.00707744 \n",
            "253/390 [==================>...........] - ETA: 20s - loss: 1.7244 - acc: 0.5438 - LRFinder: val_loss: 4.4026 - lr = 0.00709846 \n",
            "254/390 [==================>...........] - ETA: 20s - loss: 1.7240 - acc: 0.5440 - LRFinder: val_loss: 4.2323 - lr = 0.00711949 \n",
            "255/390 [==================>...........] - ETA: 20s - loss: 1.7240 - acc: 0.5440 - LRFinder: val_loss: 4.3457 - lr = 0.00714051 \n",
            "256/390 [==================>...........] - ETA: 20s - loss: 1.7241 - acc: 0.5440 - LRFinder: val_loss: 4.5741 - lr = 0.00716154 \n",
            "257/390 [==================>...........] - ETA: 20s - loss: 1.7239 - acc: 0.5440 - LRFinder: val_loss: 4.6223 - lr = 0.00718256 \n",
            "258/390 [==================>...........] - ETA: 20s - loss: 1.7239 - acc: 0.5440 - LRFinder: val_loss: 4.6279 - lr = 0.00720359 \n",
            "259/390 [==================>...........] - ETA: 20s - loss: 1.7243 - acc: 0.5439 - LRFinder: val_loss: 4.5199 - lr = 0.00722462 \n",
            "260/390 [===================>..........] - ETA: 19s - loss: 1.7246 - acc: 0.5438 - LRFinder: val_loss: 4.3893 - lr = 0.00724564 \n",
            "261/390 [===================>..........] - ETA: 19s - loss: 1.7250 - acc: 0.5437 - LRFinder: val_loss: 4.4606 - lr = 0.00726667 \n",
            "262/390 [===================>..........] - ETA: 19s - loss: 1.7243 - acc: 0.5441 - LRFinder: val_loss: 4.2783 - lr = 0.00728769 \n",
            "263/390 [===================>..........] - ETA: 19s - loss: 1.7241 - acc: 0.5439 - LRFinder: val_loss: 4.2784 - lr = 0.00730872 \n",
            "264/390 [===================>..........] - ETA: 19s - loss: 1.7242 - acc: 0.5438 - LRFinder: val_loss: 4.2368 - lr = 0.00732974 \n",
            "265/390 [===================>..........] - ETA: 19s - loss: 1.7242 - acc: 0.5440 - LRFinder: val_loss: 4.6975 - lr = 0.00735077 \n",
            "266/390 [===================>..........] - ETA: 18s - loss: 1.7246 - acc: 0.5438 - LRFinder: val_loss: 4.6973 - lr = 0.00737179 \n",
            "267/390 [===================>..........] - ETA: 18s - loss: 1.7245 - acc: 0.5439 - LRFinder: val_loss: 4.6883 - lr = 0.00739282 \n",
            "268/390 [===================>..........] - ETA: 18s - loss: 1.7242 - acc: 0.5442 - LRFinder: val_loss: 4.4073 - lr = 0.00741385 \n",
            "269/390 [===================>..........] - ETA: 18s - loss: 1.7242 - acc: 0.5443 - LRFinder: val_loss: 4.2440 - lr = 0.00743487 \n",
            "270/390 [===================>..........] - ETA: 18s - loss: 1.7249 - acc: 0.5442 - LRFinder: val_loss: 4.4144 - lr = 0.00745590 \n",
            "271/390 [===================>..........] - ETA: 18s - loss: 1.7250 - acc: 0.5441 - LRFinder: val_loss: 4.3452 - lr = 0.00747692 \n",
            "272/390 [===================>..........] - ETA: 18s - loss: 1.7251 - acc: 0.5441 - LRFinder: val_loss: 4.2759 - lr = 0.00749795 \n",
            "273/390 [====================>.........] - ETA: 17s - loss: 1.7246 - acc: 0.5443 - LRFinder: val_loss: 4.5906 - lr = 0.00751897 \n",
            "274/390 [====================>.........] - ETA: 17s - loss: 1.7241 - acc: 0.5444 - LRFinder: val_loss: 4.4066 - lr = 0.00754000 \n",
            "275/390 [====================>.........] - ETA: 17s - loss: 1.7240 - acc: 0.5444 - LRFinder: val_loss: 4.5881 - lr = 0.00756103 \n",
            "276/390 [====================>.........] - ETA: 17s - loss: 1.7239 - acc: 0.5444 - LRFinder: val_loss: 4.5390 - lr = 0.00758205 \n",
            "277/390 [====================>.........] - ETA: 17s - loss: 1.7235 - acc: 0.5446 - LRFinder: val_loss: 4.5472 - lr = 0.00760308 \n",
            "278/390 [====================>.........] - ETA: 17s - loss: 1.7234 - acc: 0.5445 - LRFinder: val_loss: 4.5397 - lr = 0.00762410 \n",
            "279/390 [====================>.........] - ETA: 16s - loss: 1.7234 - acc: 0.5445 - LRFinder: val_loss: 4.4539 - lr = 0.00764513 \n",
            "280/390 [====================>.........] - ETA: 16s - loss: 1.7231 - acc: 0.5444 - LRFinder: val_loss: 4.4621 - lr = 0.00766615 \n",
            "281/390 [====================>.........] - ETA: 16s - loss: 1.7236 - acc: 0.5443 - LRFinder: val_loss: 4.6458 - lr = 0.00768718 \n",
            "282/390 [====================>.........] - ETA: 16s - loss: 1.7234 - acc: 0.5443 - LRFinder: val_loss: 4.4519 - lr = 0.00770821 \n",
            "283/390 [====================>.........] - ETA: 16s - loss: 1.7229 - acc: 0.5445 - LRFinder: val_loss: 4.6421 - lr = 0.00772923 \n",
            "284/390 [====================>.........] - ETA: 16s - loss: 1.7227 - acc: 0.5445 - LRFinder: val_loss: 4.7328 - lr = 0.00775026 \n",
            "285/390 [====================>.........] - ETA: 15s - loss: 1.7221 - acc: 0.5446 - LRFinder: val_loss: 4.7757 - lr = 0.00777128 \n",
            "286/390 [=====================>........] - ETA: 15s - loss: 1.7218 - acc: 0.5448 - LRFinder: val_loss: 4.6358 - lr = 0.00779231 \n",
            "287/390 [=====================>........] - ETA: 15s - loss: 1.7216 - acc: 0.5448 - LRFinder: val_loss: 4.8640 - lr = 0.00781333 \n",
            "288/390 [=====================>........] - ETA: 15s - loss: 1.7217 - acc: 0.5448 - LRFinder: val_loss: 4.7108 - lr = 0.00783436 \n",
            "289/390 [=====================>........] - ETA: 15s - loss: 1.7217 - acc: 0.5449 - LRFinder: val_loss: 4.8311 - lr = 0.00785538 \n",
            "290/390 [=====================>........] - ETA: 15s - loss: 1.7214 - acc: 0.5452 - LRFinder: val_loss: 4.6828 - lr = 0.00787641 \n",
            "291/390 [=====================>........] - ETA: 15s - loss: 1.7213 - acc: 0.5452 - LRFinder: val_loss: 4.6524 - lr = 0.00789744 \n",
            "292/390 [=====================>........] - ETA: 14s - loss: 1.7214 - acc: 0.5450 - LRFinder: val_loss: 4.6929 - lr = 0.00791846 \n",
            "293/390 [=====================>........] - ETA: 14s - loss: 1.7219 - acc: 0.5448 - LRFinder: val_loss: 4.8575 - lr = 0.00793949 \n",
            "294/390 [=====================>........] - ETA: 14s - loss: 1.7217 - acc: 0.5447 - LRFinder: val_loss: 4.4776 - lr = 0.00796051 \n",
            "295/390 [=====================>........] - ETA: 14s - loss: 1.7216 - acc: 0.5448 - LRFinder: val_loss: 4.3673 - lr = 0.00798154 \n",
            "296/390 [=====================>........] - ETA: 14s - loss: 1.7218 - acc: 0.5446 - LRFinder: val_loss: 4.3842 - lr = 0.00800256 \n",
            "297/390 [=====================>........] - ETA: 14s - loss: 1.7219 - acc: 0.5446 - LRFinder: val_loss: 4.2468 - lr = 0.00802359 \n",
            "298/390 [=====================>........] - ETA: 13s - loss: 1.7213 - acc: 0.5448 - LRFinder: val_loss: 4.6157 - lr = 0.00804462 \n",
            "299/390 [======================>.......] - ETA: 13s - loss: 1.7209 - acc: 0.5449 - LRFinder: val_loss: 4.4171 - lr = 0.00806564 \n",
            "300/390 [======================>.......] - ETA: 13s - loss: 1.7206 - acc: 0.5449 - LRFinder: val_loss: 4.5287 - lr = 0.00808667 \n",
            "301/390 [======================>.......] - ETA: 13s - loss: 1.7206 - acc: 0.5448 - LRFinder: val_loss: 4.2948 - lr = 0.00810769 \n",
            "302/390 [======================>.......] - ETA: 13s - loss: 1.7206 - acc: 0.5448 - LRFinder: val_loss: 4.2563 - lr = 0.00812872 \n",
            "303/390 [======================>.......] - ETA: 13s - loss: 1.7204 - acc: 0.5448 - LRFinder: val_loss: 4.2443 - lr = 0.00814974 \n",
            "304/390 [======================>.......] - ETA: 13s - loss: 1.7199 - acc: 0.5451 - LRFinder: val_loss: 4.2514 - lr = 0.00817077 \n",
            "305/390 [======================>.......] - ETA: 12s - loss: 1.7195 - acc: 0.5454 - LRFinder: val_loss: 4.1995 - lr = 0.00819179 \n",
            "306/390 [======================>.......] - ETA: 12s - loss: 1.7193 - acc: 0.5453 - LRFinder: val_loss: 4.2943 - lr = 0.00821282 \n",
            "307/390 [======================>.......] - ETA: 12s - loss: 1.7190 - acc: 0.5454 - LRFinder: val_loss: 4.4473 - lr = 0.00823385 \n",
            "308/390 [======================>.......] - ETA: 12s - loss: 1.7183 - acc: 0.5457 - LRFinder: val_loss: 4.2059 - lr = 0.00825487 \n",
            "309/390 [======================>.......] - ETA: 12s - loss: 1.7184 - acc: 0.5457 - LRFinder: val_loss: 4.1557 - lr = 0.00827590 \n",
            "310/390 [======================>.......] - ETA: 12s - loss: 1.7184 - acc: 0.5456 - LRFinder: val_loss: 4.0564 - lr = 0.00829692 \n",
            "311/390 [======================>.......] - ETA: 11s - loss: 1.7186 - acc: 0.5457 - LRFinder: val_loss: 3.9056 - lr = 0.00831795 \n",
            "312/390 [=======================>......] - ETA: 11s - loss: 1.7183 - acc: 0.5459 - LRFinder: val_loss: 3.8806 - lr = 0.00833897 \n",
            "313/390 [=======================>......] - ETA: 11s - loss: 1.7187 - acc: 0.5457 - LRFinder: val_loss: 4.1088 - lr = 0.00836000 \n",
            "314/390 [=======================>......] - ETA: 11s - loss: 1.7182 - acc: 0.5458 - LRFinder: val_loss: 4.0473 - lr = 0.00838103 \n",
            "315/390 [=======================>......] - ETA: 11s - loss: 1.7186 - acc: 0.5457 - LRFinder: val_loss: 4.1931 - lr = 0.00840205 \n",
            "316/390 [=======================>......] - ETA: 11s - loss: 1.7178 - acc: 0.5461 - LRFinder: val_loss: 4.3056 - lr = 0.00842308 \n",
            "317/390 [=======================>......] - ETA: 11s - loss: 1.7175 - acc: 0.5462 - LRFinder: val_loss: 4.1813 - lr = 0.00844410 \n",
            "318/390 [=======================>......] - ETA: 10s - loss: 1.7175 - acc: 0.5461 - LRFinder: val_loss: 3.9613 - lr = 0.00846513 \n",
            "319/390 [=======================>......] - ETA: 10s - loss: 1.7170 - acc: 0.5464 - LRFinder: val_loss: 4.1529 - lr = 0.00848615 \n",
            "320/390 [=======================>......] - ETA: 10s - loss: 1.7163 - acc: 0.5467 - LRFinder: val_loss: 4.2890 - lr = 0.00850718 \n",
            "321/390 [=======================>......] - ETA: 10s - loss: 1.7161 - acc: 0.5467 - LRFinder: val_loss: 4.3729 - lr = 0.00852821 \n",
            "322/390 [=======================>......] - ETA: 10s - loss: 1.7158 - acc: 0.5468 - LRFinder: val_loss: 4.6258 - lr = 0.00854923 \n",
            "323/390 [=======================>......] - ETA: 10s - loss: 1.7156 - acc: 0.5469 - LRFinder: val_loss: 4.4647 - lr = 0.00857026 \n",
            "324/390 [=======================>......] - ETA: 9s - loss: 1.7152 - acc: 0.5471  - LRFinder: val_loss: 4.5545 - lr = 0.00859128 \n",
            "325/390 [========================>.....] - ETA: 9s - loss: 1.7148 - acc: 0.5471 - LRFinder: val_loss: 4.3440 - lr = 0.00861231 \n",
            "326/390 [========================>.....] - ETA: 9s - loss: 1.7145 - acc: 0.5471 - LRFinder: val_loss: 4.3791 - lr = 0.00863333 \n",
            "327/390 [========================>.....] - ETA: 9s - loss: 1.7148 - acc: 0.5469 - LRFinder: val_loss: 4.6666 - lr = 0.00865436 \n",
            "328/390 [========================>.....] - ETA: 9s - loss: 1.7150 - acc: 0.5467 - LRFinder: val_loss: 4.4311 - lr = 0.00867538 \n",
            "329/390 [========================>.....] - ETA: 9s - loss: 1.7149 - acc: 0.5468 - LRFinder: val_loss: 4.4124 - lr = 0.00869641 \n",
            "330/390 [========================>.....] - ETA: 9s - loss: 1.7145 - acc: 0.5470 - LRFinder: val_loss: 4.4984 - lr = 0.00871744 \n",
            "331/390 [========================>.....] - ETA: 8s - loss: 1.7146 - acc: 0.5468 - LRFinder: val_loss: 4.3553 - lr = 0.00873846 \n",
            "332/390 [========================>.....] - ETA: 8s - loss: 1.7149 - acc: 0.5466 - LRFinder: val_loss: 4.2585 - lr = 0.00875949 \n",
            "333/390 [========================>.....] - ETA: 8s - loss: 1.7143 - acc: 0.5468 - LRFinder: val_loss: 4.4727 - lr = 0.00878051 \n",
            "334/390 [========================>.....] - ETA: 8s - loss: 1.7146 - acc: 0.5467 - LRFinder: val_loss: 4.3836 - lr = 0.00880154 \n",
            "335/390 [========================>.....] - ETA: 8s - loss: 1.7143 - acc: 0.5469 - LRFinder: val_loss: 4.4767 - lr = 0.00882256 \n",
            "336/390 [========================>.....] - ETA: 8s - loss: 1.7144 - acc: 0.5469 - LRFinder: val_loss: 4.3832 - lr = 0.00884359 \n",
            "337/390 [========================>.....] - ETA: 8s - loss: 1.7140 - acc: 0.5471 - LRFinder: val_loss: 4.5386 - lr = 0.00886462 \n",
            "338/390 [=========================>....] - ETA: 7s - loss: 1.7137 - acc: 0.5472 - LRFinder: val_loss: 4.2515 - lr = 0.00888564 \n",
            "339/390 [=========================>....] - ETA: 7s - loss: 1.7134 - acc: 0.5474 - LRFinder: val_loss: 4.2464 - lr = 0.00890667 \n",
            "340/390 [=========================>....] - ETA: 7s - loss: 1.7131 - acc: 0.5476 - LRFinder: val_loss: 4.2088 - lr = 0.00892769 \n",
            "341/390 [=========================>....] - ETA: 7s - loss: 1.7128 - acc: 0.5477 - LRFinder: val_loss: 4.2330 - lr = 0.00894872 \n",
            "342/390 [=========================>....] - ETA: 7s - loss: 1.7126 - acc: 0.5477 - LRFinder: val_loss: 4.2071 - lr = 0.00896974 \n",
            "343/390 [=========================>....] - ETA: 7s - loss: 1.7123 - acc: 0.5478 - LRFinder: val_loss: 4.3020 - lr = 0.00899077 \n",
            "344/390 [=========================>....] - ETA: 6s - loss: 1.7120 - acc: 0.5479 - LRFinder: val_loss: 4.2990 - lr = 0.00901179 \n",
            "345/390 [=========================>....] - ETA: 6s - loss: 1.7119 - acc: 0.5478 - LRFinder: val_loss: 4.3260 - lr = 0.00903282 \n",
            "346/390 [=========================>....] - ETA: 6s - loss: 1.7121 - acc: 0.5479 - LRFinder: val_loss: 4.2734 - lr = 0.00905385 \n",
            "347/390 [=========================>....] - ETA: 6s - loss: 1.7119 - acc: 0.5479 - LRFinder: val_loss: 4.3446 - lr = 0.00907487 \n",
            "348/390 [=========================>....] - ETA: 6s - loss: 1.7118 - acc: 0.5479 - LRFinder: val_loss: 4.2050 - lr = 0.00909590 \n",
            "349/390 [=========================>....] - ETA: 6s - loss: 1.7117 - acc: 0.5480 - LRFinder: val_loss: 4.2259 - lr = 0.00911692 \n",
            "350/390 [=========================>....] - ETA: 6s - loss: 1.7114 - acc: 0.5482 - LRFinder: val_loss: 4.4960 - lr = 0.00913795 \n",
            "351/390 [==========================>...] - ETA: 5s - loss: 1.7114 - acc: 0.5481 - LRFinder: val_loss: 4.5401 - lr = 0.00915897 \n",
            "352/390 [==========================>...] - ETA: 5s - loss: 1.7110 - acc: 0.5483 - LRFinder: val_loss: 4.4123 - lr = 0.00918000 \n",
            "353/390 [==========================>...] - ETA: 5s - loss: 1.7107 - acc: 0.5485 - LRFinder: val_loss: 4.4966 - lr = 0.00920103 \n",
            "354/390 [==========================>...] - ETA: 5s - loss: 1.7100 - acc: 0.5487 - LRFinder: val_loss: 4.6899 - lr = 0.00922205 \n",
            "355/390 [==========================>...] - ETA: 5s - loss: 1.7098 - acc: 0.5487 - LRFinder: val_loss: 4.5801 - lr = 0.00924308 \n",
            "356/390 [==========================>...] - ETA: 5s - loss: 1.7096 - acc: 0.5488 - LRFinder: val_loss: 4.6620 - lr = 0.00926410 \n",
            "357/390 [==========================>...] - ETA: 4s - loss: 1.7100 - acc: 0.5488 - LRFinder: val_loss: 4.8017 - lr = 0.00928513 \n",
            "358/390 [==========================>...] - ETA: 4s - loss: 1.7099 - acc: 0.5489 - LRFinder: val_loss: 4.8022 - lr = 0.00930615 \n",
            "359/390 [==========================>...] - ETA: 4s - loss: 1.7097 - acc: 0.5490 - LRFinder: val_loss: 4.4181 - lr = 0.00932718 \n",
            "360/390 [==========================>...] - ETA: 4s - loss: 1.7095 - acc: 0.5490 - LRFinder: val_loss: 4.3179 - lr = 0.00934821 \n",
            "361/390 [==========================>...] - ETA: 4s - loss: 1.7100 - acc: 0.5488 - LRFinder: val_loss: 4.4248 - lr = 0.00936923 \n",
            "362/390 [==========================>...] - ETA: 4s - loss: 1.7102 - acc: 0.5488 - LRFinder: val_loss: 4.1295 - lr = 0.00939026 \n",
            "363/390 [==========================>...] - ETA: 4s - loss: 1.7099 - acc: 0.5490 - LRFinder: val_loss: 4.3482 - lr = 0.00941128 \n",
            "364/390 [===========================>..] - ETA: 3s - loss: 1.7094 - acc: 0.5492 - LRFinder: val_loss: 4.3637 - lr = 0.00943231 \n",
            "365/390 [===========================>..] - ETA: 3s - loss: 1.7087 - acc: 0.5495 - LRFinder: val_loss: 4.4520 - lr = 0.00945333 \n",
            "366/390 [===========================>..] - ETA: 3s - loss: 1.7089 - acc: 0.5494 - LRFinder: val_loss: 4.7774 - lr = 0.00947436 \n",
            "367/390 [===========================>..] - ETA: 3s - loss: 1.7088 - acc: 0.5495 - LRFinder: val_loss: 4.6101 - lr = 0.00949538 \n",
            "368/390 [===========================>..] - ETA: 3s - loss: 1.7080 - acc: 0.5498 - LRFinder: val_loss: 4.5920 - lr = 0.00951641 \n",
            "369/390 [===========================>..] - ETA: 3s - loss: 1.7073 - acc: 0.5499 - LRFinder: val_loss: 4.8543 - lr = 0.00953744 \n",
            "370/390 [===========================>..] - ETA: 3s - loss: 1.7069 - acc: 0.5500 - LRFinder: val_loss: 4.9824 - lr = 0.00955846 \n",
            "371/390 [===========================>..] - ETA: 2s - loss: 1.7063 - acc: 0.5502 - LRFinder: val_loss: 4.7598 - lr = 0.00957949 \n",
            "372/390 [===========================>..] - ETA: 2s - loss: 1.7060 - acc: 0.5502 - LRFinder: val_loss: 4.8578 - lr = 0.00960051 \n",
            "373/390 [===========================>..] - ETA: 2s - loss: 1.7064 - acc: 0.5502 - LRFinder: val_loss: 4.9335 - lr = 0.00962154 \n",
            "374/390 [===========================>..] - ETA: 2s - loss: 1.7065 - acc: 0.5501 - LRFinder: val_loss: 4.6932 - lr = 0.00964256 \n",
            "375/390 [===========================>..] - ETA: 2s - loss: 1.7064 - acc: 0.5501 - LRFinder: val_loss: 4.6586 - lr = 0.00966359 \n",
            "376/390 [===========================>..] - ETA: 2s - loss: 1.7063 - acc: 0.5502 - LRFinder: val_loss: 4.8093 - lr = 0.00968462 \n",
            "377/390 [============================>.] - ETA: 1s - loss: 1.7060 - acc: 0.5501 - LRFinder: val_loss: 4.6423 - lr = 0.00970564 \n",
            "378/390 [============================>.] - ETA: 1s - loss: 1.7059 - acc: 0.5500 - LRFinder: val_loss: 4.8052 - lr = 0.00972667 \n",
            "379/390 [============================>.] - ETA: 1s - loss: 1.7053 - acc: 0.5502 - LRFinder: val_loss: 5.0451 - lr = 0.00974769 \n",
            "380/390 [============================>.] - ETA: 1s - loss: 1.7059 - acc: 0.5500 - LRFinder: val_loss: 5.2021 - lr = 0.00976872 \n",
            "381/390 [============================>.] - ETA: 1s - loss: 1.7057 - acc: 0.5500 - LRFinder: val_loss: 5.3362 - lr = 0.00978974 \n",
            "382/390 [============================>.] - ETA: 1s - loss: 1.7054 - acc: 0.5500 - LRFinder: val_loss: 5.1151 - lr = 0.00981077 \n",
            "383/390 [============================>.] - ETA: 1s - loss: 1.7050 - acc: 0.5501 - LRFinder: val_loss: 5.0526 - lr = 0.00983179 \n",
            "384/390 [============================>.] - ETA: 0s - loss: 1.7051 - acc: 0.5502 - LRFinder: val_loss: 5.3727 - lr = 0.00985282 \n",
            "385/390 [============================>.] - ETA: 0s - loss: 1.7046 - acc: 0.5504 - LRFinder: val_loss: 5.0087 - lr = 0.00987385 \n",
            "386/390 [============================>.] - ETA: 0s - loss: 1.7046 - acc: 0.5503 - LRFinder: val_loss: 5.3784 - lr = 0.00989487 \n",
            "387/390 [============================>.] - ETA: 0s - loss: 1.7044 - acc: 0.5503 - LRFinder: val_loss: 5.1862 - lr = 0.00991590 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 1.7041 - acc: 0.5506 - LRFinder: val_loss: 5.3435 - lr = 0.00993692 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.7040 - acc: 0.5507 - LRFinder: val_loss: 5.2555 - lr = 0.00995795 \n",
            " - LRFinder: val_loss: 4.8897 - lr = 0.00997897 \n",
            "390/390 [==============================] - 60s 154ms/step - loss: 1.7041 - acc: 0.5507 - val_loss: 5.1013 - val_acc: 0.1518\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {weights/momentum/momentum-0.99/}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAESCAYAAAAR2wXeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKsT/Ax2vFRq0kWP0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2gUKJtFkDxv",
        "colab_type": "text"
      },
      "source": [
        "#BEST MOMENTUM IS 0.9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yv5SXdqMa5f",
        "colab_type": "code",
        "outputId": "6ed26570-0991-44a9-c117-c32b1c66781d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# INITIAL WEIGHT DECAY FACTORS\n",
        "# WEIGHT_DECAY_FACTORS = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
        "\n",
        "# FINEGRAINED WEIGHT DECAY FACTORS\n",
        "WEIGHT_DECAY_FACTORS = [1e-7, 3e-7, 3e-6]\n",
        "\n",
        "for weight_decay in WEIGHT_DECAY_FACTORS:\n",
        "     lr_finder = LRFinder(n_train, BATCH_SIZE, minimum_lr=0.0018, maximum_lr=1e-2,\n",
        "                          validation_data=(X_test, Y_test),\n",
        "                          validation_sample_rate=5,\n",
        "                          lr_scale='linear', save_dir='weights/weight_decay/weight_decay-%s/' % str(weight_decay),\n",
        "                          verbose=True)\n",
        "\n",
        "#     # set the weight_decay here !\n",
        "#     # lr doesnt matter as it will be over written by the callback\n",
        "     optimizer = SGD(lr=0.0038, momentum=0.9, nesterov=True)\n",
        "     model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "#\n",
        "#         # Fit the model on the batches generated by datagen.flow().\n",
        "     model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE, shuffle=True),\n",
        "                             steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
        "                             validation_data=(X_test, Y_test),\n",
        "                             epochs=nb_epoch, verbose=1,\n",
        "                             callbacks=[lr_finder])\n",
        "\n",
        "# from plot we see, the model isnt impacted by the weight_decay very much at all\n",
        "# so we can use any of them.\n",
        "\n",
        "for weight_decay in WEIGHT_DECAY_FACTORS:\n",
        "    directory = 'weights/weight_decay/weight_decay-%s/' % str(weight_decay)\n",
        "\n",
        "    losses, lrs = LRFinder.restore_schedule_from_dir(directory, 10, 5)\n",
        "    plt.plot(lrs, losses, label='weight_decay=%0.7f' % weight_decay)\n",
        "\n",
        "plt.title(\"Weight Decay\")\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 11:40 - loss: 1.5831 - acc: 0.5781 - LRFinder: val_loss: 5.0984 - lr = 0.00180000 \n",
            "  2/390 [..............................] - ETA: 8:14 - loss: 1.6028 - acc: 0.5703  - LRFinder: val_loss: 4.9152 - lr = 0.00182103 \n",
            "  3/390 [..............................] - ETA: 5:51 - loss: 1.6175 - acc: 0.5833 - LRFinder: val_loss: 4.9592 - lr = 0.00184205 \n",
            "  4/390 [..............................] - ETA: 4:36 - loss: 1.5910 - acc: 0.5879 - LRFinder: val_loss: 5.1198 - lr = 0.00186308 \n",
            "  5/390 [..............................] - ETA: 3:51 - loss: 1.5728 - acc: 0.6000 - LRFinder: val_loss: 5.1574 - lr = 0.00188410 \n",
            "  6/390 [..............................] - ETA: 3:21 - loss: 1.5761 - acc: 0.5951 - LRFinder: val_loss: 5.1040 - lr = 0.00190513 \n",
            "  7/390 [..............................] - ETA: 3:00 - loss: 1.5927 - acc: 0.5926 - LRFinder: val_loss: 4.7790 - lr = 0.00192615 \n",
            "  8/390 [..............................] - ETA: 2:44 - loss: 1.5647 - acc: 0.6104 - LRFinder: val_loss: 4.8109 - lr = 0.00194718 \n",
            "  9/390 [..............................] - ETA: 2:31 - loss: 1.5555 - acc: 0.6102 - LRFinder: val_loss: 4.7450 - lr = 0.00196821 \n",
            " 10/390 [..............................] - ETA: 2:21 - loss: 1.5584 - acc: 0.6102 - LRFinder: val_loss: 4.8642 - lr = 0.00198923 \n",
            " 11/390 [..............................] - ETA: 2:13 - loss: 1.5629 - acc: 0.6065 - LRFinder: val_loss: 4.8182 - lr = 0.00201026 \n",
            " 12/390 [..............................] - ETA: 2:06 - loss: 1.5573 - acc: 0.6068 - LRFinder: val_loss: 4.6927 - lr = 0.00203128 \n",
            " 13/390 [>.............................] - ETA: 2:00 - loss: 1.5487 - acc: 0.6100 - LRFinder: val_loss: 4.8199 - lr = 0.00205231 \n",
            " 14/390 [>.............................] - ETA: 1:55 - loss: 1.5506 - acc: 0.6088 - LRFinder: val_loss: 4.7789 - lr = 0.00207333 \n",
            " 15/390 [>.............................] - ETA: 1:51 - loss: 1.5543 - acc: 0.6083 - LRFinder: val_loss: 4.5718 - lr = 0.00209436 \n",
            " 16/390 [>.............................] - ETA: 1:47 - loss: 1.5442 - acc: 0.6123 - LRFinder: val_loss: 4.5575 - lr = 0.00211538 \n",
            " 17/390 [>.............................] - ETA: 1:43 - loss: 1.5382 - acc: 0.6149 - LRFinder: val_loss: 4.6340 - lr = 0.00213641 \n",
            " 18/390 [>.............................] - ETA: 1:40 - loss: 1.5372 - acc: 0.6150 - LRFinder: val_loss: 4.5575 - lr = 0.00215744 \n",
            " 19/390 [>.............................] - ETA: 1:38 - loss: 1.5307 - acc: 0.6155 - LRFinder: val_loss: 4.4070 - lr = 0.00217846 \n",
            " 20/390 [>.............................] - ETA: 1:35 - loss: 1.5342 - acc: 0.6160 - LRFinder: val_loss: 4.4748 - lr = 0.00219949 \n",
            " 21/390 [>.............................] - ETA: 1:33 - loss: 1.5327 - acc: 0.6164 - LRFinder: val_loss: 4.3932 - lr = 0.00222051 \n",
            " 22/390 [>.............................] - ETA: 1:31 - loss: 1.5340 - acc: 0.6175 - LRFinder: val_loss: 4.4806 - lr = 0.00224154 \n",
            " 23/390 [>.............................] - ETA: 1:29 - loss: 1.5292 - acc: 0.6179 - LRFinder: val_loss: 4.5065 - lr = 0.00226256 \n",
            " 24/390 [>.............................] - ETA: 1:27 - loss: 1.5371 - acc: 0.6165 - LRFinder: val_loss: 4.3557 - lr = 0.00228359 \n",
            " 25/390 [>.............................] - ETA: 1:26 - loss: 1.5415 - acc: 0.6172 - LRFinder: val_loss: 4.4602 - lr = 0.00230462 \n",
            " 26/390 [=>............................] - ETA: 1:24 - loss: 1.5388 - acc: 0.6172 - LRFinder: val_loss: 4.3657 - lr = 0.00232564 \n",
            " 27/390 [=>............................] - ETA: 1:23 - loss: 1.5368 - acc: 0.6146 - LRFinder: val_loss: 4.4596 - lr = 0.00234667 \n",
            " 28/390 [=>............................] - ETA: 1:21 - loss: 1.5349 - acc: 0.6150 - LRFinder: val_loss: 4.2860 - lr = 0.00236769 \n",
            " 29/390 [=>............................] - ETA: 1:20 - loss: 1.5397 - acc: 0.6137 - LRFinder: val_loss: 4.2119 - lr = 0.00238872 \n",
            " 30/390 [=>............................] - ETA: 1:19 - loss: 1.5420 - acc: 0.6133 - LRFinder: val_loss: 4.4660 - lr = 0.00240974 \n",
            " 31/390 [=>............................] - ETA: 1:18 - loss: 1.5458 - acc: 0.6116 - LRFinder: val_loss: 4.3135 - lr = 0.00243077 \n",
            " 32/390 [=>............................] - ETA: 1:17 - loss: 1.5472 - acc: 0.6101 - LRFinder: val_loss: 4.5678 - lr = 0.00245179 \n",
            " 33/390 [=>............................] - ETA: 1:16 - loss: 1.5456 - acc: 0.6115 - LRFinder: val_loss: 4.3975 - lr = 0.00247282 \n",
            " 34/390 [=>............................] - ETA: 1:15 - loss: 1.5439 - acc: 0.6112 - LRFinder: val_loss: 4.3414 - lr = 0.00249385 \n",
            " 35/390 [=>............................] - ETA: 1:14 - loss: 1.5449 - acc: 0.6107 - LRFinder: val_loss: 4.3923 - lr = 0.00251487 \n",
            " 36/390 [=>............................] - ETA: 1:13 - loss: 1.5461 - acc: 0.6098 - LRFinder: val_loss: 4.3545 - lr = 0.00253590 \n",
            " 37/390 [=>............................] - ETA: 1:12 - loss: 1.5421 - acc: 0.6119 - LRFinder: val_loss: 4.2660 - lr = 0.00255692 \n",
            " 38/390 [=>............................] - ETA: 1:11 - loss: 1.5417 - acc: 0.6110 - LRFinder: val_loss: 4.4446 - lr = 0.00257795 \n",
            " 39/390 [==>...........................] - ETA: 1:11 - loss: 1.5399 - acc: 0.6114 - LRFinder: val_loss: 4.2451 - lr = 0.00259897 \n",
            " 40/390 [==>...........................] - ETA: 1:10 - loss: 1.5411 - acc: 0.6098 - LRFinder: val_loss: 4.2117 - lr = 0.00262000 \n",
            " 41/390 [==>...........................] - ETA: 1:09 - loss: 1.5377 - acc: 0.6120 - LRFinder: val_loss: 4.2550 - lr = 0.00264103 \n",
            " 42/390 [==>...........................] - ETA: 1:09 - loss: 1.5367 - acc: 0.6124 - LRFinder: val_loss: 4.2934 - lr = 0.00266205 \n",
            " 43/390 [==>...........................] - ETA: 1:08 - loss: 1.5351 - acc: 0.6136 - LRFinder: val_loss: 4.2190 - lr = 0.00268308 \n",
            " 44/390 [==>...........................] - ETA: 1:07 - loss: 1.5392 - acc: 0.6119 - LRFinder: val_loss: 4.1227 - lr = 0.00270410 \n",
            " 45/390 [==>...........................] - ETA: 1:07 - loss: 1.5430 - acc: 0.6115 - LRFinder: val_loss: 4.1421 - lr = 0.00272513 \n",
            " 46/390 [==>...........................] - ETA: 1:06 - loss: 1.5414 - acc: 0.6114 - LRFinder: val_loss: 4.3134 - lr = 0.00274615 \n",
            " 47/390 [==>...........................] - ETA: 1:06 - loss: 1.5384 - acc: 0.6122 - LRFinder: val_loss: 4.3067 - lr = 0.00276718 \n",
            " 48/390 [==>...........................] - ETA: 1:05 - loss: 1.5407 - acc: 0.6117 - LRFinder: val_loss: 4.0966 - lr = 0.00278821 \n",
            " 49/390 [==>...........................] - ETA: 1:05 - loss: 1.5393 - acc: 0.6118 - LRFinder: val_loss: 4.0143 - lr = 0.00280923 \n",
            " 50/390 [==>...........................] - ETA: 1:04 - loss: 1.5402 - acc: 0.6117 - LRFinder: val_loss: 4.1212 - lr = 0.00283026 \n",
            " 51/390 [==>...........................] - ETA: 1:04 - loss: 1.5391 - acc: 0.6117 - LRFinder: val_loss: 4.0201 - lr = 0.00285128 \n",
            " 52/390 [===>..........................] - ETA: 1:03 - loss: 1.5365 - acc: 0.6127 - LRFinder: val_loss: 4.0297 - lr = 0.00287231 \n",
            " 53/390 [===>..........................] - ETA: 1:03 - loss: 1.5358 - acc: 0.6131 - LRFinder: val_loss: 4.0434 - lr = 0.00289333 \n",
            " 54/390 [===>..........................] - ETA: 1:02 - loss: 1.5356 - acc: 0.6126 - LRFinder: val_loss: 4.2438 - lr = 0.00291436 \n",
            " 55/390 [===>..........................] - ETA: 1:02 - loss: 1.5358 - acc: 0.6129 - LRFinder: val_loss: 4.0167 - lr = 0.00293538 \n",
            " 56/390 [===>..........................] - ETA: 1:01 - loss: 1.5369 - acc: 0.6117 - LRFinder: val_loss: 4.0506 - lr = 0.00295641 \n",
            " 57/390 [===>..........................] - ETA: 1:01 - loss: 1.5349 - acc: 0.6132 - LRFinder: val_loss: 4.0162 - lr = 0.00297744 \n",
            " 58/390 [===>..........................] - ETA: 1:00 - loss: 1.5361 - acc: 0.6129 - LRFinder: val_loss: 4.1278 - lr = 0.00299846 \n",
            " 59/390 [===>..........................] - ETA: 1:00 - loss: 1.5364 - acc: 0.6120 - LRFinder: val_loss: 4.0795 - lr = 0.00301949 \n",
            " 60/390 [===>..........................] - ETA: 1:00 - loss: 1.5357 - acc: 0.6116 - LRFinder: val_loss: 4.0344 - lr = 0.00304051 \n",
            " 61/390 [===>..........................] - ETA: 59s - loss: 1.5358 - acc: 0.6110  - LRFinder: val_loss: 4.0265 - lr = 0.00306154 \n",
            " 62/390 [===>..........................] - ETA: 59s - loss: 1.5350 - acc: 0.6116 - LRFinder: val_loss: 3.9086 - lr = 0.00308256 \n",
            " 63/390 [===>..........................] - ETA: 59s - loss: 1.5365 - acc: 0.6114 - LRFinder: val_loss: 3.8137 - lr = 0.00310359 \n",
            " 64/390 [===>..........................] - ETA: 58s - loss: 1.5377 - acc: 0.6110 - LRFinder: val_loss: 4.0350 - lr = 0.00312462 \n",
            " 65/390 [====>.........................] - ETA: 58s - loss: 1.5385 - acc: 0.6108 - LRFinder: val_loss: 3.9338 - lr = 0.00314564 \n",
            " 66/390 [====>.........................] - ETA: 57s - loss: 1.5378 - acc: 0.6125 - LRFinder: val_loss: 3.8353 - lr = 0.00316667 \n",
            " 67/390 [====>.........................] - ETA: 57s - loss: 1.5377 - acc: 0.6119 - LRFinder: val_loss: 3.9242 - lr = 0.00318769 \n",
            " 68/390 [====>.........................] - ETA: 57s - loss: 1.5396 - acc: 0.6117 - LRFinder: val_loss: 3.8089 - lr = 0.00320872 \n",
            " 69/390 [====>.........................] - ETA: 56s - loss: 1.5379 - acc: 0.6119 - LRFinder: val_loss: 4.0048 - lr = 0.00322974 \n",
            " 70/390 [====>.........................] - ETA: 56s - loss: 1.5384 - acc: 0.6116 - LRFinder: val_loss: 3.9527 - lr = 0.00325077 \n",
            " 71/390 [====>.........................] - ETA: 56s - loss: 1.5377 - acc: 0.6119 - LRFinder: val_loss: 3.9357 - lr = 0.00327179 \n",
            " 72/390 [====>.........................] - ETA: 55s - loss: 1.5398 - acc: 0.6109 - LRFinder: val_loss: 4.1063 - lr = 0.00329282 \n",
            " 73/390 [====>.........................] - ETA: 55s - loss: 1.5379 - acc: 0.6112 - LRFinder: val_loss: 3.9721 - lr = 0.00331385 \n",
            " 74/390 [====>.........................] - ETA: 55s - loss: 1.5363 - acc: 0.6119 - LRFinder: val_loss: 4.0278 - lr = 0.00333487 \n",
            " 75/390 [====>.........................] - ETA: 55s - loss: 1.5377 - acc: 0.6115 - LRFinder: val_loss: 4.0336 - lr = 0.00335590 \n",
            " 76/390 [====>.........................] - ETA: 54s - loss: 1.5377 - acc: 0.6116 - LRFinder: val_loss: 4.0675 - lr = 0.00337692 \n",
            " 77/390 [====>.........................] - ETA: 54s - loss: 1.5385 - acc: 0.6118 - LRFinder: val_loss: 4.2301 - lr = 0.00339795 \n",
            " 78/390 [=====>........................] - ETA: 54s - loss: 1.5389 - acc: 0.6116 - LRFinder: val_loss: 4.2477 - lr = 0.00341897 \n",
            " 79/390 [=====>........................] - ETA: 53s - loss: 1.5377 - acc: 0.6112 - LRFinder: val_loss: 4.1314 - lr = 0.00344000 \n",
            " 80/390 [=====>........................] - ETA: 53s - loss: 1.5382 - acc: 0.6104 - LRFinder: val_loss: 4.1731 - lr = 0.00346103 \n",
            " 81/390 [=====>........................] - ETA: 53s - loss: 1.5381 - acc: 0.6103 - LRFinder: val_loss: 4.3254 - lr = 0.00348205 \n",
            " 82/390 [=====>........................] - ETA: 52s - loss: 1.5367 - acc: 0.6108 - LRFinder: val_loss: 4.2861 - lr = 0.00350308 \n",
            " 83/390 [=====>........................] - ETA: 52s - loss: 1.5380 - acc: 0.6105 - LRFinder: val_loss: 4.3102 - lr = 0.00352410 \n",
            " 84/390 [=====>........................] - ETA: 52s - loss: 1.5372 - acc: 0.6109 - LRFinder: val_loss: 4.4912 - lr = 0.00354513 \n",
            " 85/390 [=====>........................] - ETA: 52s - loss: 1.5370 - acc: 0.6109 - LRFinder: val_loss: 4.0915 - lr = 0.00356615 \n",
            " 86/390 [=====>........................] - ETA: 51s - loss: 1.5383 - acc: 0.6105 - LRFinder: val_loss: 4.3446 - lr = 0.00358718 \n",
            " 87/390 [=====>........................] - ETA: 51s - loss: 1.5371 - acc: 0.6106 - LRFinder: val_loss: 4.2529 - lr = 0.00360821 \n",
            " 88/390 [=====>........................] - ETA: 51s - loss: 1.5354 - acc: 0.6114 - LRFinder: val_loss: 4.2319 - lr = 0.00362923 \n",
            " 89/390 [=====>........................] - ETA: 51s - loss: 1.5355 - acc: 0.6114 - LRFinder: val_loss: 4.1849 - lr = 0.00365026 \n",
            " 90/390 [=====>........................] - ETA: 50s - loss: 1.5358 - acc: 0.6109 - LRFinder: val_loss: 4.0805 - lr = 0.00367128 \n",
            " 91/390 [======>.......................] - ETA: 50s - loss: 1.5363 - acc: 0.6107 - LRFinder: val_loss: 4.3236 - lr = 0.00369231 \n",
            " 92/390 [======>.......................] - ETA: 50s - loss: 1.5342 - acc: 0.6116 - LRFinder: val_loss: 4.1766 - lr = 0.00371333 \n",
            " 93/390 [======>.......................] - ETA: 50s - loss: 1.5355 - acc: 0.6112 - LRFinder: val_loss: 4.0842 - lr = 0.00373436 \n",
            " 94/390 [======>.......................] - ETA: 49s - loss: 1.5350 - acc: 0.6115 - LRFinder: val_loss: 4.2749 - lr = 0.00375538 \n",
            " 95/390 [======>.......................] - ETA: 49s - loss: 1.5333 - acc: 0.6123 - LRFinder: val_loss: 4.1437 - lr = 0.00377641 \n",
            " 96/390 [======>.......................] - ETA: 49s - loss: 1.5347 - acc: 0.6119 - LRFinder: val_loss: 4.1486 - lr = 0.00379744 \n",
            " 97/390 [======>.......................] - ETA: 49s - loss: 1.5351 - acc: 0.6123 - LRFinder: val_loss: 3.9806 - lr = 0.00381846 \n",
            " 98/390 [======>.......................] - ETA: 48s - loss: 1.5347 - acc: 0.6125 - LRFinder: val_loss: 4.0575 - lr = 0.00383949 \n",
            " 99/390 [======>.......................] - ETA: 48s - loss: 1.5345 - acc: 0.6121 - LRFinder: val_loss: 4.1665 - lr = 0.00386051 \n",
            "100/390 [======>.......................] - ETA: 48s - loss: 1.5348 - acc: 0.6118 - LRFinder: val_loss: 4.0815 - lr = 0.00388154 \n",
            "101/390 [======>.......................] - ETA: 48s - loss: 1.5335 - acc: 0.6120 - LRFinder: val_loss: 4.0955 - lr = 0.00390256 \n",
            "102/390 [======>.......................] - ETA: 48s - loss: 1.5320 - acc: 0.6125 - LRFinder: val_loss: 4.3089 - lr = 0.00392359 \n",
            "103/390 [======>.......................] - ETA: 47s - loss: 1.5312 - acc: 0.6126 - LRFinder: val_loss: 4.1873 - lr = 0.00394462 \n",
            "104/390 [=======>......................] - ETA: 47s - loss: 1.5299 - acc: 0.6129 - LRFinder: val_loss: 4.1662 - lr = 0.00396564 \n",
            "105/390 [=======>......................] - ETA: 47s - loss: 1.5290 - acc: 0.6132 - LRFinder: val_loss: 4.2596 - lr = 0.00398667 \n",
            "106/390 [=======>......................] - ETA: 47s - loss: 1.5287 - acc: 0.6133 - LRFinder: val_loss: 4.0560 - lr = 0.00400769 \n",
            "107/390 [=======>......................] - ETA: 46s - loss: 1.5284 - acc: 0.6134 - LRFinder: val_loss: 4.2782 - lr = 0.00402872 \n",
            "108/390 [=======>......................] - ETA: 46s - loss: 1.5280 - acc: 0.6131 - LRFinder: val_loss: 4.1086 - lr = 0.00404974 \n",
            "109/390 [=======>......................] - ETA: 46s - loss: 1.5281 - acc: 0.6132 - LRFinder: val_loss: 4.1425 - lr = 0.00407077 \n",
            "110/390 [=======>......................] - ETA: 46s - loss: 1.5280 - acc: 0.6131 - LRFinder: val_loss: 4.0377 - lr = 0.00409179 \n",
            "111/390 [=======>......................] - ETA: 46s - loss: 1.5292 - acc: 0.6127 - LRFinder: val_loss: 4.0611 - lr = 0.00411282 \n",
            "112/390 [=======>......................] - ETA: 45s - loss: 1.5303 - acc: 0.6126 - LRFinder: val_loss: 4.0263 - lr = 0.00413385 \n",
            "113/390 [=======>......................] - ETA: 45s - loss: 1.5300 - acc: 0.6132 - LRFinder: val_loss: 4.0175 - lr = 0.00415487 \n",
            "114/390 [=======>......................] - ETA: 45s - loss: 1.5285 - acc: 0.6139 - LRFinder: val_loss: 4.1580 - lr = 0.00417590 \n",
            "115/390 [=======>......................] - ETA: 45s - loss: 1.5294 - acc: 0.6135 - LRFinder: val_loss: 3.9748 - lr = 0.00419692 \n",
            "116/390 [=======>......................] - ETA: 44s - loss: 1.5298 - acc: 0.6133 - LRFinder: val_loss: 4.0507 - lr = 0.00421795 \n",
            "117/390 [========>.....................] - ETA: 44s - loss: 1.5297 - acc: 0.6132 - LRFinder: val_loss: 4.0064 - lr = 0.00423897 \n",
            "118/390 [========>.....................] - ETA: 44s - loss: 1.5294 - acc: 0.6135 - LRFinder: val_loss: 3.8321 - lr = 0.00426000 \n",
            "119/390 [========>.....................] - ETA: 44s - loss: 1.5292 - acc: 0.6136 - LRFinder: val_loss: 3.8366 - lr = 0.00428103 \n",
            "120/390 [========>.....................] - ETA: 44s - loss: 1.5302 - acc: 0.6136 - LRFinder: val_loss: 3.9145 - lr = 0.00430205 \n",
            "121/390 [========>.....................] - ETA: 43s - loss: 1.5296 - acc: 0.6142 - LRFinder: val_loss: 4.0009 - lr = 0.00432308 \n",
            "122/390 [========>.....................] - ETA: 43s - loss: 1.5290 - acc: 0.6146 - LRFinder: val_loss: 3.8719 - lr = 0.00434410 \n",
            "123/390 [========>.....................] - ETA: 43s - loss: 1.5287 - acc: 0.6150 - LRFinder: val_loss: 3.9720 - lr = 0.00436513 \n",
            "124/390 [========>.....................] - ETA: 43s - loss: 1.5304 - acc: 0.6139 - LRFinder: val_loss: 4.0469 - lr = 0.00438615 \n",
            "125/390 [========>.....................] - ETA: 43s - loss: 1.5307 - acc: 0.6136 - LRFinder: val_loss: 3.9083 - lr = 0.00440718 \n",
            "126/390 [========>.....................] - ETA: 42s - loss: 1.5313 - acc: 0.6133 - LRFinder: val_loss: 4.0496 - lr = 0.00442821 \n",
            "127/390 [========>.....................] - ETA: 42s - loss: 1.5314 - acc: 0.6134 - LRFinder: val_loss: 4.0037 - lr = 0.00444923 \n",
            "128/390 [========>.....................] - ETA: 42s - loss: 1.5319 - acc: 0.6135 - LRFinder: val_loss: 3.9295 - lr = 0.00447026 \n",
            "129/390 [========>.....................] - ETA: 42s - loss: 1.5315 - acc: 0.6135 - LRFinder: val_loss: 4.0191 - lr = 0.00449128 \n",
            "130/390 [=========>....................] - ETA: 42s - loss: 1.5299 - acc: 0.6140 - LRFinder: val_loss: 4.0365 - lr = 0.00451231 \n",
            "131/390 [=========>....................] - ETA: 41s - loss: 1.5297 - acc: 0.6140 - LRFinder: val_loss: 4.0281 - lr = 0.00453333 \n",
            "132/390 [=========>....................] - ETA: 41s - loss: 1.5283 - acc: 0.6144 - LRFinder: val_loss: 4.0347 - lr = 0.00455436 \n",
            "133/390 [=========>....................] - ETA: 41s - loss: 1.5274 - acc: 0.6144 - LRFinder: val_loss: 3.9342 - lr = 0.00457538 \n",
            "134/390 [=========>....................] - ETA: 41s - loss: 1.5263 - acc: 0.6148 - LRFinder: val_loss: 3.9475 - lr = 0.00459641 \n",
            "135/390 [=========>....................] - ETA: 41s - loss: 1.5259 - acc: 0.6149 - LRFinder: val_loss: 4.0893 - lr = 0.00461744 \n",
            "136/390 [=========>....................] - ETA: 41s - loss: 1.5250 - acc: 0.6152 - LRFinder: val_loss: 4.0188 - lr = 0.00463846 \n",
            "137/390 [=========>....................] - ETA: 40s - loss: 1.5245 - acc: 0.6153 - LRFinder: val_loss: 4.0398 - lr = 0.00465949 \n",
            "138/390 [=========>....................] - ETA: 40s - loss: 1.5245 - acc: 0.6154 - LRFinder: val_loss: 4.0715 - lr = 0.00468051 \n",
            "139/390 [=========>....................] - ETA: 40s - loss: 1.5246 - acc: 0.6154 - LRFinder: val_loss: 4.0168 - lr = 0.00470154 \n",
            "140/390 [=========>....................] - ETA: 40s - loss: 1.5245 - acc: 0.6159 - LRFinder: val_loss: 3.8902 - lr = 0.00472256 \n",
            "141/390 [=========>....................] - ETA: 40s - loss: 1.5254 - acc: 0.6154 - LRFinder: val_loss: 3.9992 - lr = 0.00474359 \n",
            "142/390 [=========>....................] - ETA: 39s - loss: 1.5247 - acc: 0.6158 - LRFinder: val_loss: 4.0504 - lr = 0.00476462 \n",
            "143/390 [==========>...................] - ETA: 39s - loss: 1.5242 - acc: 0.6159 - LRFinder: val_loss: 4.0755 - lr = 0.00478564 \n",
            "144/390 [==========>...................] - ETA: 39s - loss: 1.5236 - acc: 0.6160 - LRFinder: val_loss: 3.9117 - lr = 0.00480667 \n",
            "145/390 [==========>...................] - ETA: 39s - loss: 1.5222 - acc: 0.6165 - LRFinder: val_loss: 4.2708 - lr = 0.00482769 \n",
            "146/390 [==========>...................] - ETA: 39s - loss: 1.5211 - acc: 0.6169 - LRFinder: val_loss: 4.0696 - lr = 0.00484872 \n",
            "147/390 [==========>...................] - ETA: 38s - loss: 1.5222 - acc: 0.6164 - LRFinder: val_loss: 3.9642 - lr = 0.00486974 \n",
            "148/390 [==========>...................] - ETA: 38s - loss: 1.5220 - acc: 0.6164 - LRFinder: val_loss: 4.1671 - lr = 0.00489077 \n",
            "149/390 [==========>...................] - ETA: 38s - loss: 1.5222 - acc: 0.6168 - LRFinder: val_loss: 4.0216 - lr = 0.00491179 \n",
            "150/390 [==========>...................] - ETA: 38s - loss: 1.5224 - acc: 0.6166 - LRFinder: val_loss: 4.0549 - lr = 0.00493282 \n",
            "151/390 [==========>...................] - ETA: 38s - loss: 1.5219 - acc: 0.6167 - LRFinder: val_loss: 4.0934 - lr = 0.00495385 \n",
            "152/390 [==========>...................] - ETA: 38s - loss: 1.5217 - acc: 0.6168 - LRFinder: val_loss: 4.1322 - lr = 0.00497487 \n",
            "153/390 [==========>...................] - ETA: 37s - loss: 1.5207 - acc: 0.6171 - LRFinder: val_loss: 4.2758 - lr = 0.00499590 \n",
            "154/390 [==========>...................] - ETA: 37s - loss: 1.5203 - acc: 0.6173 - LRFinder: val_loss: 3.9813 - lr = 0.00501692 \n",
            "155/390 [==========>...................] - ETA: 37s - loss: 1.5212 - acc: 0.6167 - LRFinder: val_loss: 4.1085 - lr = 0.00503795 \n",
            "156/390 [===========>..................] - ETA: 37s - loss: 1.5204 - acc: 0.6169 - LRFinder: val_loss: 3.7570 - lr = 0.00505897 \n",
            "157/390 [===========>..................] - ETA: 37s - loss: 1.5201 - acc: 0.6171 - LRFinder: val_loss: 3.9800 - lr = 0.00508000 \n",
            "158/390 [===========>..................] - ETA: 36s - loss: 1.5196 - acc: 0.6174 - LRFinder: val_loss: 4.1117 - lr = 0.00510103 \n",
            "159/390 [===========>..................] - ETA: 36s - loss: 1.5193 - acc: 0.6177 - LRFinder: val_loss: 4.1195 - lr = 0.00512205 \n",
            "160/390 [===========>..................] - ETA: 36s - loss: 1.5187 - acc: 0.6176 - LRFinder: val_loss: 4.0792 - lr = 0.00514308 \n",
            "161/390 [===========>..................] - ETA: 36s - loss: 1.5182 - acc: 0.6179 - LRFinder: val_loss: 4.1613 - lr = 0.00516410 \n",
            "162/390 [===========>..................] - ETA: 36s - loss: 1.5180 - acc: 0.6182 - LRFinder: val_loss: 4.3161 - lr = 0.00518513 \n",
            "163/390 [===========>..................] - ETA: 36s - loss: 1.5179 - acc: 0.6180 - LRFinder: val_loss: 4.0649 - lr = 0.00520615 \n",
            "164/390 [===========>..................] - ETA: 35s - loss: 1.5162 - acc: 0.6186 - LRFinder: val_loss: 4.1271 - lr = 0.00522718 \n",
            "165/390 [===========>..................] - ETA: 35s - loss: 1.5162 - acc: 0.6187 - LRFinder: val_loss: 4.1578 - lr = 0.00524821 \n",
            "166/390 [===========>..................] - ETA: 35s - loss: 1.5154 - acc: 0.6189 - LRFinder: val_loss: 4.0702 - lr = 0.00526923 \n",
            "167/390 [===========>..................] - ETA: 35s - loss: 1.5144 - acc: 0.6191 - LRFinder: val_loss: 4.1755 - lr = 0.00529026 \n",
            "168/390 [===========>..................] - ETA: 35s - loss: 1.5137 - acc: 0.6193 - LRFinder: val_loss: 4.0127 - lr = 0.00531128 \n",
            "169/390 [============>.................] - ETA: 34s - loss: 1.5132 - acc: 0.6194 - LRFinder: val_loss: 4.0372 - lr = 0.00533231 \n",
            "170/390 [============>.................] - ETA: 34s - loss: 1.5131 - acc: 0.6194 - LRFinder: val_loss: 4.0960 - lr = 0.00535333 \n",
            "171/390 [============>.................] - ETA: 34s - loss: 1.5126 - acc: 0.6199 - LRFinder: val_loss: 3.9468 - lr = 0.00537436 \n",
            "172/390 [============>.................] - ETA: 34s - loss: 1.5127 - acc: 0.6200 - LRFinder: val_loss: 4.0854 - lr = 0.00539538 \n",
            "173/390 [============>.................] - ETA: 34s - loss: 1.5120 - acc: 0.6204 - LRFinder: val_loss: 4.0482 - lr = 0.00541641 \n",
            "174/390 [============>.................] - ETA: 34s - loss: 1.5117 - acc: 0.6206 - LRFinder: val_loss: 4.0865 - lr = 0.00543744 \n",
            "175/390 [============>.................] - ETA: 33s - loss: 1.5112 - acc: 0.6207 - LRFinder: val_loss: 3.9516 - lr = 0.00545846 \n",
            "176/390 [============>.................] - ETA: 33s - loss: 1.5115 - acc: 0.6207 - LRFinder: val_loss: 3.9270 - lr = 0.00547949 \n",
            "177/390 [============>.................] - ETA: 33s - loss: 1.5112 - acc: 0.6205 - LRFinder: val_loss: 4.1193 - lr = 0.00550051 \n",
            "178/390 [============>.................] - ETA: 33s - loss: 1.5108 - acc: 0.6206 - LRFinder: val_loss: 3.9419 - lr = 0.00552154 \n",
            "179/390 [============>.................] - ETA: 33s - loss: 1.5112 - acc: 0.6207 - LRFinder: val_loss: 4.0659 - lr = 0.00554256 \n",
            "180/390 [============>.................] - ETA: 33s - loss: 1.5109 - acc: 0.6207 - LRFinder: val_loss: 4.1721 - lr = 0.00556359 \n",
            "181/390 [============>.................] - ETA: 32s - loss: 1.5102 - acc: 0.6211 - LRFinder: val_loss: 3.9226 - lr = 0.00558462 \n",
            "182/390 [=============>................] - ETA: 32s - loss: 1.5102 - acc: 0.6213 - LRFinder: val_loss: 3.9349 - lr = 0.00560564 \n",
            "183/390 [=============>................] - ETA: 32s - loss: 1.5101 - acc: 0.6211 - LRFinder: val_loss: 3.8248 - lr = 0.00562667 \n",
            "184/390 [=============>................] - ETA: 32s - loss: 1.5091 - acc: 0.6214 - LRFinder: val_loss: 3.8007 - lr = 0.00564769 \n",
            "185/390 [=============>................] - ETA: 32s - loss: 1.5088 - acc: 0.6216 - LRFinder: val_loss: 3.8661 - lr = 0.00566872 \n",
            "186/390 [=============>................] - ETA: 32s - loss: 1.5083 - acc: 0.6218 - LRFinder: val_loss: 4.0273 - lr = 0.00568974 \n",
            "187/390 [=============>................] - ETA: 31s - loss: 1.5080 - acc: 0.6221 - LRFinder: val_loss: 3.8223 - lr = 0.00571077 \n",
            "188/390 [=============>................] - ETA: 31s - loss: 1.5084 - acc: 0.6220 - LRFinder: val_loss: 3.7142 - lr = 0.00573179 \n",
            "189/390 [=============>................] - ETA: 31s - loss: 1.5083 - acc: 0.6219 - LRFinder: val_loss: 3.8931 - lr = 0.00575282 \n",
            "190/390 [=============>................] - ETA: 31s - loss: 1.5080 - acc: 0.6219 - LRFinder: val_loss: 3.7901 - lr = 0.00577385 \n",
            "191/390 [=============>................] - ETA: 31s - loss: 1.5080 - acc: 0.6218 - LRFinder: val_loss: 3.9057 - lr = 0.00579487 \n",
            "192/390 [=============>................] - ETA: 31s - loss: 1.5079 - acc: 0.6219 - LRFinder: val_loss: 3.7777 - lr = 0.00581590 \n",
            "193/390 [=============>................] - ETA: 30s - loss: 1.5076 - acc: 0.6220 - LRFinder: val_loss: 3.8576 - lr = 0.00583692 \n",
            "194/390 [=============>................] - ETA: 30s - loss: 1.5073 - acc: 0.6219 - LRFinder: val_loss: 3.8789 - lr = 0.00585795 \n",
            "195/390 [==============>...............] - ETA: 30s - loss: 1.5069 - acc: 0.6223 - LRFinder: val_loss: 3.8628 - lr = 0.00587897 \n",
            "196/390 [==============>...............] - ETA: 30s - loss: 1.5062 - acc: 0.6225 - LRFinder: val_loss: 3.7191 - lr = 0.00590000 \n",
            "197/390 [==============>...............] - ETA: 30s - loss: 1.5055 - acc: 0.6228 - LRFinder: val_loss: 3.7721 - lr = 0.00592103 \n",
            "198/390 [==============>...............] - ETA: 30s - loss: 1.5057 - acc: 0.6227 - LRFinder: val_loss: 3.9742 - lr = 0.00594205 \n",
            "199/390 [==============>...............] - ETA: 29s - loss: 1.5048 - acc: 0.6229 - LRFinder: val_loss: 3.9216 - lr = 0.00596308 \n",
            "200/390 [==============>...............] - ETA: 29s - loss: 1.5052 - acc: 0.6229 - LRFinder: val_loss: 3.6782 - lr = 0.00598410 \n",
            "201/390 [==============>...............] - ETA: 29s - loss: 1.5048 - acc: 0.6231 - LRFinder: val_loss: 3.9172 - lr = 0.00600513 \n",
            "202/390 [==============>...............] - ETA: 29s - loss: 1.5050 - acc: 0.6230 - LRFinder: val_loss: 3.8660 - lr = 0.00602615 \n",
            "203/390 [==============>...............] - ETA: 29s - loss: 1.5046 - acc: 0.6230 - LRFinder: val_loss: 3.8371 - lr = 0.00604718 \n",
            "204/390 [==============>...............] - ETA: 29s - loss: 1.5043 - acc: 0.6232 - LRFinder: val_loss: 3.7501 - lr = 0.00606821 \n",
            "205/390 [==============>...............] - ETA: 28s - loss: 1.5039 - acc: 0.6231 - LRFinder: val_loss: 3.9247 - lr = 0.00608923 \n",
            "206/390 [==============>...............] - ETA: 28s - loss: 1.5042 - acc: 0.6228 - LRFinder: val_loss: 3.7674 - lr = 0.00611026 \n",
            "207/390 [==============>...............] - ETA: 28s - loss: 1.5043 - acc: 0.6227 - LRFinder: val_loss: 3.7446 - lr = 0.00613128 \n",
            "208/390 [===============>..............] - ETA: 28s - loss: 1.5039 - acc: 0.6226 - LRFinder: val_loss: 3.6781 - lr = 0.00615231 \n",
            "209/390 [===============>..............] - ETA: 28s - loss: 1.5035 - acc: 0.6226 - LRFinder: val_loss: 3.8132 - lr = 0.00617333 \n",
            "210/390 [===============>..............] - ETA: 28s - loss: 1.5034 - acc: 0.6227 - LRFinder: val_loss: 3.7421 - lr = 0.00619436 \n",
            "211/390 [===============>..............] - ETA: 27s - loss: 1.5028 - acc: 0.6230 - LRFinder: val_loss: 3.8898 - lr = 0.00621538 \n",
            "212/390 [===============>..............] - ETA: 27s - loss: 1.5036 - acc: 0.6225 - LRFinder: val_loss: 3.9138 - lr = 0.00623641 \n",
            "213/390 [===============>..............] - ETA: 27s - loss: 1.5032 - acc: 0.6228 - LRFinder: val_loss: 3.9113 - lr = 0.00625744 \n",
            "214/390 [===============>..............] - ETA: 27s - loss: 1.5027 - acc: 0.6231 - LRFinder: val_loss: 3.8026 - lr = 0.00627846 \n",
            "215/390 [===============>..............] - ETA: 27s - loss: 1.5028 - acc: 0.6231 - LRFinder: val_loss: 3.8946 - lr = 0.00629949 \n",
            "216/390 [===============>..............] - ETA: 27s - loss: 1.5025 - acc: 0.6233 - LRFinder: val_loss: 3.7916 - lr = 0.00632051 \n",
            "217/390 [===============>..............] - ETA: 26s - loss: 1.5030 - acc: 0.6230 - LRFinder: val_loss: 3.8498 - lr = 0.00634154 \n",
            "218/390 [===============>..............] - ETA: 26s - loss: 1.5030 - acc: 0.6228 - LRFinder: val_loss: 3.9452 - lr = 0.00636256 \n",
            "219/390 [===============>..............] - ETA: 26s - loss: 1.5030 - acc: 0.6225 - LRFinder: val_loss: 3.8658 - lr = 0.00638359 \n",
            "220/390 [===============>..............] - ETA: 26s - loss: 1.5021 - acc: 0.6231 - LRFinder: val_loss: 3.9745 - lr = 0.00640462 \n",
            "221/390 [================>.............] - ETA: 26s - loss: 1.5018 - acc: 0.6235 - LRFinder: val_loss: 3.8868 - lr = 0.00642564 \n",
            "222/390 [================>.............] - ETA: 26s - loss: 1.5016 - acc: 0.6236 - LRFinder: val_loss: 3.7040 - lr = 0.00644667 \n",
            "223/390 [================>.............] - ETA: 25s - loss: 1.5016 - acc: 0.6237 - LRFinder: val_loss: 3.7952 - lr = 0.00646769 \n",
            "224/390 [================>.............] - ETA: 25s - loss: 1.5018 - acc: 0.6235 - LRFinder: val_loss: 3.7879 - lr = 0.00648872 \n",
            "225/390 [================>.............] - ETA: 25s - loss: 1.5017 - acc: 0.6236 - LRFinder: val_loss: 3.8543 - lr = 0.00650974 \n",
            "226/390 [================>.............] - ETA: 25s - loss: 1.5018 - acc: 0.6237 - LRFinder: val_loss: 3.9019 - lr = 0.00653077 \n",
            "227/390 [================>.............] - ETA: 25s - loss: 1.5018 - acc: 0.6237 - LRFinder: val_loss: 3.8115 - lr = 0.00655179 \n",
            "228/390 [================>.............] - ETA: 25s - loss: 1.5012 - acc: 0.6240 - LRFinder: val_loss: 3.9203 - lr = 0.00657282 \n",
            "229/390 [================>.............] - ETA: 24s - loss: 1.5011 - acc: 0.6239 - LRFinder: val_loss: 4.0508 - lr = 0.00659385 \n",
            "230/390 [================>.............] - ETA: 24s - loss: 1.5009 - acc: 0.6240 - LRFinder: val_loss: 4.0756 - lr = 0.00661487 \n",
            "231/390 [================>.............] - ETA: 24s - loss: 1.5009 - acc: 0.6238 - LRFinder: val_loss: 3.9471 - lr = 0.00663590 \n",
            "232/390 [================>.............] - ETA: 24s - loss: 1.5004 - acc: 0.6238 - LRFinder: val_loss: 3.9079 - lr = 0.00665692 \n",
            "233/390 [================>.............] - ETA: 24s - loss: 1.4998 - acc: 0.6240 - LRFinder: val_loss: 3.8449 - lr = 0.00667795 \n",
            "234/390 [=================>............] - ETA: 24s - loss: 1.5001 - acc: 0.6239 - LRFinder: val_loss: 4.0188 - lr = 0.00669897 \n",
            "235/390 [=================>............] - ETA: 23s - loss: 1.5000 - acc: 0.6241 - LRFinder: val_loss: 3.8487 - lr = 0.00672000 \n",
            "236/390 [=================>............] - ETA: 23s - loss: 1.4998 - acc: 0.6243 - LRFinder: val_loss: 3.6738 - lr = 0.00674103 \n",
            "237/390 [=================>............] - ETA: 23s - loss: 1.4991 - acc: 0.6245 - LRFinder: val_loss: 3.6511 - lr = 0.00676205 \n",
            "238/390 [=================>............] - ETA: 23s - loss: 1.4988 - acc: 0.6245 - LRFinder: val_loss: 3.7766 - lr = 0.00678308 \n",
            "239/390 [=================>............] - ETA: 23s - loss: 1.4992 - acc: 0.6243 - LRFinder: val_loss: 3.6822 - lr = 0.00680410 \n",
            "240/390 [=================>............] - ETA: 23s - loss: 1.4979 - acc: 0.6248 - LRFinder: val_loss: 3.6740 - lr = 0.00682513 \n",
            "241/390 [=================>............] - ETA: 23s - loss: 1.4975 - acc: 0.6252 - LRFinder: val_loss: 3.6296 - lr = 0.00684615 \n",
            "242/390 [=================>............] - ETA: 22s - loss: 1.4971 - acc: 0.6253 - LRFinder: val_loss: 3.7607 - lr = 0.00686718 \n",
            "243/390 [=================>............] - ETA: 22s - loss: 1.4966 - acc: 0.6255 - LRFinder: val_loss: 3.7695 - lr = 0.00688821 \n",
            "244/390 [=================>............] - ETA: 22s - loss: 1.4961 - acc: 0.6256 - LRFinder: val_loss: 3.7442 - lr = 0.00690923 \n",
            "245/390 [=================>............] - ETA: 22s - loss: 1.4959 - acc: 0.6255 - LRFinder: val_loss: 3.6211 - lr = 0.00693026 \n",
            "246/390 [=================>............] - ETA: 22s - loss: 1.4957 - acc: 0.6255 - LRFinder: val_loss: 3.6688 - lr = 0.00695128 \n",
            "247/390 [==================>...........] - ETA: 22s - loss: 1.4954 - acc: 0.6258 - LRFinder: val_loss: 3.6151 - lr = 0.00697231 \n",
            "248/390 [==================>...........] - ETA: 21s - loss: 1.4954 - acc: 0.6260 - LRFinder: val_loss: 3.6947 - lr = 0.00699333 \n",
            "249/390 [==================>...........] - ETA: 21s - loss: 1.4960 - acc: 0.6258 - LRFinder: val_loss: 3.7766 - lr = 0.00701436 \n",
            "250/390 [==================>...........] - ETA: 21s - loss: 1.4954 - acc: 0.6260 - LRFinder: val_loss: 3.7939 - lr = 0.00703538 \n",
            "251/390 [==================>...........] - ETA: 21s - loss: 1.4957 - acc: 0.6257 - LRFinder: val_loss: 3.8328 - lr = 0.00705641 \n",
            "252/390 [==================>...........] - ETA: 21s - loss: 1.4964 - acc: 0.6255 - LRFinder: val_loss: 3.9304 - lr = 0.00707744 \n",
            "253/390 [==================>...........] - ETA: 21s - loss: 1.4970 - acc: 0.6253 - LRFinder: val_loss: 3.8910 - lr = 0.00709846 \n",
            "254/390 [==================>...........] - ETA: 20s - loss: 1.4966 - acc: 0.6253 - LRFinder: val_loss: 3.8508 - lr = 0.00711949 \n",
            "255/390 [==================>...........] - ETA: 20s - loss: 1.4968 - acc: 0.6254 - LRFinder: val_loss: 3.7333 - lr = 0.00714051 \n",
            "256/390 [==================>...........] - ETA: 20s - loss: 1.4966 - acc: 0.6256 - LRFinder: val_loss: 3.7573 - lr = 0.00716154 \n",
            "257/390 [==================>...........] - ETA: 20s - loss: 1.4964 - acc: 0.6257 - LRFinder: val_loss: 3.6707 - lr = 0.00718256 \n",
            "258/390 [==================>...........] - ETA: 20s - loss: 1.4961 - acc: 0.6258 - LRFinder: val_loss: 3.7713 - lr = 0.00720359 \n",
            "259/390 [==================>...........] - ETA: 20s - loss: 1.4956 - acc: 0.6260 - LRFinder: val_loss: 3.6900 - lr = 0.00722462 \n",
            "260/390 [===================>..........] - ETA: 20s - loss: 1.4956 - acc: 0.6260 - LRFinder: val_loss: 3.7463 - lr = 0.00724564 \n",
            "261/390 [===================>..........] - ETA: 19s - loss: 1.4953 - acc: 0.6260 - LRFinder: val_loss: 3.6617 - lr = 0.00726667 \n",
            "262/390 [===================>..........] - ETA: 19s - loss: 1.4953 - acc: 0.6261 - LRFinder: val_loss: 3.8995 - lr = 0.00728769 \n",
            "263/390 [===================>..........] - ETA: 19s - loss: 1.4947 - acc: 0.6263 - LRFinder: val_loss: 3.9658 - lr = 0.00730872 \n",
            "264/390 [===================>..........] - ETA: 19s - loss: 1.4947 - acc: 0.6261 - LRFinder: val_loss: 4.0548 - lr = 0.00732974 \n",
            "265/390 [===================>..........] - ETA: 19s - loss: 1.4947 - acc: 0.6261 - LRFinder: val_loss: 3.9161 - lr = 0.00735077 \n",
            "266/390 [===================>..........] - ETA: 19s - loss: 1.4946 - acc: 0.6261 - LRFinder: val_loss: 4.0958 - lr = 0.00737179 \n",
            "267/390 [===================>..........] - ETA: 18s - loss: 1.4946 - acc: 0.6261 - LRFinder: val_loss: 4.0957 - lr = 0.00739282 \n",
            "268/390 [===================>..........] - ETA: 18s - loss: 1.4943 - acc: 0.6262 - LRFinder: val_loss: 3.9188 - lr = 0.00741385 \n",
            "269/390 [===================>..........] - ETA: 18s - loss: 1.4943 - acc: 0.6262 - LRFinder: val_loss: 3.9192 - lr = 0.00743487 \n",
            "270/390 [===================>..........] - ETA: 18s - loss: 1.4942 - acc: 0.6265 - LRFinder: val_loss: 3.9373 - lr = 0.00745590 \n",
            "271/390 [===================>..........] - ETA: 18s - loss: 1.4936 - acc: 0.6267 - LRFinder: val_loss: 3.8769 - lr = 0.00747692 \n",
            "272/390 [===================>..........] - ETA: 18s - loss: 1.4935 - acc: 0.6268 - LRFinder: val_loss: 3.9945 - lr = 0.00749795 \n",
            "273/390 [====================>.........] - ETA: 17s - loss: 1.4936 - acc: 0.6267 - LRFinder: val_loss: 4.0385 - lr = 0.00751897 \n",
            "274/390 [====================>.........] - ETA: 17s - loss: 1.4936 - acc: 0.6266 - LRFinder: val_loss: 3.9818 - lr = 0.00754000 \n",
            "275/390 [====================>.........] - ETA: 17s - loss: 1.4936 - acc: 0.6267 - LRFinder: val_loss: 4.0674 - lr = 0.00756103 \n",
            "276/390 [====================>.........] - ETA: 17s - loss: 1.4938 - acc: 0.6268 - LRFinder: val_loss: 3.9002 - lr = 0.00758205 \n",
            "277/390 [====================>.........] - ETA: 17s - loss: 1.4938 - acc: 0.6268 - LRFinder: val_loss: 3.7226 - lr = 0.00760308 \n",
            "278/390 [====================>.........] - ETA: 17s - loss: 1.4936 - acc: 0.6269 - LRFinder: val_loss: 3.9639 - lr = 0.00762410 \n",
            "279/390 [====================>.........] - ETA: 17s - loss: 1.4940 - acc: 0.6266 - LRFinder: val_loss: 3.9228 - lr = 0.00764513 \n",
            "280/390 [====================>.........] - ETA: 16s - loss: 1.4939 - acc: 0.6266 - LRFinder: val_loss: 3.8096 - lr = 0.00766615 \n",
            "281/390 [====================>.........] - ETA: 16s - loss: 1.4934 - acc: 0.6268 - LRFinder: val_loss: 3.9750 - lr = 0.00768718 \n",
            "282/390 [====================>.........] - ETA: 16s - loss: 1.4935 - acc: 0.6266 - LRFinder: val_loss: 4.0283 - lr = 0.00770821 \n",
            "283/390 [====================>.........] - ETA: 16s - loss: 1.4937 - acc: 0.6264 - LRFinder: val_loss: 4.0515 - lr = 0.00772923 \n",
            "284/390 [====================>.........] - ETA: 16s - loss: 1.4929 - acc: 0.6267 - LRFinder: val_loss: 3.9656 - lr = 0.00775026 \n",
            "285/390 [====================>.........] - ETA: 16s - loss: 1.4928 - acc: 0.6268 - LRFinder: val_loss: 3.9756 - lr = 0.00777128 \n",
            "286/390 [=====================>........] - ETA: 15s - loss: 1.4927 - acc: 0.6268 - LRFinder: val_loss: 3.9018 - lr = 0.00779231 \n",
            "287/390 [=====================>........] - ETA: 15s - loss: 1.4926 - acc: 0.6269 - LRFinder: val_loss: 3.9911 - lr = 0.00781333 \n",
            "288/390 [=====================>........] - ETA: 15s - loss: 1.4923 - acc: 0.6269 - LRFinder: val_loss: 3.9291 - lr = 0.00783436 \n",
            "289/390 [=====================>........] - ETA: 15s - loss: 1.4922 - acc: 0.6268 - LRFinder: val_loss: 3.7848 - lr = 0.00785538 \n",
            "290/390 [=====================>........] - ETA: 15s - loss: 1.4922 - acc: 0.6268 - LRFinder: val_loss: 3.9283 - lr = 0.00787641 \n",
            "291/390 [=====================>........] - ETA: 15s - loss: 1.4920 - acc: 0.6268 - LRFinder: val_loss: 3.9525 - lr = 0.00789744 \n",
            "292/390 [=====================>........] - ETA: 14s - loss: 1.4921 - acc: 0.6269 - LRFinder: val_loss: 3.8940 - lr = 0.00791846 \n",
            "293/390 [=====================>........] - ETA: 14s - loss: 1.4926 - acc: 0.6265 - LRFinder: val_loss: 3.9872 - lr = 0.00793949 \n",
            "294/390 [=====================>........] - ETA: 14s - loss: 1.4924 - acc: 0.6267 - LRFinder: val_loss: 3.9698 - lr = 0.00796051 \n",
            "295/390 [=====================>........] - ETA: 14s - loss: 1.4924 - acc: 0.6267 - LRFinder: val_loss: 3.7828 - lr = 0.00798154 \n",
            "296/390 [=====================>........] - ETA: 14s - loss: 1.4923 - acc: 0.6267 - LRFinder: val_loss: 3.7768 - lr = 0.00800256 \n",
            "297/390 [=====================>........] - ETA: 14s - loss: 1.4922 - acc: 0.6267 - LRFinder: val_loss: 3.7372 - lr = 0.00802359 \n",
            "298/390 [=====================>........] - ETA: 14s - loss: 1.4925 - acc: 0.6267 - LRFinder: val_loss: 3.5190 - lr = 0.00804462 \n",
            "299/390 [======================>.......] - ETA: 13s - loss: 1.4918 - acc: 0.6268 - LRFinder: val_loss: 3.7866 - lr = 0.00806564 \n",
            "300/390 [======================>.......] - ETA: 13s - loss: 1.4917 - acc: 0.6267 - LRFinder: val_loss: 3.7535 - lr = 0.00808667 \n",
            "301/390 [======================>.......] - ETA: 13s - loss: 1.4917 - acc: 0.6266 - LRFinder: val_loss: 3.7225 - lr = 0.00810769 \n",
            "302/390 [======================>.......] - ETA: 13s - loss: 1.4922 - acc: 0.6263 - LRFinder: val_loss: 3.6737 - lr = 0.00812872 \n",
            "303/390 [======================>.......] - ETA: 13s - loss: 1.4923 - acc: 0.6264 - LRFinder: val_loss: 3.6551 - lr = 0.00814974 \n",
            "304/390 [======================>.......] - ETA: 13s - loss: 1.4919 - acc: 0.6264 - LRFinder: val_loss: 3.7554 - lr = 0.00817077 \n",
            "305/390 [======================>.......] - ETA: 12s - loss: 1.4919 - acc: 0.6264 - LRFinder: val_loss: 3.9532 - lr = 0.00819179 \n",
            "306/390 [======================>.......] - ETA: 12s - loss: 1.4916 - acc: 0.6264 - LRFinder: val_loss: 3.9445 - lr = 0.00821282 \n",
            "307/390 [======================>.......] - ETA: 12s - loss: 1.4915 - acc: 0.6263 - LRFinder: val_loss: 3.7544 - lr = 0.00823385 \n",
            "308/390 [======================>.......] - ETA: 12s - loss: 1.4919 - acc: 0.6260 - LRFinder: val_loss: 3.8449 - lr = 0.00825487 \n",
            "309/390 [======================>.......] - ETA: 12s - loss: 1.4915 - acc: 0.6261 - LRFinder: val_loss: 4.0033 - lr = 0.00827590 \n",
            "310/390 [======================>.......] - ETA: 12s - loss: 1.4914 - acc: 0.6260 - LRFinder: val_loss: 3.8956 - lr = 0.00829692 \n",
            "311/390 [======================>.......] - ETA: 12s - loss: 1.4911 - acc: 0.6262 - LRFinder: val_loss: 3.7530 - lr = 0.00831795 \n",
            "312/390 [=======================>......] - ETA: 11s - loss: 1.4907 - acc: 0.6263 - LRFinder: val_loss: 3.9972 - lr = 0.00833897 \n",
            "313/390 [=======================>......] - ETA: 11s - loss: 1.4899 - acc: 0.6266 - LRFinder: val_loss: 3.9036 - lr = 0.00836000 \n",
            "314/390 [=======================>......] - ETA: 11s - loss: 1.4898 - acc: 0.6265 - LRFinder: val_loss: 3.7968 - lr = 0.00838103 \n",
            "315/390 [=======================>......] - ETA: 11s - loss: 1.4894 - acc: 0.6266 - LRFinder: val_loss: 3.9719 - lr = 0.00840205 \n",
            "316/390 [=======================>......] - ETA: 11s - loss: 1.4890 - acc: 0.6268 - LRFinder: val_loss: 4.0077 - lr = 0.00842308 \n",
            "317/390 [=======================>......] - ETA: 11s - loss: 1.4894 - acc: 0.6267 - LRFinder: val_loss: 4.0596 - lr = 0.00844410 \n",
            "318/390 [=======================>......] - ETA: 10s - loss: 1.4890 - acc: 0.6268 - LRFinder: val_loss: 3.9845 - lr = 0.00846513 \n",
            "319/390 [=======================>......] - ETA: 10s - loss: 1.4884 - acc: 0.6271 - LRFinder: val_loss: 3.8565 - lr = 0.00848615 \n",
            "320/390 [=======================>......] - ETA: 10s - loss: 1.4883 - acc: 0.6271 - LRFinder: val_loss: 3.8857 - lr = 0.00850718 \n",
            "321/390 [=======================>......] - ETA: 10s - loss: 1.4879 - acc: 0.6272 - LRFinder: val_loss: 3.8482 - lr = 0.00852821 \n",
            "322/390 [=======================>......] - ETA: 10s - loss: 1.4876 - acc: 0.6273 - LRFinder: val_loss: 3.8716 - lr = 0.00854923 \n",
            "323/390 [=======================>......] - ETA: 10s - loss: 1.4875 - acc: 0.6273 - LRFinder: val_loss: 3.7851 - lr = 0.00857026 \n",
            "324/390 [=======================>......] - ETA: 10s - loss: 1.4873 - acc: 0.6274 - LRFinder: val_loss: 3.7747 - lr = 0.00859128 \n",
            "325/390 [========================>.....] - ETA: 9s - loss: 1.4873 - acc: 0.6275  - LRFinder: val_loss: 3.7833 - lr = 0.00861231 \n",
            "326/390 [========================>.....] - ETA: 9s - loss: 1.4872 - acc: 0.6276 - LRFinder: val_loss: 3.6020 - lr = 0.00863333 \n",
            "327/390 [========================>.....] - ETA: 9s - loss: 1.4868 - acc: 0.6278 - LRFinder: val_loss: 3.6904 - lr = 0.00865436 \n",
            "328/390 [========================>.....] - ETA: 9s - loss: 1.4866 - acc: 0.6279 - LRFinder: val_loss: 3.7030 - lr = 0.00867538 \n",
            "329/390 [========================>.....] - ETA: 9s - loss: 1.4866 - acc: 0.6280 - LRFinder: val_loss: 3.7041 - lr = 0.00869641 \n",
            "330/390 [========================>.....] - ETA: 9s - loss: 1.4867 - acc: 0.6280 - LRFinder: val_loss: 3.7394 - lr = 0.00871744 \n",
            "331/390 [========================>.....] - ETA: 8s - loss: 1.4866 - acc: 0.6280 - LRFinder: val_loss: 3.9975 - lr = 0.00873846 \n",
            "332/390 [========================>.....] - ETA: 8s - loss: 1.4867 - acc: 0.6280 - LRFinder: val_loss: 3.7842 - lr = 0.00875949 \n",
            "333/390 [========================>.....] - ETA: 8s - loss: 1.4864 - acc: 0.6281 - LRFinder: val_loss: 3.7577 - lr = 0.00878051 \n",
            "334/390 [========================>.....] - ETA: 8s - loss: 1.4862 - acc: 0.6280 - LRFinder: val_loss: 3.7939 - lr = 0.00880154 \n",
            "335/390 [========================>.....] - ETA: 8s - loss: 1.4859 - acc: 0.6282 - LRFinder: val_loss: 3.8321 - lr = 0.00882256 \n",
            "336/390 [========================>.....] - ETA: 8s - loss: 1.4858 - acc: 0.6283 - LRFinder: val_loss: 3.9353 - lr = 0.00884359 \n",
            "337/390 [========================>.....] - ETA: 8s - loss: 1.4852 - acc: 0.6285 - LRFinder: val_loss: 3.8058 - lr = 0.00886462 \n",
            "338/390 [=========================>....] - ETA: 7s - loss: 1.4855 - acc: 0.6283 - LRFinder: val_loss: 3.9384 - lr = 0.00888564 \n",
            "339/390 [=========================>....] - ETA: 7s - loss: 1.4862 - acc: 0.6281 - LRFinder: val_loss: 4.0701 - lr = 0.00890667 \n",
            "340/390 [=========================>....] - ETA: 7s - loss: 1.4862 - acc: 0.6282 - LRFinder: val_loss: 3.8297 - lr = 0.00892769 \n",
            "341/390 [=========================>....] - ETA: 7s - loss: 1.4865 - acc: 0.6281 - LRFinder: val_loss: 3.8277 - lr = 0.00894872 \n",
            "342/390 [=========================>....] - ETA: 7s - loss: 1.4864 - acc: 0.6280 - LRFinder: val_loss: 3.7856 - lr = 0.00896974 \n",
            "343/390 [=========================>....] - ETA: 7s - loss: 1.4863 - acc: 0.6280 - LRFinder: val_loss: 3.8324 - lr = 0.00899077 \n",
            "344/390 [=========================>....] - ETA: 6s - loss: 1.4865 - acc: 0.6280 - LRFinder: val_loss: 3.8736 - lr = 0.00901179 \n",
            "345/390 [=========================>....] - ETA: 6s - loss: 1.4867 - acc: 0.6279 - LRFinder: val_loss: 3.9881 - lr = 0.00903282 \n",
            "346/390 [=========================>....] - ETA: 6s - loss: 1.4864 - acc: 0.6280 - LRFinder: val_loss: 3.9281 - lr = 0.00905385 \n",
            "347/390 [=========================>....] - ETA: 6s - loss: 1.4862 - acc: 0.6279 - LRFinder: val_loss: 3.9326 - lr = 0.00907487 \n",
            "348/390 [=========================>....] - ETA: 6s - loss: 1.4862 - acc: 0.6281 - LRFinder: val_loss: 4.1689 - lr = 0.00909590 \n",
            "349/390 [=========================>....] - ETA: 6s - loss: 1.4863 - acc: 0.6279 - LRFinder: val_loss: 3.8434 - lr = 0.00911692 \n",
            "350/390 [=========================>....] - ETA: 6s - loss: 1.4857 - acc: 0.6282 - LRFinder: val_loss: 3.9821 - lr = 0.00913795 \n",
            "351/390 [==========================>...] - ETA: 5s - loss: 1.4859 - acc: 0.6280 - LRFinder: val_loss: 3.9857 - lr = 0.00915897 \n",
            "352/390 [==========================>...] - ETA: 5s - loss: 1.4858 - acc: 0.6281 - LRFinder: val_loss: 3.9576 - lr = 0.00918000 \n",
            "353/390 [==========================>...] - ETA: 5s - loss: 1.4861 - acc: 0.6280 - LRFinder: val_loss: 3.9461 - lr = 0.00920103 \n",
            "354/390 [==========================>...] - ETA: 5s - loss: 1.4861 - acc: 0.6279 - LRFinder: val_loss: 3.7540 - lr = 0.00922205 \n",
            "355/390 [==========================>...] - ETA: 5s - loss: 1.4861 - acc: 0.6279 - LRFinder: val_loss: 3.9735 - lr = 0.00924308 \n",
            "356/390 [==========================>...] - ETA: 5s - loss: 1.4857 - acc: 0.6280 - LRFinder: val_loss: 3.8062 - lr = 0.00926410 \n",
            "357/390 [==========================>...] - ETA: 4s - loss: 1.4857 - acc: 0.6279 - LRFinder: val_loss: 3.9983 - lr = 0.00928513 \n",
            "358/390 [==========================>...] - ETA: 4s - loss: 1.4860 - acc: 0.6278 - LRFinder: val_loss: 3.8955 - lr = 0.00930615 \n",
            "359/390 [==========================>...] - ETA: 4s - loss: 1.4859 - acc: 0.6277 - LRFinder: val_loss: 3.6593 - lr = 0.00932718 \n",
            "360/390 [==========================>...] - ETA: 4s - loss: 1.4856 - acc: 0.6278 - LRFinder: val_loss: 3.9458 - lr = 0.00934821 \n",
            "361/390 [==========================>...] - ETA: 4s - loss: 1.4857 - acc: 0.6278 - LRFinder: val_loss: 3.9423 - lr = 0.00936923 \n",
            "362/390 [==========================>...] - ETA: 4s - loss: 1.4857 - acc: 0.6277 - LRFinder: val_loss: 3.8459 - lr = 0.00939026 \n",
            "363/390 [==========================>...] - ETA: 4s - loss: 1.4863 - acc: 0.6274 - LRFinder: val_loss: 3.7111 - lr = 0.00941128 \n",
            "364/390 [===========================>..] - ETA: 3s - loss: 1.4861 - acc: 0.6274 - LRFinder: val_loss: 3.6831 - lr = 0.00943231 \n",
            "365/390 [===========================>..] - ETA: 3s - loss: 1.4861 - acc: 0.6272 - LRFinder: val_loss: 3.7575 - lr = 0.00945333 \n",
            "366/390 [===========================>..] - ETA: 3s - loss: 1.4862 - acc: 0.6271 - LRFinder: val_loss: 3.7024 - lr = 0.00947436 \n",
            "367/390 [===========================>..] - ETA: 3s - loss: 1.4858 - acc: 0.6274 - LRFinder: val_loss: 3.9149 - lr = 0.00949538 \n",
            "368/390 [===========================>..] - ETA: 3s - loss: 1.4854 - acc: 0.6275 - LRFinder: val_loss: 3.8050 - lr = 0.00951641 \n",
            "369/390 [===========================>..] - ETA: 3s - loss: 1.4856 - acc: 0.6274 - LRFinder: val_loss: 3.7398 - lr = 0.00953744 \n",
            "370/390 [===========================>..] - ETA: 3s - loss: 1.4850 - acc: 0.6277 - LRFinder: val_loss: 3.7947 - lr = 0.00955846 \n",
            "371/390 [===========================>..] - ETA: 2s - loss: 1.4849 - acc: 0.6279 - LRFinder: val_loss: 3.9394 - lr = 0.00957949 \n",
            "372/390 [===========================>..] - ETA: 2s - loss: 1.4849 - acc: 0.6279 - LRFinder: val_loss: 4.0027 - lr = 0.00960051 \n",
            "373/390 [===========================>..] - ETA: 2s - loss: 1.4843 - acc: 0.6280 - LRFinder: val_loss: 3.8640 - lr = 0.00962154 \n",
            "374/390 [===========================>..] - ETA: 2s - loss: 1.4840 - acc: 0.6282 - LRFinder: val_loss: 3.9537 - lr = 0.00964256 \n",
            "375/390 [===========================>..] - ETA: 2s - loss: 1.4840 - acc: 0.6282 - LRFinder: val_loss: 3.8274 - lr = 0.00966359 \n",
            "376/390 [===========================>..] - ETA: 2s - loss: 1.4837 - acc: 0.6284 - LRFinder: val_loss: 3.8697 - lr = 0.00968462 \n",
            "377/390 [============================>.] - ETA: 1s - loss: 1.4831 - acc: 0.6286 - LRFinder: val_loss: 3.9086 - lr = 0.00970564 \n",
            "378/390 [============================>.] - ETA: 1s - loss: 1.4830 - acc: 0.6287 - LRFinder: val_loss: 3.8225 - lr = 0.00972667 \n",
            "379/390 [============================>.] - ETA: 1s - loss: 1.4831 - acc: 0.6287 - LRFinder: val_loss: 3.8603 - lr = 0.00974769 \n",
            "380/390 [============================>.] - ETA: 1s - loss: 1.4831 - acc: 0.6287 - LRFinder: val_loss: 3.8658 - lr = 0.00976872 \n",
            "381/390 [============================>.] - ETA: 1s - loss: 1.4832 - acc: 0.6288 - LRFinder: val_loss: 3.7726 - lr = 0.00978974 \n",
            "382/390 [============================>.] - ETA: 1s - loss: 1.4833 - acc: 0.6286 - LRFinder: val_loss: 3.8718 - lr = 0.00981077 \n",
            "383/390 [============================>.] - ETA: 1s - loss: 1.4835 - acc: 0.6285 - LRFinder: val_loss: 3.7663 - lr = 0.00983179 \n",
            "384/390 [============================>.] - ETA: 0s - loss: 1.4834 - acc: 0.6286 - LRFinder: val_loss: 3.7709 - lr = 0.00985282 \n",
            "385/390 [============================>.] - ETA: 0s - loss: 1.4831 - acc: 0.6287 - LRFinder: val_loss: 3.9479 - lr = 0.00987385 \n",
            "386/390 [============================>.] - ETA: 0s - loss: 1.4827 - acc: 0.6288 - LRFinder: val_loss: 3.8848 - lr = 0.00989487 \n",
            "387/390 [============================>.] - ETA: 0s - loss: 1.4831 - acc: 0.6288 - LRFinder: val_loss: 4.0093 - lr = 0.00991590 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 1.4831 - acc: 0.6288 - LRFinder: val_loss: 4.1164 - lr = 0.00993692 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.4839 - acc: 0.6287 - LRFinder: val_loss: 4.1571 - lr = 0.00995795 \n",
            " - LRFinder: val_loss: 3.9639 - lr = 0.00997897 \n",
            "390/390 [==============================] - 60s 155ms/step - loss: 1.4840 - acc: 0.6286 - val_loss: 4.0119 - val_acc: 0.1721\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {weights/weight_decay/weight_decay-1e-07/}\n",
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 11:49 - loss: 1.4397 - acc: 0.6250 - LRFinder: val_loss: 3.8779 - lr = 0.00180000 \n",
            "  2/390 [..............................] - ETA: 8:31 - loss: 1.3840 - acc: 0.6289  - LRFinder: val_loss: 4.1312 - lr = 0.00182103 \n",
            "  3/390 [..............................] - ETA: 6:01 - loss: 1.4272 - acc: 0.6224 - LRFinder: val_loss: 4.0792 - lr = 0.00184205 \n",
            "  4/390 [..............................] - ETA: 4:45 - loss: 1.4078 - acc: 0.6250 - LRFinder: val_loss: 4.0001 - lr = 0.00186308 \n",
            "  5/390 [..............................] - ETA: 3:58 - loss: 1.4038 - acc: 0.6312 - LRFinder: val_loss: 4.4345 - lr = 0.00188410 \n",
            "  6/390 [..............................] - ETA: 3:27 - loss: 1.4011 - acc: 0.6315 - LRFinder: val_loss: 4.0714 - lr = 0.00190513 \n",
            "  7/390 [..............................] - ETA: 3:05 - loss: 1.4001 - acc: 0.6328 - LRFinder: val_loss: 4.0190 - lr = 0.00192615 \n",
            "  8/390 [..............................] - ETA: 2:48 - loss: 1.4055 - acc: 0.6348 - LRFinder: val_loss: 4.1031 - lr = 0.00194718 \n",
            "  9/390 [..............................] - ETA: 2:35 - loss: 1.4098 - acc: 0.6345 - LRFinder: val_loss: 4.0904 - lr = 0.00196821 \n",
            " 10/390 [..............................] - ETA: 2:24 - loss: 1.3964 - acc: 0.6438 - LRFinder: val_loss: 4.0811 - lr = 0.00198923 \n",
            " 11/390 [..............................] - ETA: 2:16 - loss: 1.4054 - acc: 0.6428 - LRFinder: val_loss: 4.0794 - lr = 0.00201026 \n",
            " 12/390 [..............................] - ETA: 2:09 - loss: 1.4107 - acc: 0.6432 - LRFinder: val_loss: 4.0640 - lr = 0.00203128 \n",
            " 13/390 [>.............................] - ETA: 2:03 - loss: 1.4130 - acc: 0.6466 - LRFinder: val_loss: 4.0474 - lr = 0.00205231 \n",
            " 14/390 [>.............................] - ETA: 1:57 - loss: 1.4188 - acc: 0.6429 - LRFinder: val_loss: 3.9291 - lr = 0.00207333 \n",
            " 15/390 [>.............................] - ETA: 1:53 - loss: 1.4188 - acc: 0.6443 - LRFinder: val_loss: 4.0212 - lr = 0.00209436 \n",
            " 16/390 [>.............................] - ETA: 1:49 - loss: 1.4240 - acc: 0.6416 - LRFinder: val_loss: 3.9793 - lr = 0.00211538 \n",
            " 17/390 [>.............................] - ETA: 1:45 - loss: 1.4317 - acc: 0.6369 - LRFinder: val_loss: 4.1540 - lr = 0.00213641 \n",
            " 18/390 [>.............................] - ETA: 1:42 - loss: 1.4378 - acc: 0.6350 - LRFinder: val_loss: 4.1076 - lr = 0.00215744 \n",
            " 19/390 [>.............................] - ETA: 1:39 - loss: 1.4312 - acc: 0.6369 - LRFinder: val_loss: 4.0650 - lr = 0.00217846 \n",
            " 20/390 [>.............................] - ETA: 1:37 - loss: 1.4264 - acc: 0.6410 - LRFinder: val_loss: 4.0115 - lr = 0.00219949 \n",
            " 21/390 [>.............................] - ETA: 1:34 - loss: 1.4249 - acc: 0.6403 - LRFinder: val_loss: 3.9226 - lr = 0.00222051 \n",
            " 22/390 [>.............................] - ETA: 1:32 - loss: 1.4271 - acc: 0.6385 - LRFinder: val_loss: 4.1034 - lr = 0.00224154 \n",
            " 23/390 [>.............................] - ETA: 1:30 - loss: 1.4241 - acc: 0.6393 - LRFinder: val_loss: 4.1407 - lr = 0.00226256 \n",
            " 24/390 [>.............................] - ETA: 1:28 - loss: 1.4192 - acc: 0.6419 - LRFinder: val_loss: 4.0932 - lr = 0.00228359 \n",
            " 25/390 [>.............................] - ETA: 1:27 - loss: 1.4229 - acc: 0.6406 - LRFinder: val_loss: 3.9827 - lr = 0.00230462 \n",
            " 26/390 [=>............................] - ETA: 1:25 - loss: 1.4190 - acc: 0.6433 - LRFinder: val_loss: 4.1613 - lr = 0.00232564 \n",
            " 27/390 [=>............................] - ETA: 1:24 - loss: 1.4179 - acc: 0.6453 - LRFinder: val_loss: 3.9977 - lr = 0.00234667 \n",
            " 28/390 [=>............................] - ETA: 1:22 - loss: 1.4185 - acc: 0.6445 - LRFinder: val_loss: 3.9701 - lr = 0.00236769 \n",
            " 29/390 [=>............................] - ETA: 1:21 - loss: 1.4166 - acc: 0.6452 - LRFinder: val_loss: 4.1176 - lr = 0.00238872 \n",
            " 30/390 [=>............................] - ETA: 1:20 - loss: 1.4203 - acc: 0.6451 - LRFinder: val_loss: 3.9028 - lr = 0.00240974 \n",
            " 31/390 [=>............................] - ETA: 1:19 - loss: 1.4219 - acc: 0.6442 - LRFinder: val_loss: 3.9434 - lr = 0.00243077 \n",
            " 32/390 [=>............................] - ETA: 1:18 - loss: 1.4291 - acc: 0.6414 - LRFinder: val_loss: 3.9259 - lr = 0.00245179 \n",
            " 33/390 [=>............................] - ETA: 1:17 - loss: 1.4277 - acc: 0.6411 - LRFinder: val_loss: 3.9499 - lr = 0.00247282 \n",
            " 34/390 [=>............................] - ETA: 1:16 - loss: 1.4273 - acc: 0.6420 - LRFinder: val_loss: 4.0558 - lr = 0.00249385 \n",
            " 35/390 [=>............................] - ETA: 1:15 - loss: 1.4244 - acc: 0.6438 - LRFinder: val_loss: 4.0577 - lr = 0.00251487 \n",
            " 36/390 [=>............................] - ETA: 1:14 - loss: 1.4235 - acc: 0.6445 - LRFinder: val_loss: 3.9780 - lr = 0.00253590 \n",
            " 37/390 [=>............................] - ETA: 1:13 - loss: 1.4255 - acc: 0.6436 - LRFinder: val_loss: 3.9680 - lr = 0.00255692 \n",
            " 38/390 [=>............................] - ETA: 1:12 - loss: 1.4259 - acc: 0.6431 - LRFinder: val_loss: 3.9954 - lr = 0.00257795 \n",
            " 39/390 [==>...........................] - ETA: 1:11 - loss: 1.4266 - acc: 0.6430 - LRFinder: val_loss: 3.7674 - lr = 0.00259897 \n",
            " 40/390 [==>...........................] - ETA: 1:11 - loss: 1.4259 - acc: 0.6438 - LRFinder: val_loss: 3.7159 - lr = 0.00262000 \n",
            " 41/390 [==>...........................] - ETA: 1:10 - loss: 1.4266 - acc: 0.6437 - LRFinder: val_loss: 3.9303 - lr = 0.00264103 \n",
            " 42/390 [==>...........................] - ETA: 1:09 - loss: 1.4286 - acc: 0.6432 - LRFinder: val_loss: 3.8465 - lr = 0.00266205 \n",
            " 43/390 [==>...........................] - ETA: 1:09 - loss: 1.4264 - acc: 0.6443 - LRFinder: val_loss: 3.9645 - lr = 0.00268308 \n",
            " 44/390 [==>...........................] - ETA: 1:08 - loss: 1.4247 - acc: 0.6449 - LRFinder: val_loss: 3.9245 - lr = 0.00270410 \n",
            " 45/390 [==>...........................] - ETA: 1:07 - loss: 1.4246 - acc: 0.6443 - LRFinder: val_loss: 3.7197 - lr = 0.00272513 \n",
            " 46/390 [==>...........................] - ETA: 1:07 - loss: 1.4231 - acc: 0.6450 - LRFinder: val_loss: 3.7816 - lr = 0.00274615 \n",
            " 47/390 [==>...........................] - ETA: 1:06 - loss: 1.4221 - acc: 0.6446 - LRFinder: val_loss: 3.8818 - lr = 0.00276718 \n",
            " 48/390 [==>...........................] - ETA: 1:06 - loss: 1.4253 - acc: 0.6444 - LRFinder: val_loss: 3.8796 - lr = 0.00278821 \n",
            " 49/390 [==>...........................] - ETA: 1:05 - loss: 1.4281 - acc: 0.6443 - LRFinder: val_loss: 4.0093 - lr = 0.00280923 \n",
            " 50/390 [==>...........................] - ETA: 1:05 - loss: 1.4301 - acc: 0.6431 - LRFinder: val_loss: 3.8265 - lr = 0.00283026 \n",
            " 51/390 [==>...........................] - ETA: 1:04 - loss: 1.4335 - acc: 0.6428 - LRFinder: val_loss: 4.0986 - lr = 0.00285128 \n",
            " 52/390 [===>..........................] - ETA: 1:04 - loss: 1.4346 - acc: 0.6426 - LRFinder: val_loss: 3.8802 - lr = 0.00287231 \n",
            " 53/390 [===>..........................] - ETA: 1:03 - loss: 1.4350 - acc: 0.6417 - LRFinder: val_loss: 3.9371 - lr = 0.00289333 \n",
            " 54/390 [===>..........................] - ETA: 1:03 - loss: 1.4373 - acc: 0.6408 - LRFinder: val_loss: 4.0790 - lr = 0.00291436 \n",
            " 55/390 [===>..........................] - ETA: 1:02 - loss: 1.4368 - acc: 0.6412 - LRFinder: val_loss: 3.9740 - lr = 0.00293538 \n",
            " 56/390 [===>..........................] - ETA: 1:02 - loss: 1.4364 - acc: 0.6426 - LRFinder: val_loss: 4.0318 - lr = 0.00295641 \n",
            " 57/390 [===>..........................] - ETA: 1:01 - loss: 1.4368 - acc: 0.6423 - LRFinder: val_loss: 4.0699 - lr = 0.00297744 \n",
            " 58/390 [===>..........................] - ETA: 1:01 - loss: 1.4389 - acc: 0.6421 - LRFinder: val_loss: 4.1979 - lr = 0.00299846 \n",
            " 59/390 [===>..........................] - ETA: 1:01 - loss: 1.4369 - acc: 0.6429 - LRFinder: val_loss: 4.0231 - lr = 0.00301949 \n",
            " 60/390 [===>..........................] - ETA: 1:00 - loss: 1.4356 - acc: 0.6432 - LRFinder: val_loss: 4.1508 - lr = 0.00304051 \n",
            " 61/390 [===>..........................] - ETA: 1:00 - loss: 1.4346 - acc: 0.6438 - LRFinder: val_loss: 4.0450 - lr = 0.00306154 \n",
            " 62/390 [===>..........................] - ETA: 59s - loss: 1.4361 - acc: 0.6438  - LRFinder: val_loss: 3.9927 - lr = 0.00308256 \n",
            " 63/390 [===>..........................] - ETA: 59s - loss: 1.4351 - acc: 0.6445 - LRFinder: val_loss: 4.0371 - lr = 0.00310359 \n",
            " 64/390 [===>..........................] - ETA: 59s - loss: 1.4357 - acc: 0.6442 - LRFinder: val_loss: 4.1039 - lr = 0.00312462 \n",
            " 65/390 [====>.........................] - ETA: 58s - loss: 1.4356 - acc: 0.6439 - LRFinder: val_loss: 4.0170 - lr = 0.00314564 \n",
            " 66/390 [====>.........................] - ETA: 58s - loss: 1.4359 - acc: 0.6444 - LRFinder: val_loss: 4.1866 - lr = 0.00316667 \n",
            " 67/390 [====>.........................] - ETA: 57s - loss: 1.4361 - acc: 0.6441 - LRFinder: val_loss: 4.0523 - lr = 0.00318769 \n",
            " 68/390 [====>.........................] - ETA: 57s - loss: 1.4350 - acc: 0.6443 - LRFinder: val_loss: 4.1389 - lr = 0.00320872 \n",
            " 69/390 [====>.........................] - ETA: 57s - loss: 1.4350 - acc: 0.6445 - LRFinder: val_loss: 3.9822 - lr = 0.00322974 \n",
            " 70/390 [====>.........................] - ETA: 56s - loss: 1.4337 - acc: 0.6451 - LRFinder: val_loss: 4.0820 - lr = 0.00325077 \n",
            " 71/390 [====>.........................] - ETA: 56s - loss: 1.4324 - acc: 0.6458 - LRFinder: val_loss: 3.9391 - lr = 0.00327179 \n",
            " 72/390 [====>.........................] - ETA: 56s - loss: 1.4336 - acc: 0.6457 - LRFinder: val_loss: 4.1109 - lr = 0.00329282 \n",
            " 73/390 [====>.........................] - ETA: 55s - loss: 1.4325 - acc: 0.6462 - LRFinder: val_loss: 4.0058 - lr = 0.00331385 \n",
            " 74/390 [====>.........................] - ETA: 55s - loss: 1.4307 - acc: 0.6467 - LRFinder: val_loss: 3.9641 - lr = 0.00333487 \n",
            " 75/390 [====>.........................] - ETA: 55s - loss: 1.4318 - acc: 0.6462 - LRFinder: val_loss: 4.1027 - lr = 0.00335590 \n",
            " 76/390 [====>.........................] - ETA: 55s - loss: 1.4318 - acc: 0.6471 - LRFinder: val_loss: 3.7923 - lr = 0.00337692 \n",
            " 77/390 [====>.........................] - ETA: 54s - loss: 1.4323 - acc: 0.6466 - LRFinder: val_loss: 3.7338 - lr = 0.00339795 \n",
            " 78/390 [=====>........................] - ETA: 54s - loss: 1.4315 - acc: 0.6473 - LRFinder: val_loss: 3.8947 - lr = 0.00341897 \n",
            " 79/390 [=====>........................] - ETA: 54s - loss: 1.4297 - acc: 0.6478 - LRFinder: val_loss: 3.8455 - lr = 0.00344000 \n",
            " 80/390 [=====>........................] - ETA: 53s - loss: 1.4288 - acc: 0.6478 - LRFinder: val_loss: 3.9012 - lr = 0.00346103 \n",
            " 81/390 [=====>........................] - ETA: 53s - loss: 1.4302 - acc: 0.6470 - LRFinder: val_loss: 3.9214 - lr = 0.00348205 \n",
            " 82/390 [=====>........................] - ETA: 53s - loss: 1.4297 - acc: 0.6476 - LRFinder: val_loss: 3.8581 - lr = 0.00350308 \n",
            " 83/390 [=====>........................] - ETA: 53s - loss: 1.4283 - acc: 0.6480 - LRFinder: val_loss: 4.1238 - lr = 0.00352410 \n",
            " 84/390 [=====>........................] - ETA: 52s - loss: 1.4296 - acc: 0.6477 - LRFinder: val_loss: 4.0694 - lr = 0.00354513 \n",
            " 85/390 [=====>........................] - ETA: 52s - loss: 1.4280 - acc: 0.6482 - LRFinder: val_loss: 3.9546 - lr = 0.00356615 \n",
            " 86/390 [=====>........................] - ETA: 52s - loss: 1.4274 - acc: 0.6483 - LRFinder: val_loss: 3.8828 - lr = 0.00358718 \n",
            " 87/390 [=====>........................] - ETA: 51s - loss: 1.4271 - acc: 0.6477 - LRFinder: val_loss: 3.9397 - lr = 0.00360821 \n",
            " 88/390 [=====>........................] - ETA: 51s - loss: 1.4272 - acc: 0.6483 - LRFinder: val_loss: 4.0341 - lr = 0.00362923 \n",
            " 89/390 [=====>........................] - ETA: 51s - loss: 1.4275 - acc: 0.6483 - LRFinder: val_loss: 4.0592 - lr = 0.00365026 \n",
            " 90/390 [=====>........................] - ETA: 51s - loss: 1.4255 - acc: 0.6490 - LRFinder: val_loss: 3.9974 - lr = 0.00367128 \n",
            " 91/390 [======>.......................] - ETA: 50s - loss: 1.4262 - acc: 0.6490 - LRFinder: val_loss: 4.1297 - lr = 0.00369231 \n",
            " 92/390 [======>.......................] - ETA: 50s - loss: 1.4259 - acc: 0.6494 - LRFinder: val_loss: 3.9425 - lr = 0.00371333 \n",
            " 93/390 [======>.......................] - ETA: 50s - loss: 1.4261 - acc: 0.6494 - LRFinder: val_loss: 3.8631 - lr = 0.00373436 \n",
            " 94/390 [======>.......................] - ETA: 50s - loss: 1.4247 - acc: 0.6497 - LRFinder: val_loss: 3.8535 - lr = 0.00375538 \n",
            " 95/390 [======>.......................] - ETA: 49s - loss: 1.4251 - acc: 0.6493 - LRFinder: val_loss: 4.0128 - lr = 0.00377641 \n",
            " 96/390 [======>.......................] - ETA: 49s - loss: 1.4257 - acc: 0.6488 - LRFinder: val_loss: 3.9177 - lr = 0.00379744 \n",
            " 97/390 [======>.......................] - ETA: 49s - loss: 1.4257 - acc: 0.6489 - LRFinder: val_loss: 3.8822 - lr = 0.00381846 \n",
            " 98/390 [======>.......................] - ETA: 49s - loss: 1.4272 - acc: 0.6486 - LRFinder: val_loss: 4.1265 - lr = 0.00383949 \n",
            " 99/390 [======>.......................] - ETA: 48s - loss: 1.4274 - acc: 0.6484 - LRFinder: val_loss: 3.7486 - lr = 0.00386051 \n",
            "100/390 [======>.......................] - ETA: 48s - loss: 1.4277 - acc: 0.6483 - LRFinder: val_loss: 3.9907 - lr = 0.00388154 \n",
            "101/390 [======>.......................] - ETA: 48s - loss: 1.4270 - acc: 0.6489 - LRFinder: val_loss: 4.0826 - lr = 0.00390256 \n",
            "102/390 [======>.......................] - ETA: 48s - loss: 1.4260 - acc: 0.6492 - LRFinder: val_loss: 4.0188 - lr = 0.00392359 \n",
            "103/390 [======>.......................] - ETA: 48s - loss: 1.4258 - acc: 0.6490 - LRFinder: val_loss: 3.8113 - lr = 0.00394462 \n",
            "104/390 [=======>......................] - ETA: 47s - loss: 1.4257 - acc: 0.6494 - LRFinder: val_loss: 4.1106 - lr = 0.00396564 \n",
            "105/390 [=======>......................] - ETA: 47s - loss: 1.4259 - acc: 0.6495 - LRFinder: val_loss: 3.8734 - lr = 0.00398667 \n",
            "106/390 [=======>......................] - ETA: 47s - loss: 1.4242 - acc: 0.6500 - LRFinder: val_loss: 4.0283 - lr = 0.00400769 \n",
            "107/390 [=======>......................] - ETA: 47s - loss: 1.4252 - acc: 0.6496 - LRFinder: val_loss: 4.1173 - lr = 0.00402872 \n",
            "108/390 [=======>......................] - ETA: 46s - loss: 1.4246 - acc: 0.6500 - LRFinder: val_loss: 3.9820 - lr = 0.00404974 \n",
            "109/390 [=======>......................] - ETA: 46s - loss: 1.4242 - acc: 0.6504 - LRFinder: val_loss: 4.1127 - lr = 0.00407077 \n",
            "110/390 [=======>......................] - ETA: 46s - loss: 1.4244 - acc: 0.6506 - LRFinder: val_loss: 4.1882 - lr = 0.00409179 \n",
            "111/390 [=======>......................] - ETA: 46s - loss: 1.4248 - acc: 0.6506 - LRFinder: val_loss: 4.1244 - lr = 0.00411282 \n",
            "112/390 [=======>......................] - ETA: 46s - loss: 1.4246 - acc: 0.6505 - LRFinder: val_loss: 4.0297 - lr = 0.00413385 \n",
            "113/390 [=======>......................] - ETA: 45s - loss: 1.4236 - acc: 0.6511 - LRFinder: val_loss: 4.0452 - lr = 0.00415487 \n",
            "114/390 [=======>......................] - ETA: 45s - loss: 1.4239 - acc: 0.6510 - LRFinder: val_loss: 4.0352 - lr = 0.00417590 \n",
            "115/390 [=======>......................] - ETA: 45s - loss: 1.4238 - acc: 0.6510 - LRFinder: val_loss: 3.9460 - lr = 0.00419692 \n",
            "116/390 [=======>......................] - ETA: 45s - loss: 1.4233 - acc: 0.6509 - LRFinder: val_loss: 4.0762 - lr = 0.00421795 \n",
            "117/390 [========>.....................] - ETA: 44s - loss: 1.4227 - acc: 0.6510 - LRFinder: val_loss: 4.0376 - lr = 0.00423897 \n",
            "118/390 [========>.....................] - ETA: 44s - loss: 1.4225 - acc: 0.6512 - LRFinder: val_loss: 3.9745 - lr = 0.00426000 \n",
            "119/390 [========>.....................] - ETA: 44s - loss: 1.4225 - acc: 0.6509 - LRFinder: val_loss: 4.1741 - lr = 0.00428103 \n",
            "120/390 [========>.....................] - ETA: 44s - loss: 1.4213 - acc: 0.6513 - LRFinder: val_loss: 4.1396 - lr = 0.00430205 \n",
            "121/390 [========>.....................] - ETA: 44s - loss: 1.4222 - acc: 0.6511 - LRFinder: val_loss: 4.0935 - lr = 0.00432308 \n",
            "122/390 [========>.....................] - ETA: 43s - loss: 1.4230 - acc: 0.6509 - LRFinder: val_loss: 4.0419 - lr = 0.00434410 \n",
            "123/390 [========>.....................] - ETA: 43s - loss: 1.4237 - acc: 0.6508 - LRFinder: val_loss: 4.0968 - lr = 0.00436513 \n",
            "124/390 [========>.....................] - ETA: 43s - loss: 1.4246 - acc: 0.6505 - LRFinder: val_loss: 4.1405 - lr = 0.00438615 \n",
            "125/390 [========>.....................] - ETA: 43s - loss: 1.4244 - acc: 0.6504 - LRFinder: val_loss: 4.2994 - lr = 0.00440718 \n",
            "126/390 [========>.....................] - ETA: 43s - loss: 1.4240 - acc: 0.6506 - LRFinder: val_loss: 4.1172 - lr = 0.00442821 \n",
            "127/390 [========>.....................] - ETA: 42s - loss: 1.4232 - acc: 0.6508 - LRFinder: val_loss: 4.2967 - lr = 0.00444923 \n",
            "128/390 [========>.....................] - ETA: 42s - loss: 1.4230 - acc: 0.6507 - LRFinder: val_loss: 4.1704 - lr = 0.00447026 \n",
            "129/390 [========>.....................] - ETA: 42s - loss: 1.4254 - acc: 0.6496 - LRFinder: val_loss: 4.2251 - lr = 0.00449128 \n",
            "130/390 [=========>....................] - ETA: 42s - loss: 1.4257 - acc: 0.6492 - LRFinder: val_loss: 4.1590 - lr = 0.00451231 \n",
            "131/390 [=========>....................] - ETA: 42s - loss: 1.4251 - acc: 0.6495 - LRFinder: val_loss: 4.2953 - lr = 0.00453333 \n",
            "132/390 [=========>....................] - ETA: 41s - loss: 1.4244 - acc: 0.6497 - LRFinder: val_loss: 4.2060 - lr = 0.00455436 \n",
            "133/390 [=========>....................] - ETA: 41s - loss: 1.4240 - acc: 0.6499 - LRFinder: val_loss: 4.0068 - lr = 0.00457538 \n",
            "134/390 [=========>....................] - ETA: 41s - loss: 1.4241 - acc: 0.6500 - LRFinder: val_loss: 4.0754 - lr = 0.00459641 \n",
            "135/390 [=========>....................] - ETA: 41s - loss: 1.4232 - acc: 0.6503 - LRFinder: val_loss: 4.0634 - lr = 0.00461744 \n",
            "136/390 [=========>....................] - ETA: 41s - loss: 1.4234 - acc: 0.6502 - LRFinder: val_loss: 4.0526 - lr = 0.00463846 \n",
            "137/390 [=========>....................] - ETA: 40s - loss: 1.4236 - acc: 0.6502 - LRFinder: val_loss: 3.9325 - lr = 0.00465949 \n",
            "138/390 [=========>....................] - ETA: 40s - loss: 1.4231 - acc: 0.6507 - LRFinder: val_loss: 3.8995 - lr = 0.00468051 \n",
            "139/390 [=========>....................] - ETA: 40s - loss: 1.4234 - acc: 0.6506 - LRFinder: val_loss: 4.0486 - lr = 0.00470154 \n",
            "140/390 [=========>....................] - ETA: 40s - loss: 1.4237 - acc: 0.6504 - LRFinder: val_loss: 3.9834 - lr = 0.00472256 \n",
            "141/390 [=========>....................] - ETA: 40s - loss: 1.4240 - acc: 0.6505 - LRFinder: val_loss: 3.7609 - lr = 0.00474359 \n",
            "142/390 [=========>....................] - ETA: 39s - loss: 1.4251 - acc: 0.6499 - LRFinder: val_loss: 4.1035 - lr = 0.00476462 \n",
            "143/390 [==========>...................] - ETA: 39s - loss: 1.4241 - acc: 0.6505 - LRFinder: val_loss: 3.8285 - lr = 0.00478564 \n",
            "144/390 [==========>...................] - ETA: 39s - loss: 1.4244 - acc: 0.6506 - LRFinder: val_loss: 3.9509 - lr = 0.00480667 \n",
            "145/390 [==========>...................] - ETA: 39s - loss: 1.4247 - acc: 0.6505 - LRFinder: val_loss: 4.0157 - lr = 0.00482769 \n",
            "146/390 [==========>...................] - ETA: 39s - loss: 1.4244 - acc: 0.6505 - LRFinder: val_loss: 3.7680 - lr = 0.00484872 \n",
            "147/390 [==========>...................] - ETA: 38s - loss: 1.4239 - acc: 0.6507 - LRFinder: val_loss: 3.9241 - lr = 0.00486974 \n",
            "148/390 [==========>...................] - ETA: 38s - loss: 1.4242 - acc: 0.6510 - LRFinder: val_loss: 4.0085 - lr = 0.00489077 \n",
            "149/390 [==========>...................] - ETA: 38s - loss: 1.4244 - acc: 0.6507 - LRFinder: val_loss: 4.0311 - lr = 0.00491179 \n",
            "150/390 [==========>...................] - ETA: 38s - loss: 1.4245 - acc: 0.6506 - LRFinder: val_loss: 3.9137 - lr = 0.00493282 \n",
            "151/390 [==========>...................] - ETA: 38s - loss: 1.4245 - acc: 0.6506 - LRFinder: val_loss: 3.7532 - lr = 0.00495385 \n",
            "152/390 [==========>...................] - ETA: 38s - loss: 1.4251 - acc: 0.6501 - LRFinder: val_loss: 3.9199 - lr = 0.00497487 \n",
            "153/390 [==========>...................] - ETA: 37s - loss: 1.4252 - acc: 0.6502 - LRFinder: val_loss: 3.9138 - lr = 0.00499590 \n",
            "154/390 [==========>...................] - ETA: 37s - loss: 1.4247 - acc: 0.6507 - LRFinder: val_loss: 3.8699 - lr = 0.00501692 \n",
            "155/390 [==========>...................] - ETA: 37s - loss: 1.4247 - acc: 0.6507 - LRFinder: val_loss: 3.9755 - lr = 0.00503795 \n",
            "156/390 [===========>..................] - ETA: 37s - loss: 1.4242 - acc: 0.6508 - LRFinder: val_loss: 3.9710 - lr = 0.00505897 \n",
            "157/390 [===========>..................] - ETA: 37s - loss: 1.4252 - acc: 0.6504 - LRFinder: val_loss: 3.9928 - lr = 0.00508000 \n",
            "158/390 [===========>..................] - ETA: 37s - loss: 1.4267 - acc: 0.6498 - LRFinder: val_loss: 4.0809 - lr = 0.00510103 \n",
            "159/390 [===========>..................] - ETA: 36s - loss: 1.4267 - acc: 0.6496 - LRFinder: val_loss: 4.0227 - lr = 0.00512205 \n",
            "160/390 [===========>..................] - ETA: 36s - loss: 1.4265 - acc: 0.6498 - LRFinder: val_loss: 4.0823 - lr = 0.00514308 \n",
            "161/390 [===========>..................] - ETA: 36s - loss: 1.4264 - acc: 0.6500 - LRFinder: val_loss: 4.0483 - lr = 0.00516410 \n",
            "162/390 [===========>..................] - ETA: 36s - loss: 1.4263 - acc: 0.6499 - LRFinder: val_loss: 3.9337 - lr = 0.00518513 \n",
            "163/390 [===========>..................] - ETA: 36s - loss: 1.4262 - acc: 0.6500 - LRFinder: val_loss: 3.8610 - lr = 0.00520615 \n",
            "164/390 [===========>..................] - ETA: 35s - loss: 1.4255 - acc: 0.6505 - LRFinder: val_loss: 3.8747 - lr = 0.00522718 \n",
            "165/390 [===========>..................] - ETA: 35s - loss: 1.4262 - acc: 0.6500 - LRFinder: val_loss: 3.8632 - lr = 0.00524821 \n",
            "166/390 [===========>..................] - ETA: 35s - loss: 1.4266 - acc: 0.6498 - LRFinder: val_loss: 3.8025 - lr = 0.00526923 \n",
            "167/390 [===========>..................] - ETA: 35s - loss: 1.4260 - acc: 0.6499 - LRFinder: val_loss: 3.8133 - lr = 0.00529026 \n",
            "168/390 [===========>..................] - ETA: 35s - loss: 1.4258 - acc: 0.6499 - LRFinder: val_loss: 3.9935 - lr = 0.00531128 \n",
            "169/390 [============>.................] - ETA: 35s - loss: 1.4255 - acc: 0.6500 - LRFinder: val_loss: 4.0181 - lr = 0.00533231 \n",
            "170/390 [============>.................] - ETA: 34s - loss: 1.4257 - acc: 0.6496 - LRFinder: val_loss: 3.9432 - lr = 0.00535333 \n",
            "171/390 [============>.................] - ETA: 34s - loss: 1.4263 - acc: 0.6498 - LRFinder: val_loss: 3.8634 - lr = 0.00537436 \n",
            "172/390 [============>.................] - ETA: 34s - loss: 1.4262 - acc: 0.6497 - LRFinder: val_loss: 3.8722 - lr = 0.00539538 \n",
            "173/390 [============>.................] - ETA: 34s - loss: 1.4253 - acc: 0.6501 - LRFinder: val_loss: 3.6494 - lr = 0.00541641 \n",
            "174/390 [============>.................] - ETA: 34s - loss: 1.4253 - acc: 0.6504 - LRFinder: val_loss: 3.7937 - lr = 0.00543744 \n",
            "175/390 [============>.................] - ETA: 34s - loss: 1.4244 - acc: 0.6510 - LRFinder: val_loss: 3.7875 - lr = 0.00545846 \n",
            "176/390 [============>.................] - ETA: 33s - loss: 1.4237 - acc: 0.6514 - LRFinder: val_loss: 3.5207 - lr = 0.00547949 \n",
            "177/390 [============>.................] - ETA: 33s - loss: 1.4238 - acc: 0.6513 - LRFinder: val_loss: 3.8119 - lr = 0.00550051 \n",
            "178/390 [============>.................] - ETA: 33s - loss: 1.4230 - acc: 0.6516 - LRFinder: val_loss: 3.8512 - lr = 0.00552154 \n",
            "179/390 [============>.................] - ETA: 33s - loss: 1.4221 - acc: 0.6519 - LRFinder: val_loss: 3.6963 - lr = 0.00554256 \n",
            "180/390 [============>.................] - ETA: 33s - loss: 1.4223 - acc: 0.6518 - LRFinder: val_loss: 3.7446 - lr = 0.00556359 \n",
            "181/390 [============>.................] - ETA: 32s - loss: 1.4220 - acc: 0.6517 - LRFinder: val_loss: 3.7493 - lr = 0.00558462 \n",
            "182/390 [=============>................] - ETA: 32s - loss: 1.4217 - acc: 0.6519 - LRFinder: val_loss: 3.7441 - lr = 0.00560564 \n",
            "183/390 [=============>................] - ETA: 32s - loss: 1.4228 - acc: 0.6514 - LRFinder: val_loss: 3.6221 - lr = 0.00562667 \n",
            "184/390 [=============>................] - ETA: 32s - loss: 1.4222 - acc: 0.6518 - LRFinder: val_loss: 3.6175 - lr = 0.00564769 \n",
            "185/390 [=============>................] - ETA: 32s - loss: 1.4225 - acc: 0.6517 - LRFinder: val_loss: 3.8593 - lr = 0.00566872 \n",
            "186/390 [=============>................] - ETA: 32s - loss: 1.4218 - acc: 0.6521 - LRFinder: val_loss: 3.8575 - lr = 0.00568974 \n",
            "187/390 [=============>................] - ETA: 31s - loss: 1.4207 - acc: 0.6523 - LRFinder: val_loss: 3.7137 - lr = 0.00571077 \n",
            "188/390 [=============>................] - ETA: 31s - loss: 1.4202 - acc: 0.6526 - LRFinder: val_loss: 3.9350 - lr = 0.00573179 \n",
            "189/390 [=============>................] - ETA: 31s - loss: 1.4197 - acc: 0.6527 - LRFinder: val_loss: 3.9161 - lr = 0.00575282 \n",
            "190/390 [=============>................] - ETA: 31s - loss: 1.4198 - acc: 0.6525 - LRFinder: val_loss: 3.8728 - lr = 0.00577385 \n",
            "191/390 [=============>................] - ETA: 31s - loss: 1.4198 - acc: 0.6523 - LRFinder: val_loss: 3.8772 - lr = 0.00579487 \n",
            "192/390 [=============>................] - ETA: 31s - loss: 1.4192 - acc: 0.6526 - LRFinder: val_loss: 3.7630 - lr = 0.00581590 \n",
            "193/390 [=============>................] - ETA: 30s - loss: 1.4197 - acc: 0.6525 - LRFinder: val_loss: 3.9291 - lr = 0.00583692 \n",
            "194/390 [=============>................] - ETA: 30s - loss: 1.4198 - acc: 0.6526 - LRFinder: val_loss: 3.8193 - lr = 0.00585795 \n",
            "195/390 [==============>...............] - ETA: 30s - loss: 1.4198 - acc: 0.6526 - LRFinder: val_loss: 3.9581 - lr = 0.00587897 \n",
            "196/390 [==============>...............] - ETA: 30s - loss: 1.4206 - acc: 0.6522 - LRFinder: val_loss: 3.9312 - lr = 0.00590000 \n",
            "197/390 [==============>...............] - ETA: 30s - loss: 1.4207 - acc: 0.6522 - LRFinder: val_loss: 3.9426 - lr = 0.00592103 \n",
            "198/390 [==============>...............] - ETA: 30s - loss: 1.4209 - acc: 0.6520 - LRFinder: val_loss: 3.9148 - lr = 0.00594205 \n",
            "199/390 [==============>...............] - ETA: 29s - loss: 1.4207 - acc: 0.6522 - LRFinder: val_loss: 3.7729 - lr = 0.00596308 \n",
            "200/390 [==============>...............] - ETA: 29s - loss: 1.4210 - acc: 0.6520 - LRFinder: val_loss: 3.7863 - lr = 0.00598410 \n",
            "201/390 [==============>...............] - ETA: 29s - loss: 1.4207 - acc: 0.6520 - LRFinder: val_loss: 3.8791 - lr = 0.00600513 \n",
            "202/390 [==============>...............] - ETA: 29s - loss: 1.4205 - acc: 0.6520 - LRFinder: val_loss: 3.9273 - lr = 0.00602615 \n",
            "203/390 [==============>...............] - ETA: 29s - loss: 1.4206 - acc: 0.6519 - LRFinder: val_loss: 3.8651 - lr = 0.00604718 \n",
            "204/390 [==============>...............] - ETA: 29s - loss: 1.4205 - acc: 0.6519 - LRFinder: val_loss: 3.9852 - lr = 0.00606821 \n",
            "205/390 [==============>...............] - ETA: 28s - loss: 1.4205 - acc: 0.6517 - LRFinder: val_loss: 3.8800 - lr = 0.00608923 \n",
            "206/390 [==============>...............] - ETA: 28s - loss: 1.4204 - acc: 0.6517 - LRFinder: val_loss: 4.1552 - lr = 0.00611026 \n",
            "207/390 [==============>...............] - ETA: 28s - loss: 1.4202 - acc: 0.6519 - LRFinder: val_loss: 3.8864 - lr = 0.00613128 \n",
            "208/390 [===============>..............] - ETA: 28s - loss: 1.4202 - acc: 0.6518 - LRFinder: val_loss: 3.7858 - lr = 0.00615231 \n",
            "209/390 [===============>..............] - ETA: 28s - loss: 1.4194 - acc: 0.6519 - LRFinder: val_loss: 4.0590 - lr = 0.00617333 \n",
            "210/390 [===============>..............] - ETA: 28s - loss: 1.4195 - acc: 0.6520 - LRFinder: val_loss: 3.8831 - lr = 0.00619436 \n",
            "211/390 [===============>..............] - ETA: 27s - loss: 1.4199 - acc: 0.6518 - LRFinder: val_loss: 3.9412 - lr = 0.00621538 \n",
            "212/390 [===============>..............] - ETA: 27s - loss: 1.4200 - acc: 0.6518 - LRFinder: val_loss: 3.9351 - lr = 0.00623641 \n",
            "213/390 [===============>..............] - ETA: 27s - loss: 1.4205 - acc: 0.6517 - LRFinder: val_loss: 4.0199 - lr = 0.00625744 \n",
            "214/390 [===============>..............] - ETA: 27s - loss: 1.4207 - acc: 0.6514 - LRFinder: val_loss: 3.8806 - lr = 0.00627846 \n",
            "215/390 [===============>..............] - ETA: 27s - loss: 1.4208 - acc: 0.6513 - LRFinder: val_loss: 3.9828 - lr = 0.00629949 \n",
            "216/390 [===============>..............] - ETA: 27s - loss: 1.4211 - acc: 0.6513 - LRFinder: val_loss: 4.0562 - lr = 0.00632051 \n",
            "217/390 [===============>..............] - ETA: 26s - loss: 1.4212 - acc: 0.6508 - LRFinder: val_loss: 3.9337 - lr = 0.00634154 \n",
            "218/390 [===============>..............] - ETA: 26s - loss: 1.4207 - acc: 0.6512 - LRFinder: val_loss: 3.8672 - lr = 0.00636256 \n",
            "219/390 [===============>..............] - ETA: 26s - loss: 1.4202 - acc: 0.6513 - LRFinder: val_loss: 3.9036 - lr = 0.00638359 \n",
            "220/390 [===============>..............] - ETA: 26s - loss: 1.4199 - acc: 0.6513 - LRFinder: val_loss: 3.9352 - lr = 0.00640462 \n",
            "221/390 [================>.............] - ETA: 26s - loss: 1.4201 - acc: 0.6512 - LRFinder: val_loss: 3.9067 - lr = 0.00642564 \n",
            "222/390 [================>.............] - ETA: 26s - loss: 1.4194 - acc: 0.6513 - LRFinder: val_loss: 3.8291 - lr = 0.00644667 \n",
            "223/390 [================>.............] - ETA: 25s - loss: 1.4193 - acc: 0.6515 - LRFinder: val_loss: 3.7801 - lr = 0.00646769 \n",
            "224/390 [================>.............] - ETA: 25s - loss: 1.4199 - acc: 0.6516 - LRFinder: val_loss: 3.8872 - lr = 0.00648872 \n",
            "225/390 [================>.............] - ETA: 25s - loss: 1.4202 - acc: 0.6514 - LRFinder: val_loss: 3.9629 - lr = 0.00650974 \n",
            "226/390 [================>.............] - ETA: 25s - loss: 1.4197 - acc: 0.6516 - LRFinder: val_loss: 3.9774 - lr = 0.00653077 \n",
            "227/390 [================>.............] - ETA: 25s - loss: 1.4194 - acc: 0.6517 - LRFinder: val_loss: 3.8412 - lr = 0.00655179 \n",
            "228/390 [================>.............] - ETA: 25s - loss: 1.4189 - acc: 0.6518 - LRFinder: val_loss: 3.9292 - lr = 0.00657282 \n",
            "229/390 [================>.............] - ETA: 24s - loss: 1.4184 - acc: 0.6521 - LRFinder: val_loss: 3.9196 - lr = 0.00659385 \n",
            "230/390 [================>.............] - ETA: 24s - loss: 1.4191 - acc: 0.6517 - LRFinder: val_loss: 3.7935 - lr = 0.00661487 \n",
            "231/390 [================>.............] - ETA: 24s - loss: 1.4187 - acc: 0.6519 - LRFinder: val_loss: 3.8452 - lr = 0.00663590 \n",
            "232/390 [================>.............] - ETA: 24s - loss: 1.4182 - acc: 0.6522 - LRFinder: val_loss: 3.8355 - lr = 0.00665692 \n",
            "233/390 [================>.............] - ETA: 24s - loss: 1.4188 - acc: 0.6520 - LRFinder: val_loss: 3.7040 - lr = 0.00667795 \n",
            "234/390 [=================>............] - ETA: 24s - loss: 1.4187 - acc: 0.6519 - LRFinder: val_loss: 3.6000 - lr = 0.00669897 \n",
            "235/390 [=================>............] - ETA: 23s - loss: 1.4189 - acc: 0.6517 - LRFinder: val_loss: 3.6452 - lr = 0.00672000 \n",
            "236/390 [=================>............] - ETA: 23s - loss: 1.4192 - acc: 0.6515 - LRFinder: val_loss: 3.9188 - lr = 0.00674103 \n",
            "237/390 [=================>............] - ETA: 23s - loss: 1.4186 - acc: 0.6519 - LRFinder: val_loss: 3.7845 - lr = 0.00676205 \n",
            "238/390 [=================>............] - ETA: 23s - loss: 1.4186 - acc: 0.6520 - LRFinder: val_loss: 3.7136 - lr = 0.00678308 \n",
            "239/390 [=================>............] - ETA: 23s - loss: 1.4187 - acc: 0.6519 - LRFinder: val_loss: 3.6955 - lr = 0.00680410 \n",
            "240/390 [=================>............] - ETA: 23s - loss: 1.4185 - acc: 0.6520 - LRFinder: val_loss: 3.7366 - lr = 0.00682513 \n",
            "241/390 [=================>............] - ETA: 23s - loss: 1.4180 - acc: 0.6522 - LRFinder: val_loss: 3.7573 - lr = 0.00684615 \n",
            "242/390 [=================>............] - ETA: 22s - loss: 1.4178 - acc: 0.6523 - LRFinder: val_loss: 3.7847 - lr = 0.00686718 \n",
            "243/390 [=================>............] - ETA: 22s - loss: 1.4172 - acc: 0.6526 - LRFinder: val_loss: 3.9595 - lr = 0.00688821 \n",
            "244/390 [=================>............] - ETA: 22s - loss: 1.4170 - acc: 0.6525 - LRFinder: val_loss: 3.7442 - lr = 0.00690923 \n",
            "245/390 [=================>............] - ETA: 22s - loss: 1.4176 - acc: 0.6522 - LRFinder: val_loss: 3.7145 - lr = 0.00693026 \n",
            "246/390 [=================>............] - ETA: 22s - loss: 1.4175 - acc: 0.6522 - LRFinder: val_loss: 3.8884 - lr = 0.00695128 \n",
            "247/390 [==================>...........] - ETA: 22s - loss: 1.4174 - acc: 0.6523 - LRFinder: val_loss: 3.8116 - lr = 0.00697231 \n",
            "248/390 [==================>...........] - ETA: 21s - loss: 1.4175 - acc: 0.6523 - LRFinder: val_loss: 4.0549 - lr = 0.00699333 \n",
            "249/390 [==================>...........] - ETA: 21s - loss: 1.4175 - acc: 0.6522 - LRFinder: val_loss: 3.9173 - lr = 0.00701436 \n",
            "250/390 [==================>...........] - ETA: 21s - loss: 1.4182 - acc: 0.6520 - LRFinder: val_loss: 4.0756 - lr = 0.00703538 \n",
            "251/390 [==================>...........] - ETA: 21s - loss: 1.4186 - acc: 0.6518 - LRFinder: val_loss: 4.0045 - lr = 0.00705641 \n",
            "252/390 [==================>...........] - ETA: 21s - loss: 1.4183 - acc: 0.6520 - LRFinder: val_loss: 4.0416 - lr = 0.00707744 \n",
            "253/390 [==================>...........] - ETA: 21s - loss: 1.4180 - acc: 0.6521 - LRFinder: val_loss: 3.8586 - lr = 0.00709846 \n",
            "254/390 [==================>...........] - ETA: 20s - loss: 1.4182 - acc: 0.6521 - LRFinder: val_loss: 3.9730 - lr = 0.00711949 \n",
            "255/390 [==================>...........] - ETA: 20s - loss: 1.4188 - acc: 0.6518 - LRFinder: val_loss: 3.7812 - lr = 0.00714051 \n",
            "256/390 [==================>...........] - ETA: 20s - loss: 1.4180 - acc: 0.6522 - LRFinder: val_loss: 3.8288 - lr = 0.00716154 \n",
            "257/390 [==================>...........] - ETA: 20s - loss: 1.4185 - acc: 0.6519 - LRFinder: val_loss: 3.5976 - lr = 0.00718256 \n",
            "258/390 [==================>...........] - ETA: 20s - loss: 1.4189 - acc: 0.6517 - LRFinder: val_loss: 3.8393 - lr = 0.00720359 \n",
            "259/390 [==================>...........] - ETA: 20s - loss: 1.4188 - acc: 0.6517 - LRFinder: val_loss: 3.6549 - lr = 0.00722462 \n",
            "260/390 [===================>..........] - ETA: 19s - loss: 1.4188 - acc: 0.6517 - LRFinder: val_loss: 3.8558 - lr = 0.00724564 \n",
            "261/390 [===================>..........] - ETA: 19s - loss: 1.4189 - acc: 0.6517 - LRFinder: val_loss: 3.8992 - lr = 0.00726667 \n",
            "262/390 [===================>..........] - ETA: 19s - loss: 1.4191 - acc: 0.6515 - LRFinder: val_loss: 3.8488 - lr = 0.00728769 \n",
            "263/390 [===================>..........] - ETA: 19s - loss: 1.4196 - acc: 0.6513 - LRFinder: val_loss: 3.8419 - lr = 0.00730872 \n",
            "264/390 [===================>..........] - ETA: 19s - loss: 1.4199 - acc: 0.6511 - LRFinder: val_loss: 3.9687 - lr = 0.00732974 \n",
            "265/390 [===================>..........] - ETA: 19s - loss: 1.4195 - acc: 0.6512 - LRFinder: val_loss: 4.0630 - lr = 0.00735077 \n",
            "266/390 [===================>..........] - ETA: 19s - loss: 1.4194 - acc: 0.6512 - LRFinder: val_loss: 4.0267 - lr = 0.00737179 \n",
            "267/390 [===================>..........] - ETA: 18s - loss: 1.4194 - acc: 0.6511 - LRFinder: val_loss: 4.0762 - lr = 0.00739282 \n",
            "268/390 [===================>..........] - ETA: 18s - loss: 1.4189 - acc: 0.6511 - LRFinder: val_loss: 4.0870 - lr = 0.00741385 \n",
            "269/390 [===================>..........] - ETA: 18s - loss: 1.4187 - acc: 0.6512 - LRFinder: val_loss: 4.0139 - lr = 0.00743487 \n",
            "270/390 [===================>..........] - ETA: 18s - loss: 1.4184 - acc: 0.6513 - LRFinder: val_loss: 4.4003 - lr = 0.00745590 \n",
            "271/390 [===================>..........] - ETA: 18s - loss: 1.4182 - acc: 0.6514 - LRFinder: val_loss: 4.1509 - lr = 0.00747692 \n",
            "272/390 [===================>..........] - ETA: 18s - loss: 1.4184 - acc: 0.6511 - LRFinder: val_loss: 4.1739 - lr = 0.00749795 \n",
            "273/390 [====================>.........] - ETA: 17s - loss: 1.4183 - acc: 0.6512 - LRFinder: val_loss: 4.0336 - lr = 0.00751897 \n",
            "274/390 [====================>.........] - ETA: 17s - loss: 1.4183 - acc: 0.6511 - LRFinder: val_loss: 4.0782 - lr = 0.00754000 \n",
            "275/390 [====================>.........] - ETA: 17s - loss: 1.4182 - acc: 0.6510 - LRFinder: val_loss: 4.1327 - lr = 0.00756103 \n",
            "276/390 [====================>.........] - ETA: 17s - loss: 1.4182 - acc: 0.6511 - LRFinder: val_loss: 4.0242 - lr = 0.00758205 \n",
            "277/390 [====================>.........] - ETA: 17s - loss: 1.4182 - acc: 0.6511 - LRFinder: val_loss: 3.9533 - lr = 0.00760308 \n",
            "278/390 [====================>.........] - ETA: 17s - loss: 1.4183 - acc: 0.6511 - LRFinder: val_loss: 3.8742 - lr = 0.00762410 \n",
            "279/390 [====================>.........] - ETA: 17s - loss: 1.4187 - acc: 0.6513 - LRFinder: val_loss: 3.7418 - lr = 0.00764513 \n",
            "280/390 [====================>.........] - ETA: 16s - loss: 1.4191 - acc: 0.6511 - LRFinder: val_loss: 3.7763 - lr = 0.00766615 \n",
            "281/390 [====================>.........] - ETA: 16s - loss: 1.4186 - acc: 0.6512 - LRFinder: val_loss: 3.6773 - lr = 0.00768718 \n",
            "282/390 [====================>.........] - ETA: 16s - loss: 1.4183 - acc: 0.6513 - LRFinder: val_loss: 3.7783 - lr = 0.00770821 \n",
            "283/390 [====================>.........] - ETA: 16s - loss: 1.4186 - acc: 0.6511 - LRFinder: val_loss: 3.7339 - lr = 0.00772923 \n",
            "284/390 [====================>.........] - ETA: 16s - loss: 1.4180 - acc: 0.6513 - LRFinder: val_loss: 3.9339 - lr = 0.00775026 \n",
            "285/390 [====================>.........] - ETA: 16s - loss: 1.4181 - acc: 0.6513 - LRFinder: val_loss: 3.8334 - lr = 0.00777128 \n",
            "286/390 [=====================>........] - ETA: 15s - loss: 1.4175 - acc: 0.6515 - LRFinder: val_loss: 3.8741 - lr = 0.00779231 \n",
            "287/390 [=====================>........] - ETA: 15s - loss: 1.4179 - acc: 0.6515 - LRFinder: val_loss: 3.7165 - lr = 0.00781333 \n",
            "288/390 [=====================>........] - ETA: 15s - loss: 1.4180 - acc: 0.6514 - LRFinder: val_loss: 3.7683 - lr = 0.00783436 \n",
            "289/390 [=====================>........] - ETA: 15s - loss: 1.4184 - acc: 0.6515 - LRFinder: val_loss: 3.9192 - lr = 0.00785538 \n",
            "290/390 [=====================>........] - ETA: 15s - loss: 1.4182 - acc: 0.6516 - LRFinder: val_loss: 3.7642 - lr = 0.00787641 \n",
            "291/390 [=====================>........] - ETA: 15s - loss: 1.4177 - acc: 0.6518 - LRFinder: val_loss: 3.9673 - lr = 0.00789744 \n",
            "292/390 [=====================>........] - ETA: 14s - loss: 1.4181 - acc: 0.6517 - LRFinder: val_loss: 3.8253 - lr = 0.00791846 \n",
            "293/390 [=====================>........] - ETA: 14s - loss: 1.4186 - acc: 0.6514 - LRFinder: val_loss: 3.8706 - lr = 0.00793949 \n",
            "294/390 [=====================>........] - ETA: 14s - loss: 1.4185 - acc: 0.6514 - LRFinder: val_loss: 3.9083 - lr = 0.00796051 \n",
            "295/390 [=====================>........] - ETA: 14s - loss: 1.4180 - acc: 0.6518 - LRFinder: val_loss: 3.9022 - lr = 0.00798154 \n",
            "296/390 [=====================>........] - ETA: 14s - loss: 1.4177 - acc: 0.6519 - LRFinder: val_loss: 3.9267 - lr = 0.00800256 \n",
            "297/390 [=====================>........] - ETA: 14s - loss: 1.4173 - acc: 0.6521 - LRFinder: val_loss: 3.8278 - lr = 0.00802359 \n",
            "298/390 [=====================>........] - ETA: 14s - loss: 1.4171 - acc: 0.6521 - LRFinder: val_loss: 3.9560 - lr = 0.00804462 \n",
            "299/390 [======================>.......] - ETA: 13s - loss: 1.4169 - acc: 0.6522 - LRFinder: val_loss: 3.7954 - lr = 0.00806564 \n",
            "300/390 [======================>.......] - ETA: 13s - loss: 1.4164 - acc: 0.6524 - LRFinder: val_loss: 3.8114 - lr = 0.00808667 \n",
            "301/390 [======================>.......] - ETA: 13s - loss: 1.4162 - acc: 0.6525 - LRFinder: val_loss: 3.9700 - lr = 0.00810769 \n",
            "302/390 [======================>.......] - ETA: 13s - loss: 1.4160 - acc: 0.6523 - LRFinder: val_loss: 3.9055 - lr = 0.00812872 \n",
            "303/390 [======================>.......] - ETA: 13s - loss: 1.4158 - acc: 0.6524 - LRFinder: val_loss: 3.8793 - lr = 0.00814974 \n",
            "304/390 [======================>.......] - ETA: 13s - loss: 1.4158 - acc: 0.6524 - LRFinder: val_loss: 4.0761 - lr = 0.00817077 \n",
            "305/390 [======================>.......] - ETA: 12s - loss: 1.4158 - acc: 0.6523 - LRFinder: val_loss: 3.7887 - lr = 0.00819179 \n",
            "306/390 [======================>.......] - ETA: 12s - loss: 1.4152 - acc: 0.6526 - LRFinder: val_loss: 3.7883 - lr = 0.00821282 \n",
            "307/390 [======================>.......] - ETA: 12s - loss: 1.4157 - acc: 0.6524 - LRFinder: val_loss: 3.7019 - lr = 0.00823385 \n",
            "308/390 [======================>.......] - ETA: 12s - loss: 1.4159 - acc: 0.6522 - LRFinder: val_loss: 3.7067 - lr = 0.00825487 \n",
            "309/390 [======================>.......] - ETA: 12s - loss: 1.4161 - acc: 0.6523 - LRFinder: val_loss: 3.7822 - lr = 0.00827590 \n",
            "310/390 [======================>.......] - ETA: 12s - loss: 1.4165 - acc: 0.6522 - LRFinder: val_loss: 3.6975 - lr = 0.00829692 \n",
            "311/390 [======================>.......] - ETA: 12s - loss: 1.4162 - acc: 0.6523 - LRFinder: val_loss: 3.7540 - lr = 0.00831795 \n",
            "312/390 [=======================>......] - ETA: 11s - loss: 1.4165 - acc: 0.6522 - LRFinder: val_loss: 3.5808 - lr = 0.00833897 \n",
            "313/390 [=======================>......] - ETA: 11s - loss: 1.4164 - acc: 0.6522 - LRFinder: val_loss: 3.7544 - lr = 0.00836000 \n",
            "314/390 [=======================>......] - ETA: 11s - loss: 1.4167 - acc: 0.6521 - LRFinder: val_loss: 3.9283 - lr = 0.00838103 \n",
            "315/390 [=======================>......] - ETA: 11s - loss: 1.4167 - acc: 0.6522 - LRFinder: val_loss: 3.7964 - lr = 0.00840205 \n",
            "316/390 [=======================>......] - ETA: 11s - loss: 1.4166 - acc: 0.6523 - LRFinder: val_loss: 3.5934 - lr = 0.00842308 \n",
            "317/390 [=======================>......] - ETA: 11s - loss: 1.4172 - acc: 0.6522 - LRFinder: val_loss: 3.7433 - lr = 0.00844410 \n",
            "318/390 [=======================>......] - ETA: 10s - loss: 1.4177 - acc: 0.6520 - LRFinder: val_loss: 3.7448 - lr = 0.00846513 \n",
            "319/390 [=======================>......] - ETA: 10s - loss: 1.4178 - acc: 0.6519 - LRFinder: val_loss: 3.5623 - lr = 0.00848615 \n",
            "320/390 [=======================>......] - ETA: 10s - loss: 1.4180 - acc: 0.6519 - LRFinder: val_loss: 3.7122 - lr = 0.00850718 \n",
            "321/390 [=======================>......] - ETA: 10s - loss: 1.4176 - acc: 0.6520 - LRFinder: val_loss: 3.6858 - lr = 0.00852821 \n",
            "322/390 [=======================>......] - ETA: 10s - loss: 1.4172 - acc: 0.6522 - LRFinder: val_loss: 3.7685 - lr = 0.00854923 \n",
            "323/390 [=======================>......] - ETA: 10s - loss: 1.4169 - acc: 0.6522 - LRFinder: val_loss: 3.7936 - lr = 0.00857026 \n",
            "324/390 [=======================>......] - ETA: 10s - loss: 1.4171 - acc: 0.6522 - LRFinder: val_loss: 3.7056 - lr = 0.00859128 \n",
            "325/390 [========================>.....] - ETA: 9s - loss: 1.4174 - acc: 0.6522  - LRFinder: val_loss: 3.5329 - lr = 0.00861231 \n",
            "326/390 [========================>.....] - ETA: 9s - loss: 1.4168 - acc: 0.6523 - LRFinder: val_loss: 3.6669 - lr = 0.00863333 \n",
            "327/390 [========================>.....] - ETA: 9s - loss: 1.4167 - acc: 0.6522 - LRFinder: val_loss: 3.6106 - lr = 0.00865436 \n",
            "328/390 [========================>.....] - ETA: 9s - loss: 1.4165 - acc: 0.6523 - LRFinder: val_loss: 3.6432 - lr = 0.00867538 \n",
            "329/390 [========================>.....] - ETA: 9s - loss: 1.4163 - acc: 0.6523 - LRFinder: val_loss: 3.5634 - lr = 0.00869641 \n",
            "330/390 [========================>.....] - ETA: 9s - loss: 1.4162 - acc: 0.6524 - LRFinder: val_loss: 3.8140 - lr = 0.00871744 \n",
            "331/390 [========================>.....] - ETA: 8s - loss: 1.4166 - acc: 0.6522 - LRFinder: val_loss: 3.7599 - lr = 0.00873846 \n",
            "332/390 [========================>.....] - ETA: 8s - loss: 1.4162 - acc: 0.6522 - LRFinder: val_loss: 3.5868 - lr = 0.00875949 \n",
            "333/390 [========================>.....] - ETA: 8s - loss: 1.4163 - acc: 0.6522 - LRFinder: val_loss: 3.6407 - lr = 0.00878051 \n",
            "334/390 [========================>.....] - ETA: 8s - loss: 1.4164 - acc: 0.6521 - LRFinder: val_loss: 3.6574 - lr = 0.00880154 \n",
            "335/390 [========================>.....] - ETA: 8s - loss: 1.4164 - acc: 0.6522 - LRFinder: val_loss: 3.8123 - lr = 0.00882256 \n",
            "336/390 [========================>.....] - ETA: 8s - loss: 1.4165 - acc: 0.6521 - LRFinder: val_loss: 3.7112 - lr = 0.00884359 \n",
            "337/390 [========================>.....] - ETA: 8s - loss: 1.4164 - acc: 0.6523 - LRFinder: val_loss: 3.7416 - lr = 0.00886462 \n",
            "338/390 [=========================>....] - ETA: 7s - loss: 1.4165 - acc: 0.6521 - LRFinder: val_loss: 3.9374 - lr = 0.00888564 \n",
            "339/390 [=========================>....] - ETA: 7s - loss: 1.4163 - acc: 0.6522 - LRFinder: val_loss: 3.8908 - lr = 0.00890667 \n",
            "340/390 [=========================>....] - ETA: 7s - loss: 1.4163 - acc: 0.6520 - LRFinder: val_loss: 3.8292 - lr = 0.00892769 \n",
            "341/390 [=========================>....] - ETA: 7s - loss: 1.4160 - acc: 0.6522 - LRFinder: val_loss: 3.8821 - lr = 0.00894872 \n",
            "342/390 [=========================>....] - ETA: 7s - loss: 1.4163 - acc: 0.6521 - LRFinder: val_loss: 3.8516 - lr = 0.00896974 \n",
            "343/390 [=========================>....] - ETA: 7s - loss: 1.4163 - acc: 0.6519 - LRFinder: val_loss: 3.7933 - lr = 0.00899077 \n",
            "344/390 [=========================>....] - ETA: 6s - loss: 1.4162 - acc: 0.6520 - LRFinder: val_loss: 3.8404 - lr = 0.00901179 \n",
            "345/390 [=========================>....] - ETA: 6s - loss: 1.4162 - acc: 0.6521 - LRFinder: val_loss: 3.7043 - lr = 0.00903282 \n",
            "346/390 [=========================>....] - ETA: 6s - loss: 1.4164 - acc: 0.6521 - LRFinder: val_loss: 3.5749 - lr = 0.00905385 \n",
            "347/390 [=========================>....] - ETA: 6s - loss: 1.4166 - acc: 0.6520 - LRFinder: val_loss: 3.8171 - lr = 0.00907487 \n",
            "348/390 [=========================>....] - ETA: 6s - loss: 1.4166 - acc: 0.6520 - LRFinder: val_loss: 3.8540 - lr = 0.00909590 \n",
            "349/390 [=========================>....] - ETA: 6s - loss: 1.4163 - acc: 0.6522 - LRFinder: val_loss: 3.8021 - lr = 0.00911692 \n",
            "350/390 [=========================>....] - ETA: 6s - loss: 1.4165 - acc: 0.6520 - LRFinder: val_loss: 3.8952 - lr = 0.00913795 \n",
            "351/390 [==========================>...] - ETA: 5s - loss: 1.4166 - acc: 0.6520 - LRFinder: val_loss: 3.8563 - lr = 0.00915897 \n",
            "352/390 [==========================>...] - ETA: 5s - loss: 1.4169 - acc: 0.6519 - LRFinder: val_loss: 3.8439 - lr = 0.00918000 \n",
            "353/390 [==========================>...] - ETA: 5s - loss: 1.4165 - acc: 0.6520 - LRFinder: val_loss: 3.8018 - lr = 0.00920103 \n",
            "354/390 [==========================>...] - ETA: 5s - loss: 1.4165 - acc: 0.6521 - LRFinder: val_loss: 4.0389 - lr = 0.00922205 \n",
            "355/390 [==========================>...] - ETA: 5s - loss: 1.4163 - acc: 0.6522 - LRFinder: val_loss: 3.8253 - lr = 0.00924308 \n",
            "356/390 [==========================>...] - ETA: 5s - loss: 1.4164 - acc: 0.6522 - LRFinder: val_loss: 3.8899 - lr = 0.00926410 \n",
            "357/390 [==========================>...] - ETA: 5s - loss: 1.4167 - acc: 0.6520 - LRFinder: val_loss: 4.0753 - lr = 0.00928513 \n",
            "358/390 [==========================>...] - ETA: 4s - loss: 1.4168 - acc: 0.6518 - LRFinder: val_loss: 4.0969 - lr = 0.00930615 \n",
            "359/390 [==========================>...] - ETA: 4s - loss: 1.4165 - acc: 0.6518 - LRFinder: val_loss: 4.0610 - lr = 0.00932718 \n",
            "360/390 [==========================>...] - ETA: 4s - loss: 1.4162 - acc: 0.6520 - LRFinder: val_loss: 3.9998 - lr = 0.00934821 \n",
            "361/390 [==========================>...] - ETA: 4s - loss: 1.4161 - acc: 0.6519 - LRFinder: val_loss: 3.9169 - lr = 0.00936923 \n",
            "362/390 [==========================>...] - ETA: 4s - loss: 1.4160 - acc: 0.6519 - LRFinder: val_loss: 3.8963 - lr = 0.00939026 \n",
            "363/390 [==========================>...] - ETA: 4s - loss: 1.4162 - acc: 0.6518 - LRFinder: val_loss: 3.7818 - lr = 0.00941128 \n",
            "364/390 [===========================>..] - ETA: 3s - loss: 1.4163 - acc: 0.6519 - LRFinder: val_loss: 3.7130 - lr = 0.00943231 \n",
            "365/390 [===========================>..] - ETA: 3s - loss: 1.4165 - acc: 0.6518 - LRFinder: val_loss: 3.8669 - lr = 0.00945333 \n",
            "366/390 [===========================>..] - ETA: 3s - loss: 1.4163 - acc: 0.6519 - LRFinder: val_loss: 3.8115 - lr = 0.00947436 \n",
            "367/390 [===========================>..] - ETA: 3s - loss: 1.4160 - acc: 0.6521 - LRFinder: val_loss: 4.0262 - lr = 0.00949538 \n",
            "368/390 [===========================>..] - ETA: 3s - loss: 1.4157 - acc: 0.6521 - LRFinder: val_loss: 3.9002 - lr = 0.00951641 \n",
            "369/390 [===========================>..] - ETA: 3s - loss: 1.4155 - acc: 0.6521 - LRFinder: val_loss: 3.7970 - lr = 0.00953744 \n",
            "370/390 [===========================>..] - ETA: 3s - loss: 1.4152 - acc: 0.6520 - LRFinder: val_loss: 3.9041 - lr = 0.00955846 \n",
            "371/390 [===========================>..] - ETA: 2s - loss: 1.4151 - acc: 0.6521 - LRFinder: val_loss: 3.9209 - lr = 0.00957949 \n",
            "372/390 [===========================>..] - ETA: 2s - loss: 1.4144 - acc: 0.6523 - LRFinder: val_loss: 3.7920 - lr = 0.00960051 \n",
            "373/390 [===========================>..] - ETA: 2s - loss: 1.4143 - acc: 0.6522 - LRFinder: val_loss: 3.8374 - lr = 0.00962154 \n",
            "374/390 [===========================>..] - ETA: 2s - loss: 1.4141 - acc: 0.6522 - LRFinder: val_loss: 3.9010 - lr = 0.00964256 \n",
            "375/390 [===========================>..] - ETA: 2s - loss: 1.4138 - acc: 0.6523 - LRFinder: val_loss: 3.9229 - lr = 0.00966359 \n",
            "376/390 [===========================>..] - ETA: 2s - loss: 1.4138 - acc: 0.6522 - LRFinder: val_loss: 3.9193 - lr = 0.00968462 \n",
            "377/390 [============================>.] - ETA: 1s - loss: 1.4141 - acc: 0.6521 - LRFinder: val_loss: 3.8810 - lr = 0.00970564 \n",
            "378/390 [============================>.] - ETA: 1s - loss: 1.4135 - acc: 0.6522 - LRFinder: val_loss: 3.8568 - lr = 0.00972667 \n",
            "379/390 [============================>.] - ETA: 1s - loss: 1.4135 - acc: 0.6522 - LRFinder: val_loss: 3.9135 - lr = 0.00974769 \n",
            "380/390 [============================>.] - ETA: 1s - loss: 1.4134 - acc: 0.6522 - LRFinder: val_loss: 3.9801 - lr = 0.00976872 \n",
            "381/390 [============================>.] - ETA: 1s - loss: 1.4132 - acc: 0.6523 - LRFinder: val_loss: 4.0083 - lr = 0.00978974 \n",
            "382/390 [============================>.] - ETA: 1s - loss: 1.4130 - acc: 0.6524 - LRFinder: val_loss: 3.9340 - lr = 0.00981077 \n",
            "383/390 [============================>.] - ETA: 1s - loss: 1.4129 - acc: 0.6523 - LRFinder: val_loss: 4.2275 - lr = 0.00983179 \n",
            "384/390 [============================>.] - ETA: 0s - loss: 1.4128 - acc: 0.6524 - LRFinder: val_loss: 4.0624 - lr = 0.00985282 \n",
            "385/390 [============================>.] - ETA: 0s - loss: 1.4129 - acc: 0.6524 - LRFinder: val_loss: 4.1111 - lr = 0.00987385 \n",
            "386/390 [============================>.] - ETA: 0s - loss: 1.4133 - acc: 0.6522 - LRFinder: val_loss: 3.8965 - lr = 0.00989487 \n",
            "387/390 [============================>.] - ETA: 0s - loss: 1.4133 - acc: 0.6522 - LRFinder: val_loss: 4.0160 - lr = 0.00991590 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 1.4131 - acc: 0.6522 - LRFinder: val_loss: 4.1207 - lr = 0.00993692 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.4133 - acc: 0.6522 - LRFinder: val_loss: 4.2790 - lr = 0.00995795 \n",
            " - LRFinder: val_loss: 4.3771 - lr = 0.00997897 \n",
            "390/390 [==============================] - 60s 155ms/step - loss: 1.4137 - acc: 0.6521 - val_loss: 4.4372 - val_acc: 0.1624\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {weights/weight_decay/weight_decay-3e-07/}\n",
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 12:12 - loss: 1.4216 - acc: 0.6562 - LRFinder: val_loss: 4.5847 - lr = 0.00180000 \n",
            "  2/390 [..............................] - ETA: 8:56 - loss: 1.4151 - acc: 0.6680  - LRFinder: val_loss: 4.4610 - lr = 0.00182103 \n",
            "  3/390 [..............................] - ETA: 6:22 - loss: 1.4254 - acc: 0.6719 - LRFinder: val_loss: 4.4064 - lr = 0.00184205 \n",
            "  4/390 [..............................] - ETA: 4:59 - loss: 1.4203 - acc: 0.6680 - LRFinder: val_loss: 4.4754 - lr = 0.00186308 \n",
            "  5/390 [..............................] - ETA: 4:10 - loss: 1.3963 - acc: 0.6766 - LRFinder: val_loss: 4.1144 - lr = 0.00188410 \n",
            "  6/390 [..............................] - ETA: 3:37 - loss: 1.3872 - acc: 0.6797 - LRFinder: val_loss: 4.3789 - lr = 0.00190513 \n",
            "  7/390 [..............................] - ETA: 3:13 - loss: 1.3935 - acc: 0.6797 - LRFinder: val_loss: 4.0900 - lr = 0.00192615 \n",
            "  8/390 [..............................] - ETA: 2:56 - loss: 1.3942 - acc: 0.6777 - LRFinder: val_loss: 4.0323 - lr = 0.00194718 \n",
            "  9/390 [..............................] - ETA: 2:42 - loss: 1.3927 - acc: 0.6788 - LRFinder: val_loss: 4.0955 - lr = 0.00196821 \n",
            " 10/390 [..............................] - ETA: 2:31 - loss: 1.4051 - acc: 0.6727 - LRFinder: val_loss: 4.0347 - lr = 0.00198923 \n",
            " 11/390 [..............................] - ETA: 2:22 - loss: 1.3980 - acc: 0.6740 - LRFinder: val_loss: 3.9887 - lr = 0.00201026 \n",
            " 12/390 [..............................] - ETA: 2:14 - loss: 1.3978 - acc: 0.6706 - LRFinder: val_loss: 4.0571 - lr = 0.00203128 \n",
            " 13/390 [>.............................] - ETA: 2:07 - loss: 1.3986 - acc: 0.6701 - LRFinder: val_loss: 3.9415 - lr = 0.00205231 \n",
            " 14/390 [>.............................] - ETA: 2:02 - loss: 1.4060 - acc: 0.6669 - LRFinder: val_loss: 3.8927 - lr = 0.00207333 \n",
            " 15/390 [>.............................] - ETA: 1:57 - loss: 1.4013 - acc: 0.6635 - LRFinder: val_loss: 4.0517 - lr = 0.00209436 \n",
            " 16/390 [>.............................] - ETA: 1:53 - loss: 1.3972 - acc: 0.6641 - LRFinder: val_loss: 3.8360 - lr = 0.00211538 \n",
            " 17/390 [>.............................] - ETA: 1:49 - loss: 1.3896 - acc: 0.6673 - LRFinder: val_loss: 3.9042 - lr = 0.00213641 \n",
            " 18/390 [>.............................] - ETA: 1:45 - loss: 1.3940 - acc: 0.6641 - LRFinder: val_loss: 3.9269 - lr = 0.00215744 \n",
            " 19/390 [>.............................] - ETA: 1:42 - loss: 1.3964 - acc: 0.6620 - LRFinder: val_loss: 3.7003 - lr = 0.00217846 \n",
            " 20/390 [>.............................] - ETA: 1:40 - loss: 1.4051 - acc: 0.6586 - LRFinder: val_loss: 3.8797 - lr = 0.00219949 \n",
            " 21/390 [>.............................] - ETA: 1:37 - loss: 1.4064 - acc: 0.6562 - LRFinder: val_loss: 3.9875 - lr = 0.00222051 \n",
            " 22/390 [>.............................] - ETA: 1:35 - loss: 1.4062 - acc: 0.6573 - LRFinder: val_loss: 3.8979 - lr = 0.00224154 \n",
            " 23/390 [>.............................] - ETA: 1:33 - loss: 1.4042 - acc: 0.6593 - LRFinder: val_loss: 3.9604 - lr = 0.00226256 \n",
            " 24/390 [>.............................] - ETA: 1:31 - loss: 1.4052 - acc: 0.6585 - LRFinder: val_loss: 4.0204 - lr = 0.00228359 \n",
            " 25/390 [>.............................] - ETA: 1:29 - loss: 1.4097 - acc: 0.6569 - LRFinder: val_loss: 3.9448 - lr = 0.00230462 \n",
            " 26/390 [=>............................] - ETA: 1:27 - loss: 1.4111 - acc: 0.6547 - LRFinder: val_loss: 3.8771 - lr = 0.00232564 \n",
            " 27/390 [=>............................] - ETA: 1:26 - loss: 1.4104 - acc: 0.6560 - LRFinder: val_loss: 3.9585 - lr = 0.00234667 \n",
            " 28/390 [=>............................] - ETA: 1:24 - loss: 1.4118 - acc: 0.6546 - LRFinder: val_loss: 3.9037 - lr = 0.00236769 \n",
            " 29/390 [=>............................] - ETA: 1:23 - loss: 1.4113 - acc: 0.6549 - LRFinder: val_loss: 3.9626 - lr = 0.00238872 \n",
            " 30/390 [=>............................] - ETA: 1:22 - loss: 1.4054 - acc: 0.6565 - LRFinder: val_loss: 3.8945 - lr = 0.00240974 \n",
            " 31/390 [=>............................] - ETA: 1:21 - loss: 1.4055 - acc: 0.6570 - LRFinder: val_loss: 4.0834 - lr = 0.00243077 \n",
            " 32/390 [=>............................] - ETA: 1:19 - loss: 1.4035 - acc: 0.6577 - LRFinder: val_loss: 4.0627 - lr = 0.00245179 \n",
            " 33/390 [=>............................] - ETA: 1:18 - loss: 1.4039 - acc: 0.6555 - LRFinder: val_loss: 3.9400 - lr = 0.00247282 \n",
            " 34/390 [=>............................] - ETA: 1:17 - loss: 1.4065 - acc: 0.6544 - LRFinder: val_loss: 3.9019 - lr = 0.00249385 \n",
            " 35/390 [=>............................] - ETA: 1:16 - loss: 1.4050 - acc: 0.6545 - LRFinder: val_loss: 4.2648 - lr = 0.00251487 \n",
            " 36/390 [=>............................] - ETA: 1:15 - loss: 1.4048 - acc: 0.6543 - LRFinder: val_loss: 3.9848 - lr = 0.00253590 \n",
            " 37/390 [=>............................] - ETA: 1:15 - loss: 1.4030 - acc: 0.6535 - LRFinder: val_loss: 4.0366 - lr = 0.00255692 \n",
            " 38/390 [=>............................] - ETA: 1:14 - loss: 1.4013 - acc: 0.6544 - LRFinder: val_loss: 4.0045 - lr = 0.00257795 \n",
            " 39/390 [==>...........................] - ETA: 1:13 - loss: 1.3993 - acc: 0.6544 - LRFinder: val_loss: 4.2380 - lr = 0.00259897 \n",
            " 40/390 [==>...........................] - ETA: 1:12 - loss: 1.3990 - acc: 0.6549 - LRFinder: val_loss: 3.8572 - lr = 0.00262000 \n",
            " 41/390 [==>...........................] - ETA: 1:11 - loss: 1.3995 - acc: 0.6545 - LRFinder: val_loss: 4.0180 - lr = 0.00264103 \n",
            " 42/390 [==>...........................] - ETA: 1:11 - loss: 1.3993 - acc: 0.6540 - LRFinder: val_loss: 4.0550 - lr = 0.00266205 \n",
            " 43/390 [==>...........................] - ETA: 1:10 - loss: 1.4000 - acc: 0.6544 - LRFinder: val_loss: 3.9776 - lr = 0.00268308 \n",
            " 44/390 [==>...........................] - ETA: 1:09 - loss: 1.3962 - acc: 0.6557 - LRFinder: val_loss: 4.1302 - lr = 0.00270410 \n",
            " 45/390 [==>...........................] - ETA: 1:09 - loss: 1.3962 - acc: 0.6566 - LRFinder: val_loss: 3.7625 - lr = 0.00272513 \n",
            " 46/390 [==>...........................] - ETA: 1:08 - loss: 1.3943 - acc: 0.6574 - LRFinder: val_loss: 3.8783 - lr = 0.00274615 \n",
            " 47/390 [==>...........................] - ETA: 1:08 - loss: 1.3964 - acc: 0.6559 - LRFinder: val_loss: 4.1592 - lr = 0.00276718 \n",
            " 48/390 [==>...........................] - ETA: 1:07 - loss: 1.3947 - acc: 0.6558 - LRFinder: val_loss: 3.9117 - lr = 0.00278821 \n",
            " 49/390 [==>...........................] - ETA: 1:06 - loss: 1.3922 - acc: 0.6566 - LRFinder: val_loss: 3.9166 - lr = 0.00280923 \n",
            " 50/390 [==>...........................] - ETA: 1:06 - loss: 1.3909 - acc: 0.6575 - LRFinder: val_loss: 3.9069 - lr = 0.00283026 \n",
            " 51/390 [==>...........................] - ETA: 1:05 - loss: 1.3909 - acc: 0.6569 - LRFinder: val_loss: 3.7972 - lr = 0.00285128 \n",
            " 52/390 [===>..........................] - ETA: 1:05 - loss: 1.3900 - acc: 0.6569 - LRFinder: val_loss: 3.7562 - lr = 0.00287231 \n",
            " 53/390 [===>..........................] - ETA: 1:04 - loss: 1.3897 - acc: 0.6577 - LRFinder: val_loss: 3.5958 - lr = 0.00289333 \n",
            " 54/390 [===>..........................] - ETA: 1:04 - loss: 1.3879 - acc: 0.6583 - LRFinder: val_loss: 3.9271 - lr = 0.00291436 \n",
            " 55/390 [===>..........................] - ETA: 1:03 - loss: 1.3855 - acc: 0.6594 - LRFinder: val_loss: 3.9845 - lr = 0.00293538 \n",
            " 56/390 [===>..........................] - ETA: 1:03 - loss: 1.3871 - acc: 0.6581 - LRFinder: val_loss: 3.8629 - lr = 0.00295641 \n",
            " 57/390 [===>..........................] - ETA: 1:02 - loss: 1.3851 - acc: 0.6580 - LRFinder: val_loss: 3.8885 - lr = 0.00297744 \n",
            " 58/390 [===>..........................] - ETA: 1:02 - loss: 1.3826 - acc: 0.6583 - LRFinder: val_loss: 3.8926 - lr = 0.00299846 \n",
            " 59/390 [===>..........................] - ETA: 1:02 - loss: 1.3826 - acc: 0.6580 - LRFinder: val_loss: 3.9582 - lr = 0.00301949 \n",
            " 60/390 [===>..........................] - ETA: 1:01 - loss: 1.3796 - acc: 0.6594 - LRFinder: val_loss: 4.0029 - lr = 0.00304051 \n",
            " 61/390 [===>..........................] - ETA: 1:01 - loss: 1.3775 - acc: 0.6601 - LRFinder: val_loss: 3.7841 - lr = 0.00306154 \n",
            " 62/390 [===>..........................] - ETA: 1:00 - loss: 1.3784 - acc: 0.6594 - LRFinder: val_loss: 3.9809 - lr = 0.00308256 \n",
            " 63/390 [===>..........................] - ETA: 1:00 - loss: 1.3780 - acc: 0.6596 - LRFinder: val_loss: 3.8245 - lr = 0.00310359 \n",
            " 64/390 [===>..........................] - ETA: 1:00 - loss: 1.3790 - acc: 0.6600 - LRFinder: val_loss: 3.9647 - lr = 0.00312462 \n",
            " 65/390 [====>.........................] - ETA: 59s - loss: 1.3792 - acc: 0.6602  - LRFinder: val_loss: 4.0918 - lr = 0.00314564 \n",
            " 66/390 [====>.........................] - ETA: 59s - loss: 1.3786 - acc: 0.6603 - LRFinder: val_loss: 3.8861 - lr = 0.00316667 \n",
            " 67/390 [====>.........................] - ETA: 58s - loss: 1.3779 - acc: 0.6608 - LRFinder: val_loss: 3.8829 - lr = 0.00318769 \n",
            " 68/390 [====>.........................] - ETA: 58s - loss: 1.3777 - acc: 0.6611 - LRFinder: val_loss: 3.7874 - lr = 0.00320872 \n",
            " 69/390 [====>.........................] - ETA: 58s - loss: 1.3799 - acc: 0.6598 - LRFinder: val_loss: 3.7034 - lr = 0.00322974 \n",
            " 70/390 [====>.........................] - ETA: 57s - loss: 1.3764 - acc: 0.6609 - LRFinder: val_loss: 3.7383 - lr = 0.00325077 \n",
            " 71/390 [====>.........................] - ETA: 57s - loss: 1.3746 - acc: 0.6621 - LRFinder: val_loss: 3.6070 - lr = 0.00327179 \n",
            " 72/390 [====>.........................] - ETA: 57s - loss: 1.3754 - acc: 0.6619 - LRFinder: val_loss: 3.6653 - lr = 0.00329282 \n",
            " 73/390 [====>.........................] - ETA: 56s - loss: 1.3737 - acc: 0.6627 - LRFinder: val_loss: 3.7304 - lr = 0.00331385 \n",
            " 74/390 [====>.........................] - ETA: 56s - loss: 1.3759 - acc: 0.6617 - LRFinder: val_loss: 3.7666 - lr = 0.00333487 \n",
            " 75/390 [====>.........................] - ETA: 56s - loss: 1.3767 - acc: 0.6615 - LRFinder: val_loss: 3.7666 - lr = 0.00335590 \n",
            " 76/390 [====>.........................] - ETA: 55s - loss: 1.3774 - acc: 0.6613 - LRFinder: val_loss: 3.7865 - lr = 0.00337692 \n",
            " 77/390 [====>.........................] - ETA: 55s - loss: 1.3777 - acc: 0.6616 - LRFinder: val_loss: 3.5963 - lr = 0.00339795 \n",
            " 78/390 [=====>........................] - ETA: 55s - loss: 1.3773 - acc: 0.6618 - LRFinder: val_loss: 3.6685 - lr = 0.00341897 \n",
            " 79/390 [=====>........................] - ETA: 54s - loss: 1.3772 - acc: 0.6620 - LRFinder: val_loss: 3.6633 - lr = 0.00344000 \n",
            " 80/390 [=====>........................] - ETA: 54s - loss: 1.3757 - acc: 0.6630 - LRFinder: val_loss: 3.6985 - lr = 0.00346103 \n",
            " 81/390 [=====>........................] - ETA: 54s - loss: 1.3744 - acc: 0.6631 - LRFinder: val_loss: 3.9180 - lr = 0.00348205 \n",
            " 82/390 [=====>........................] - ETA: 54s - loss: 1.3745 - acc: 0.6629 - LRFinder: val_loss: 3.7976 - lr = 0.00350308 \n",
            " 83/390 [=====>........................] - ETA: 53s - loss: 1.3768 - acc: 0.6622 - LRFinder: val_loss: 3.9304 - lr = 0.00352410 \n",
            " 84/390 [=====>........................] - ETA: 53s - loss: 1.3801 - acc: 0.6611 - LRFinder: val_loss: 3.8850 - lr = 0.00354513 \n",
            " 85/390 [=====>........................] - ETA: 53s - loss: 1.3795 - acc: 0.6613 - LRFinder: val_loss: 3.9699 - lr = 0.00356615 \n",
            " 86/390 [=====>........................] - ETA: 52s - loss: 1.3779 - acc: 0.6611 - LRFinder: val_loss: 3.6720 - lr = 0.00358718 \n",
            " 87/390 [=====>........................] - ETA: 52s - loss: 1.3788 - acc: 0.6607 - LRFinder: val_loss: 4.0068 - lr = 0.00360821 \n",
            " 88/390 [=====>........................] - ETA: 52s - loss: 1.3799 - acc: 0.6604 - LRFinder: val_loss: 3.9698 - lr = 0.00362923 \n",
            " 89/390 [=====>........................] - ETA: 52s - loss: 1.3797 - acc: 0.6607 - LRFinder: val_loss: 3.8014 - lr = 0.00365026 \n",
            " 90/390 [=====>........................] - ETA: 51s - loss: 1.3781 - acc: 0.6617 - LRFinder: val_loss: 3.9566 - lr = 0.00367128 \n",
            " 91/390 [======>.......................] - ETA: 51s - loss: 1.3808 - acc: 0.6610 - LRFinder: val_loss: 3.9144 - lr = 0.00369231 \n",
            " 92/390 [======>.......................] - ETA: 51s - loss: 1.3801 - acc: 0.6613 - LRFinder: val_loss: 3.8364 - lr = 0.00371333 \n",
            " 93/390 [======>.......................] - ETA: 51s - loss: 1.3809 - acc: 0.6611 - LRFinder: val_loss: 3.9950 - lr = 0.00373436 \n",
            " 94/390 [======>.......................] - ETA: 50s - loss: 1.3796 - acc: 0.6613 - LRFinder: val_loss: 3.9196 - lr = 0.00375538 \n",
            " 95/390 [======>.......................] - ETA: 50s - loss: 1.3806 - acc: 0.6615 - LRFinder: val_loss: 3.9522 - lr = 0.00377641 \n",
            " 96/390 [======>.......................] - ETA: 50s - loss: 1.3794 - acc: 0.6619 - LRFinder: val_loss: 3.9157 - lr = 0.00379744 \n",
            " 97/390 [======>.......................] - ETA: 50s - loss: 1.3800 - acc: 0.6616 - LRFinder: val_loss: 3.9694 - lr = 0.00381846 \n",
            " 98/390 [======>.......................] - ETA: 49s - loss: 1.3797 - acc: 0.6619 - LRFinder: val_loss: 3.8051 - lr = 0.00383949 \n",
            " 99/390 [======>.......................] - ETA: 49s - loss: 1.3778 - acc: 0.6623 - LRFinder: val_loss: 3.7463 - lr = 0.00386051 \n",
            "100/390 [======>.......................] - ETA: 49s - loss: 1.3784 - acc: 0.6620 - LRFinder: val_loss: 3.7745 - lr = 0.00388154 \n",
            "101/390 [======>.......................] - ETA: 49s - loss: 1.3770 - acc: 0.6627 - LRFinder: val_loss: 3.7175 - lr = 0.00390256 \n",
            "102/390 [======>.......................] - ETA: 48s - loss: 1.3775 - acc: 0.6628 - LRFinder: val_loss: 3.7828 - lr = 0.00392359 \n",
            "103/390 [======>.......................] - ETA: 48s - loss: 1.3772 - acc: 0.6632 - LRFinder: val_loss: 3.8443 - lr = 0.00394462 \n",
            "104/390 [=======>......................] - ETA: 48s - loss: 1.3779 - acc: 0.6629 - LRFinder: val_loss: 3.9360 - lr = 0.00396564 \n",
            "105/390 [=======>......................] - ETA: 48s - loss: 1.3776 - acc: 0.6628 - LRFinder: val_loss: 3.9263 - lr = 0.00398667 \n",
            "106/390 [=======>......................] - ETA: 47s - loss: 1.3769 - acc: 0.6628 - LRFinder: val_loss: 3.8963 - lr = 0.00400769 \n",
            "107/390 [=======>......................] - ETA: 47s - loss: 1.3768 - acc: 0.6629 - LRFinder: val_loss: 3.7698 - lr = 0.00402872 \n",
            "108/390 [=======>......................] - ETA: 47s - loss: 1.3784 - acc: 0.6619 - LRFinder: val_loss: 4.1153 - lr = 0.00404974 \n",
            "109/390 [=======>......................] - ETA: 47s - loss: 1.3781 - acc: 0.6618 - LRFinder: val_loss: 3.8293 - lr = 0.00407077 \n",
            "110/390 [=======>......................] - ETA: 46s - loss: 1.3787 - acc: 0.6621 - LRFinder: val_loss: 3.7717 - lr = 0.00409179 \n",
            "111/390 [=======>......................] - ETA: 46s - loss: 1.3786 - acc: 0.6620 - LRFinder: val_loss: 3.9201 - lr = 0.00411282 \n",
            "112/390 [=======>......................] - ETA: 46s - loss: 1.3785 - acc: 0.6620 - LRFinder: val_loss: 3.9969 - lr = 0.00413385 \n",
            "113/390 [=======>......................] - ETA: 46s - loss: 1.3789 - acc: 0.6618 - LRFinder: val_loss: 3.9644 - lr = 0.00415487 \n",
            "114/390 [=======>......................] - ETA: 46s - loss: 1.3786 - acc: 0.6616 - LRFinder: val_loss: 3.8092 - lr = 0.00417590 \n",
            "115/390 [=======>......................] - ETA: 45s - loss: 1.3792 - acc: 0.6614 - LRFinder: val_loss: 3.9456 - lr = 0.00419692 \n",
            "116/390 [=======>......................] - ETA: 45s - loss: 1.3788 - acc: 0.6618 - LRFinder: val_loss: 4.0538 - lr = 0.00421795 \n",
            "117/390 [========>.....................] - ETA: 45s - loss: 1.3785 - acc: 0.6614 - LRFinder: val_loss: 4.0119 - lr = 0.00423897 \n",
            "118/390 [========>.....................] - ETA: 45s - loss: 1.3767 - acc: 0.6621 - LRFinder: val_loss: 4.0299 - lr = 0.00426000 \n",
            "119/390 [========>.....................] - ETA: 45s - loss: 1.3764 - acc: 0.6622 - LRFinder: val_loss: 3.8487 - lr = 0.00428103 \n",
            "120/390 [========>.....................] - ETA: 44s - loss: 1.3760 - acc: 0.6622 - LRFinder: val_loss: 3.7224 - lr = 0.00430205 \n",
            "121/390 [========>.....................] - ETA: 44s - loss: 1.3763 - acc: 0.6620 - LRFinder: val_loss: 3.9829 - lr = 0.00432308 \n",
            "122/390 [========>.....................] - ETA: 44s - loss: 1.3767 - acc: 0.6618 - LRFinder: val_loss: 3.9401 - lr = 0.00434410 \n",
            "123/390 [========>.....................] - ETA: 44s - loss: 1.3767 - acc: 0.6617 - LRFinder: val_loss: 3.8532 - lr = 0.00436513 \n",
            "124/390 [========>.....................] - ETA: 43s - loss: 1.3767 - acc: 0.6617 - LRFinder: val_loss: 3.5580 - lr = 0.00438615 \n",
            "125/390 [========>.....................] - ETA: 43s - loss: 1.3767 - acc: 0.6620 - LRFinder: val_loss: 3.7025 - lr = 0.00440718 \n",
            "126/390 [========>.....................] - ETA: 43s - loss: 1.3761 - acc: 0.6624 - LRFinder: val_loss: 3.6656 - lr = 0.00442821 \n",
            "127/390 [========>.....................] - ETA: 43s - loss: 1.3764 - acc: 0.6625 - LRFinder: val_loss: 3.8482 - lr = 0.00444923 \n",
            "128/390 [========>.....................] - ETA: 43s - loss: 1.3761 - acc: 0.6624 - LRFinder: val_loss: 3.8702 - lr = 0.00447026 \n",
            "129/390 [========>.....................] - ETA: 42s - loss: 1.3771 - acc: 0.6625 - LRFinder: val_loss: 3.7393 - lr = 0.00449128 \n",
            "130/390 [=========>....................] - ETA: 42s - loss: 1.3777 - acc: 0.6624 - LRFinder: val_loss: 3.7736 - lr = 0.00451231 \n",
            "131/390 [=========>....................] - ETA: 42s - loss: 1.3769 - acc: 0.6626 - LRFinder: val_loss: 3.7625 - lr = 0.00453333 \n",
            "132/390 [=========>....................] - ETA: 42s - loss: 1.3764 - acc: 0.6628 - LRFinder: val_loss: 3.9529 - lr = 0.00455436 \n",
            "133/390 [=========>....................] - ETA: 42s - loss: 1.3759 - acc: 0.6629 - LRFinder: val_loss: 3.8007 - lr = 0.00457538 \n",
            "134/390 [=========>....................] - ETA: 41s - loss: 1.3763 - acc: 0.6630 - LRFinder: val_loss: 3.8094 - lr = 0.00459641 \n",
            "135/390 [=========>....................] - ETA: 41s - loss: 1.3764 - acc: 0.6631 - LRFinder: val_loss: 3.8331 - lr = 0.00461744 \n",
            "136/390 [=========>....................] - ETA: 41s - loss: 1.3774 - acc: 0.6627 - LRFinder: val_loss: 3.8696 - lr = 0.00463846 \n",
            "137/390 [=========>....................] - ETA: 41s - loss: 1.3779 - acc: 0.6622 - LRFinder: val_loss: 3.7502 - lr = 0.00465949 \n",
            "138/390 [=========>....................] - ETA: 41s - loss: 1.3785 - acc: 0.6620 - LRFinder: val_loss: 3.8590 - lr = 0.00468051 \n",
            "139/390 [=========>....................] - ETA: 41s - loss: 1.3787 - acc: 0.6620 - LRFinder: val_loss: 4.0537 - lr = 0.00470154 \n",
            "140/390 [=========>....................] - ETA: 40s - loss: 1.3783 - acc: 0.6620 - LRFinder: val_loss: 3.8944 - lr = 0.00472256 \n",
            "141/390 [=========>....................] - ETA: 40s - loss: 1.3785 - acc: 0.6620 - LRFinder: val_loss: 4.0400 - lr = 0.00474359 \n",
            "142/390 [=========>....................] - ETA: 40s - loss: 1.3790 - acc: 0.6619 - LRFinder: val_loss: 3.9071 - lr = 0.00476462 \n",
            "143/390 [==========>...................] - ETA: 40s - loss: 1.3787 - acc: 0.6622 - LRFinder: val_loss: 4.0511 - lr = 0.00478564 \n",
            "144/390 [==========>...................] - ETA: 40s - loss: 1.3780 - acc: 0.6626 - LRFinder: val_loss: 4.1835 - lr = 0.00480667 \n",
            "145/390 [==========>...................] - ETA: 39s - loss: 1.3776 - acc: 0.6629 - LRFinder: val_loss: 4.0030 - lr = 0.00482769 \n",
            "146/390 [==========>...................] - ETA: 39s - loss: 1.3776 - acc: 0.6628 - LRFinder: val_loss: 4.0287 - lr = 0.00484872 \n",
            "147/390 [==========>...................] - ETA: 39s - loss: 1.3777 - acc: 0.6628 - LRFinder: val_loss: 4.0270 - lr = 0.00486974 \n",
            "148/390 [==========>...................] - ETA: 39s - loss: 1.3776 - acc: 0.6631 - LRFinder: val_loss: 3.9094 - lr = 0.00489077 \n",
            "149/390 [==========>...................] - ETA: 39s - loss: 1.3773 - acc: 0.6631 - LRFinder: val_loss: 3.9764 - lr = 0.00491179 \n",
            "150/390 [==========>...................] - ETA: 38s - loss: 1.3770 - acc: 0.6630 - LRFinder: val_loss: 4.1764 - lr = 0.00493282 \n",
            "151/390 [==========>...................] - ETA: 38s - loss: 1.3774 - acc: 0.6629 - LRFinder: val_loss: 4.0829 - lr = 0.00495385 \n",
            "152/390 [==========>...................] - ETA: 38s - loss: 1.3779 - acc: 0.6629 - LRFinder: val_loss: 3.7791 - lr = 0.00497487 \n",
            "153/390 [==========>...................] - ETA: 38s - loss: 1.3771 - acc: 0.6634 - LRFinder: val_loss: 3.8973 - lr = 0.00499590 \n",
            "154/390 [==========>...................] - ETA: 38s - loss: 1.3772 - acc: 0.6633 - LRFinder: val_loss: 3.9262 - lr = 0.00501692 \n",
            "155/390 [==========>...................] - ETA: 37s - loss: 1.3778 - acc: 0.6633 - LRFinder: val_loss: 4.0031 - lr = 0.00503795 \n",
            "156/390 [===========>..................] - ETA: 37s - loss: 1.3780 - acc: 0.6633 - LRFinder: val_loss: 3.9831 - lr = 0.00505897 \n",
            "157/390 [===========>..................] - ETA: 37s - loss: 1.3776 - acc: 0.6631 - LRFinder: val_loss: 3.9304 - lr = 0.00508000 \n",
            "158/390 [===========>..................] - ETA: 37s - loss: 1.3768 - acc: 0.6632 - LRFinder: val_loss: 4.0238 - lr = 0.00510103 \n",
            "159/390 [===========>..................] - ETA: 37s - loss: 1.3780 - acc: 0.6628 - LRFinder: val_loss: 3.8663 - lr = 0.00512205 \n",
            "160/390 [===========>..................] - ETA: 37s - loss: 1.3778 - acc: 0.6626 - LRFinder: val_loss: 3.9145 - lr = 0.00514308 \n",
            "161/390 [===========>..................] - ETA: 36s - loss: 1.3775 - acc: 0.6630 - LRFinder: val_loss: 3.8769 - lr = 0.00516410 \n",
            "162/390 [===========>..................] - ETA: 36s - loss: 1.3789 - acc: 0.6624 - LRFinder: val_loss: 4.0001 - lr = 0.00518513 \n",
            "163/390 [===========>..................] - ETA: 36s - loss: 1.3790 - acc: 0.6624 - LRFinder: val_loss: 3.7576 - lr = 0.00520615 \n",
            "164/390 [===========>..................] - ETA: 36s - loss: 1.3788 - acc: 0.6623 - LRFinder: val_loss: 3.8130 - lr = 0.00522718 \n",
            "165/390 [===========>..................] - ETA: 36s - loss: 1.3789 - acc: 0.6625 - LRFinder: val_loss: 3.9717 - lr = 0.00524821 \n",
            "166/390 [===========>..................] - ETA: 35s - loss: 1.3789 - acc: 0.6626 - LRFinder: val_loss: 3.6754 - lr = 0.00526923 \n",
            "167/390 [===========>..................] - ETA: 35s - loss: 1.3798 - acc: 0.6623 - LRFinder: val_loss: 3.6936 - lr = 0.00529026 \n",
            "168/390 [===========>..................] - ETA: 35s - loss: 1.3791 - acc: 0.6625 - LRFinder: val_loss: 3.8344 - lr = 0.00531128 \n",
            "169/390 [============>.................] - ETA: 35s - loss: 1.3783 - acc: 0.6626 - LRFinder: val_loss: 3.7076 - lr = 0.00533231 \n",
            "170/390 [============>.................] - ETA: 35s - loss: 1.3786 - acc: 0.6622 - LRFinder: val_loss: 3.7889 - lr = 0.00535333 \n",
            "171/390 [============>.................] - ETA: 35s - loss: 1.3781 - acc: 0.6626 - LRFinder: val_loss: 3.5458 - lr = 0.00537436 \n",
            "172/390 [============>.................] - ETA: 34s - loss: 1.3779 - acc: 0.6628 - LRFinder: val_loss: 3.7688 - lr = 0.00539538 \n",
            "173/390 [============>.................] - ETA: 34s - loss: 1.3776 - acc: 0.6631 - LRFinder: val_loss: 3.7255 - lr = 0.00541641 \n",
            "174/390 [============>.................] - ETA: 34s - loss: 1.3780 - acc: 0.6630 - LRFinder: val_loss: 3.6581 - lr = 0.00543744 \n",
            "175/390 [============>.................] - ETA: 34s - loss: 1.3784 - acc: 0.6630 - LRFinder: val_loss: 3.6377 - lr = 0.00545846 \n",
            "176/390 [============>.................] - ETA: 34s - loss: 1.3775 - acc: 0.6631 - LRFinder: val_loss: 3.7546 - lr = 0.00547949 \n",
            "177/390 [============>.................] - ETA: 34s - loss: 1.3786 - acc: 0.6628 - LRFinder: val_loss: 3.6465 - lr = 0.00550051 \n",
            "178/390 [============>.................] - ETA: 33s - loss: 1.3782 - acc: 0.6631 - LRFinder: val_loss: 3.7068 - lr = 0.00552154 \n",
            "179/390 [============>.................] - ETA: 33s - loss: 1.3785 - acc: 0.6630 - LRFinder: val_loss: 3.9322 - lr = 0.00554256 \n",
            "180/390 [============>.................] - ETA: 33s - loss: 1.3783 - acc: 0.6632 - LRFinder: val_loss: 3.7535 - lr = 0.00556359 \n",
            "181/390 [============>.................] - ETA: 33s - loss: 1.3786 - acc: 0.6631 - LRFinder: val_loss: 3.9462 - lr = 0.00558462 \n",
            "182/390 [=============>................] - ETA: 33s - loss: 1.3783 - acc: 0.6631 - LRFinder: val_loss: 3.7686 - lr = 0.00560564 \n",
            "183/390 [=============>................] - ETA: 32s - loss: 1.3786 - acc: 0.6630 - LRFinder: val_loss: 3.6662 - lr = 0.00562667 \n",
            "184/390 [=============>................] - ETA: 32s - loss: 1.3788 - acc: 0.6630 - LRFinder: val_loss: 3.9038 - lr = 0.00564769 \n",
            "185/390 [=============>................] - ETA: 32s - loss: 1.3786 - acc: 0.6631 - LRFinder: val_loss: 3.9614 - lr = 0.00566872 \n",
            "186/390 [=============>................] - ETA: 32s - loss: 1.3784 - acc: 0.6633 - LRFinder: val_loss: 3.6638 - lr = 0.00568974 \n",
            "187/390 [=============>................] - ETA: 32s - loss: 1.3784 - acc: 0.6634 - LRFinder: val_loss: 3.8640 - lr = 0.00571077 \n",
            "188/390 [=============>................] - ETA: 32s - loss: 1.3789 - acc: 0.6632 - LRFinder: val_loss: 3.9130 - lr = 0.00573179 \n",
            "189/390 [=============>................] - ETA: 31s - loss: 1.3790 - acc: 0.6632 - LRFinder: val_loss: 3.9762 - lr = 0.00575282 \n",
            "190/390 [=============>................] - ETA: 31s - loss: 1.3790 - acc: 0.6632 - LRFinder: val_loss: 4.0808 - lr = 0.00577385 \n",
            "191/390 [=============>................] - ETA: 31s - loss: 1.3790 - acc: 0.6633 - LRFinder: val_loss: 3.8790 - lr = 0.00579487 \n",
            "192/390 [=============>................] - ETA: 31s - loss: 1.3790 - acc: 0.6632 - LRFinder: val_loss: 4.0840 - lr = 0.00581590 \n",
            "193/390 [=============>................] - ETA: 31s - loss: 1.3772 - acc: 0.6639 - LRFinder: val_loss: 3.8296 - lr = 0.00583692 \n",
            "194/390 [=============>................] - ETA: 31s - loss: 1.3772 - acc: 0.6638 - LRFinder: val_loss: 4.0513 - lr = 0.00585795 \n",
            "195/390 [==============>...............] - ETA: 30s - loss: 1.3763 - acc: 0.6643 - LRFinder: val_loss: 3.7886 - lr = 0.00587897 \n",
            "196/390 [==============>...............] - ETA: 30s - loss: 1.3764 - acc: 0.6643 - LRFinder: val_loss: 3.8687 - lr = 0.00590000 \n",
            "197/390 [==============>...............] - ETA: 30s - loss: 1.3759 - acc: 0.6646 - LRFinder: val_loss: 3.8982 - lr = 0.00592103 \n",
            "198/390 [==============>...............] - ETA: 30s - loss: 1.3768 - acc: 0.6643 - LRFinder: val_loss: 3.8945 - lr = 0.00594205 \n",
            "199/390 [==============>...............] - ETA: 30s - loss: 1.3772 - acc: 0.6642 - LRFinder: val_loss: 3.8962 - lr = 0.00596308 \n",
            "200/390 [==============>...............] - ETA: 30s - loss: 1.3767 - acc: 0.6645 - LRFinder: val_loss: 3.6596 - lr = 0.00598410 \n",
            "201/390 [==============>...............] - ETA: 29s - loss: 1.3759 - acc: 0.6646 - LRFinder: val_loss: 3.8525 - lr = 0.00600513 \n",
            "202/390 [==============>...............] - ETA: 29s - loss: 1.3763 - acc: 0.6646 - LRFinder: val_loss: 3.7138 - lr = 0.00602615 \n",
            "203/390 [==============>...............] - ETA: 29s - loss: 1.3765 - acc: 0.6645 - LRFinder: val_loss: 3.8533 - lr = 0.00604718 \n",
            "204/390 [==============>...............] - ETA: 29s - loss: 1.3768 - acc: 0.6643 - LRFinder: val_loss: 3.7176 - lr = 0.00606821 \n",
            "205/390 [==============>...............] - ETA: 29s - loss: 1.3763 - acc: 0.6644 - LRFinder: val_loss: 3.7422 - lr = 0.00608923 \n",
            "206/390 [==============>...............] - ETA: 29s - loss: 1.3761 - acc: 0.6646 - LRFinder: val_loss: 3.8009 - lr = 0.00611026 \n",
            "207/390 [==============>...............] - ETA: 28s - loss: 1.3768 - acc: 0.6644 - LRFinder: val_loss: 3.9453 - lr = 0.00613128 \n",
            "208/390 [===============>..............] - ETA: 28s - loss: 1.3766 - acc: 0.6645 - LRFinder: val_loss: 3.9716 - lr = 0.00615231 \n",
            "209/390 [===============>..............] - ETA: 28s - loss: 1.3764 - acc: 0.6644 - LRFinder: val_loss: 3.9632 - lr = 0.00617333 \n",
            "210/390 [===============>..............] - ETA: 28s - loss: 1.3765 - acc: 0.6644 - LRFinder: val_loss: 3.8897 - lr = 0.00619436 \n",
            "211/390 [===============>..............] - ETA: 28s - loss: 1.3761 - acc: 0.6644 - LRFinder: val_loss: 4.0023 - lr = 0.00621538 \n",
            "212/390 [===============>..............] - ETA: 28s - loss: 1.3762 - acc: 0.6642 - LRFinder: val_loss: 3.9147 - lr = 0.00623641 \n",
            "213/390 [===============>..............] - ETA: 27s - loss: 1.3762 - acc: 0.6644 - LRFinder: val_loss: 3.9436 - lr = 0.00625744 \n",
            "214/390 [===============>..............] - ETA: 27s - loss: 1.3755 - acc: 0.6645 - LRFinder: val_loss: 3.8286 - lr = 0.00627846 \n",
            "215/390 [===============>..............] - ETA: 27s - loss: 1.3754 - acc: 0.6647 - LRFinder: val_loss: 3.8251 - lr = 0.00629949 \n",
            "216/390 [===============>..............] - ETA: 27s - loss: 1.3751 - acc: 0.6648 - LRFinder: val_loss: 3.8804 - lr = 0.00632051 \n",
            "217/390 [===============>..............] - ETA: 27s - loss: 1.3749 - acc: 0.6647 - LRFinder: val_loss: 3.7942 - lr = 0.00634154 \n",
            "218/390 [===============>..............] - ETA: 27s - loss: 1.3743 - acc: 0.6650 - LRFinder: val_loss: 4.0146 - lr = 0.00636256 \n",
            "219/390 [===============>..............] - ETA: 26s - loss: 1.3745 - acc: 0.6647 - LRFinder: val_loss: 3.8330 - lr = 0.00638359 \n",
            "220/390 [===============>..............] - ETA: 26s - loss: 1.3743 - acc: 0.6648 - LRFinder: val_loss: 3.8394 - lr = 0.00640462 \n",
            "221/390 [================>.............] - ETA: 26s - loss: 1.3747 - acc: 0.6649 - LRFinder: val_loss: 3.8003 - lr = 0.00642564 \n",
            "222/390 [================>.............] - ETA: 26s - loss: 1.3742 - acc: 0.6654 - LRFinder: val_loss: 3.8209 - lr = 0.00644667 \n",
            "223/390 [================>.............] - ETA: 26s - loss: 1.3737 - acc: 0.6654 - LRFinder: val_loss: 3.7253 - lr = 0.00646769 \n",
            "224/390 [================>.............] - ETA: 26s - loss: 1.3729 - acc: 0.6658 - LRFinder: val_loss: 3.9760 - lr = 0.00648872 \n",
            "225/390 [================>.............] - ETA: 25s - loss: 1.3723 - acc: 0.6661 - LRFinder: val_loss: 3.5826 - lr = 0.00650974 \n",
            "226/390 [================>.............] - ETA: 25s - loss: 1.3721 - acc: 0.6663 - LRFinder: val_loss: 3.7506 - lr = 0.00653077 \n",
            "227/390 [================>.............] - ETA: 25s - loss: 1.3726 - acc: 0.6661 - LRFinder: val_loss: 3.7367 - lr = 0.00655179 \n",
            "228/390 [================>.............] - ETA: 25s - loss: 1.3736 - acc: 0.6658 - LRFinder: val_loss: 3.6930 - lr = 0.00657282 \n",
            "229/390 [================>.............] - ETA: 25s - loss: 1.3734 - acc: 0.6658 - LRFinder: val_loss: 3.7946 - lr = 0.00659385 \n",
            "230/390 [================>.............] - ETA: 25s - loss: 1.3732 - acc: 0.6657 - LRFinder: val_loss: 3.9865 - lr = 0.00661487 \n",
            "231/390 [================>.............] - ETA: 24s - loss: 1.3734 - acc: 0.6655 - LRFinder: val_loss: 3.9435 - lr = 0.00663590 \n",
            "232/390 [================>.............] - ETA: 24s - loss: 1.3728 - acc: 0.6657 - LRFinder: val_loss: 3.9596 - lr = 0.00665692 \n",
            "233/390 [================>.............] - ETA: 24s - loss: 1.3721 - acc: 0.6659 - LRFinder: val_loss: 3.5893 - lr = 0.00667795 \n",
            "234/390 [=================>............] - ETA: 24s - loss: 1.3725 - acc: 0.6659 - LRFinder: val_loss: 3.8934 - lr = 0.00669897 \n",
            "235/390 [=================>............] - ETA: 24s - loss: 1.3731 - acc: 0.6656 - LRFinder: val_loss: 3.8655 - lr = 0.00672000 \n",
            "236/390 [=================>............] - ETA: 24s - loss: 1.3729 - acc: 0.6658 - LRFinder: val_loss: 3.7145 - lr = 0.00674103 \n",
            "237/390 [=================>............] - ETA: 23s - loss: 1.3733 - acc: 0.6657 - LRFinder: val_loss: 3.8961 - lr = 0.00676205 \n",
            "238/390 [=================>............] - ETA: 23s - loss: 1.3732 - acc: 0.6658 - LRFinder: val_loss: 3.8969 - lr = 0.00678308 \n",
            "239/390 [=================>............] - ETA: 23s - loss: 1.3731 - acc: 0.6658 - LRFinder: val_loss: 3.8142 - lr = 0.00680410 \n",
            "240/390 [=================>............] - ETA: 23s - loss: 1.3729 - acc: 0.6660 - LRFinder: val_loss: 3.9134 - lr = 0.00682513 \n",
            "241/390 [=================>............] - ETA: 23s - loss: 1.3732 - acc: 0.6658 - LRFinder: val_loss: 3.8124 - lr = 0.00684615 \n",
            "242/390 [=================>............] - ETA: 23s - loss: 1.3728 - acc: 0.6659 - LRFinder: val_loss: 3.9097 - lr = 0.00686718 \n",
            "243/390 [=================>............] - ETA: 22s - loss: 1.3737 - acc: 0.6657 - LRFinder: val_loss: 3.9234 - lr = 0.00688821 \n",
            "244/390 [=================>............] - ETA: 22s - loss: 1.3740 - acc: 0.6657 - LRFinder: val_loss: 3.8654 - lr = 0.00690923 \n",
            "245/390 [=================>............] - ETA: 22s - loss: 1.3732 - acc: 0.6660 - LRFinder: val_loss: 3.8141 - lr = 0.00693026 \n",
            "246/390 [=================>............] - ETA: 22s - loss: 1.3734 - acc: 0.6660 - LRFinder: val_loss: 3.9272 - lr = 0.00695128 \n",
            "247/390 [==================>...........] - ETA: 22s - loss: 1.3739 - acc: 0.6657 - LRFinder: val_loss: 3.7510 - lr = 0.00697231 \n",
            "248/390 [==================>...........] - ETA: 22s - loss: 1.3739 - acc: 0.6657 - LRFinder: val_loss: 3.7723 - lr = 0.00699333 \n",
            "249/390 [==================>...........] - ETA: 21s - loss: 1.3741 - acc: 0.6656 - LRFinder: val_loss: 3.6973 - lr = 0.00701436 \n",
            "250/390 [==================>...........] - ETA: 21s - loss: 1.3740 - acc: 0.6657 - LRFinder: val_loss: 3.8672 - lr = 0.00703538 \n",
            "251/390 [==================>...........] - ETA: 21s - loss: 1.3743 - acc: 0.6657 - LRFinder: val_loss: 3.9299 - lr = 0.00705641 \n",
            "252/390 [==================>...........] - ETA: 21s - loss: 1.3742 - acc: 0.6657 - LRFinder: val_loss: 3.9867 - lr = 0.00707744 \n",
            "253/390 [==================>...........] - ETA: 21s - loss: 1.3739 - acc: 0.6656 - LRFinder: val_loss: 3.9196 - lr = 0.00709846 \n",
            "254/390 [==================>...........] - ETA: 21s - loss: 1.3736 - acc: 0.6658 - LRFinder: val_loss: 4.0586 - lr = 0.00711949 \n",
            "255/390 [==================>...........] - ETA: 20s - loss: 1.3733 - acc: 0.6658 - LRFinder: val_loss: 3.7840 - lr = 0.00714051 \n",
            "256/390 [==================>...........] - ETA: 20s - loss: 1.3729 - acc: 0.6660 - LRFinder: val_loss: 3.9346 - lr = 0.00716154 \n",
            "257/390 [==================>...........] - ETA: 20s - loss: 1.3732 - acc: 0.6659 - LRFinder: val_loss: 3.8398 - lr = 0.00718256 \n",
            "258/390 [==================>...........] - ETA: 20s - loss: 1.3731 - acc: 0.6659 - LRFinder: val_loss: 3.9096 - lr = 0.00720359 \n",
            "259/390 [==================>...........] - ETA: 20s - loss: 1.3728 - acc: 0.6661 - LRFinder: val_loss: 3.7151 - lr = 0.00722462 \n",
            "260/390 [===================>..........] - ETA: 20s - loss: 1.3727 - acc: 0.6660 - LRFinder: val_loss: 3.9728 - lr = 0.00724564 \n",
            "261/390 [===================>..........] - ETA: 20s - loss: 1.3725 - acc: 0.6662 - LRFinder: val_loss: 3.8169 - lr = 0.00726667 \n",
            "262/390 [===================>..........] - ETA: 19s - loss: 1.3728 - acc: 0.6661 - LRFinder: val_loss: 3.9152 - lr = 0.00728769 \n",
            "263/390 [===================>..........] - ETA: 19s - loss: 1.3726 - acc: 0.6660 - LRFinder: val_loss: 4.0816 - lr = 0.00730872 \n",
            "264/390 [===================>..........] - ETA: 19s - loss: 1.3728 - acc: 0.6662 - LRFinder: val_loss: 4.0620 - lr = 0.00732974 \n",
            "265/390 [===================>..........] - ETA: 19s - loss: 1.3727 - acc: 0.6662 - LRFinder: val_loss: 4.1230 - lr = 0.00735077 \n",
            "266/390 [===================>..........] - ETA: 19s - loss: 1.3721 - acc: 0.6664 - LRFinder: val_loss: 4.4546 - lr = 0.00737179 \n",
            "267/390 [===================>..........] - ETA: 19s - loss: 1.3717 - acc: 0.6664 - LRFinder: val_loss: 4.3618 - lr = 0.00739282 \n",
            "268/390 [===================>..........] - ETA: 18s - loss: 1.3721 - acc: 0.6662 - LRFinder: val_loss: 4.4949 - lr = 0.00741385 \n",
            "269/390 [===================>..........] - ETA: 18s - loss: 1.3728 - acc: 0.6660 - LRFinder: val_loss: 4.5404 - lr = 0.00743487 \n",
            "270/390 [===================>..........] - ETA: 18s - loss: 1.3740 - acc: 0.6656 - LRFinder: val_loss: 4.2422 - lr = 0.00745590 \n",
            "271/390 [===================>..........] - ETA: 18s - loss: 1.3739 - acc: 0.6657 - LRFinder: val_loss: 3.9046 - lr = 0.00747692 \n",
            "272/390 [===================>..........] - ETA: 18s - loss: 1.3737 - acc: 0.6659 - LRFinder: val_loss: 4.3352 - lr = 0.00749795 \n",
            "273/390 [====================>.........] - ETA: 18s - loss: 1.3735 - acc: 0.6658 - LRFinder: val_loss: 4.1056 - lr = 0.00751897 \n",
            "274/390 [====================>.........] - ETA: 17s - loss: 1.3736 - acc: 0.6658 - LRFinder: val_loss: 4.1899 - lr = 0.00754000 \n",
            "275/390 [====================>.........] - ETA: 17s - loss: 1.3731 - acc: 0.6661 - LRFinder: val_loss: 4.3702 - lr = 0.00756103 \n",
            "276/390 [====================>.........] - ETA: 17s - loss: 1.3733 - acc: 0.6661 - LRFinder: val_loss: 4.3674 - lr = 0.00758205 \n",
            "277/390 [====================>.........] - ETA: 17s - loss: 1.3732 - acc: 0.6661 - LRFinder: val_loss: 4.3637 - lr = 0.00760308 \n",
            "278/390 [====================>.........] - ETA: 17s - loss: 1.3729 - acc: 0.6662 - LRFinder: val_loss: 4.2300 - lr = 0.00762410 \n",
            "279/390 [====================>.........] - ETA: 17s - loss: 1.3725 - acc: 0.6663 - LRFinder: val_loss: 4.4357 - lr = 0.00764513 \n",
            "280/390 [====================>.........] - ETA: 17s - loss: 1.3721 - acc: 0.6664 - LRFinder: val_loss: 4.2473 - lr = 0.00766615 \n",
            "281/390 [====================>.........] - ETA: 16s - loss: 1.3720 - acc: 0.6664 - LRFinder: val_loss: 4.1438 - lr = 0.00768718 \n",
            "282/390 [====================>.........] - ETA: 16s - loss: 1.3725 - acc: 0.6662 - LRFinder: val_loss: 4.2705 - lr = 0.00770821 \n",
            "283/390 [====================>.........] - ETA: 16s - loss: 1.3723 - acc: 0.6662 - LRFinder: val_loss: 4.1937 - lr = 0.00772923 \n",
            "284/390 [====================>.........] - ETA: 16s - loss: 1.3727 - acc: 0.6662 - LRFinder: val_loss: 3.8517 - lr = 0.00775026 \n",
            "285/390 [====================>.........] - ETA: 16s - loss: 1.3727 - acc: 0.6661 - LRFinder: val_loss: 3.9065 - lr = 0.00777128 \n",
            "286/390 [=====================>........] - ETA: 16s - loss: 1.3729 - acc: 0.6660 - LRFinder: val_loss: 3.9700 - lr = 0.00779231 \n",
            "287/390 [=====================>........] - ETA: 15s - loss: 1.3728 - acc: 0.6659 - LRFinder: val_loss: 3.5836 - lr = 0.00781333 \n",
            "288/390 [=====================>........] - ETA: 15s - loss: 1.3727 - acc: 0.6661 - LRFinder: val_loss: 3.8184 - lr = 0.00783436 \n",
            "289/390 [=====================>........] - ETA: 15s - loss: 1.3725 - acc: 0.6660 - LRFinder: val_loss: 3.8347 - lr = 0.00785538 \n",
            "290/390 [=====================>........] - ETA: 15s - loss: 1.3724 - acc: 0.6660 - LRFinder: val_loss: 3.7464 - lr = 0.00787641 \n",
            "291/390 [=====================>........] - ETA: 15s - loss: 1.3727 - acc: 0.6660 - LRFinder: val_loss: 3.4901 - lr = 0.00789744 \n",
            "292/390 [=====================>........] - ETA: 15s - loss: 1.3726 - acc: 0.6661 - LRFinder: val_loss: 3.8831 - lr = 0.00791846 \n",
            "293/390 [=====================>........] - ETA: 14s - loss: 1.3724 - acc: 0.6661 - LRFinder: val_loss: 3.6857 - lr = 0.00793949 \n",
            "294/390 [=====================>........] - ETA: 14s - loss: 1.3722 - acc: 0.6663 - LRFinder: val_loss: 3.7671 - lr = 0.00796051 \n",
            "295/390 [=====================>........] - ETA: 14s - loss: 1.3723 - acc: 0.6663 - LRFinder: val_loss: 3.9169 - lr = 0.00798154 \n",
            "296/390 [=====================>........] - ETA: 14s - loss: 1.3718 - acc: 0.6665 - LRFinder: val_loss: 3.9559 - lr = 0.00800256 \n",
            "297/390 [=====================>........] - ETA: 14s - loss: 1.3718 - acc: 0.6666 - LRFinder: val_loss: 3.9209 - lr = 0.00802359 \n",
            "298/390 [=====================>........] - ETA: 14s - loss: 1.3716 - acc: 0.6666 - LRFinder: val_loss: 3.9612 - lr = 0.00804462 \n",
            "299/390 [======================>.......] - ETA: 14s - loss: 1.3716 - acc: 0.6665 - LRFinder: val_loss: 3.8007 - lr = 0.00806564 \n",
            "300/390 [======================>.......] - ETA: 13s - loss: 1.3719 - acc: 0.6662 - LRFinder: val_loss: 3.8866 - lr = 0.00808667 \n",
            "301/390 [======================>.......] - ETA: 13s - loss: 1.3724 - acc: 0.6660 - LRFinder: val_loss: 3.7093 - lr = 0.00810769 \n",
            "302/390 [======================>.......] - ETA: 13s - loss: 1.3722 - acc: 0.6660 - LRFinder: val_loss: 3.8043 - lr = 0.00812872 \n",
            "303/390 [======================>.......] - ETA: 13s - loss: 1.3724 - acc: 0.6658 - LRFinder: val_loss: 3.7625 - lr = 0.00814974 \n",
            "304/390 [======================>.......] - ETA: 13s - loss: 1.3726 - acc: 0.6657 - LRFinder: val_loss: 3.7726 - lr = 0.00817077 \n",
            "305/390 [======================>.......] - ETA: 13s - loss: 1.3719 - acc: 0.6661 - LRFinder: val_loss: 3.9196 - lr = 0.00819179 \n",
            "306/390 [======================>.......] - ETA: 12s - loss: 1.3718 - acc: 0.6661 - LRFinder: val_loss: 3.9349 - lr = 0.00821282 \n",
            "307/390 [======================>.......] - ETA: 12s - loss: 1.3718 - acc: 0.6660 - LRFinder: val_loss: 3.9403 - lr = 0.00823385 \n",
            "308/390 [======================>.......] - ETA: 12s - loss: 1.3720 - acc: 0.6659 - LRFinder: val_loss: 3.9555 - lr = 0.00825487 \n",
            "309/390 [======================>.......] - ETA: 12s - loss: 1.3718 - acc: 0.6661 - LRFinder: val_loss: 3.7672 - lr = 0.00827590 \n",
            "310/390 [======================>.......] - ETA: 12s - loss: 1.3714 - acc: 0.6662 - LRFinder: val_loss: 3.8219 - lr = 0.00829692 \n",
            "311/390 [======================>.......] - ETA: 12s - loss: 1.3710 - acc: 0.6665 - LRFinder: val_loss: 3.7812 - lr = 0.00831795 \n",
            "312/390 [=======================>......] - ETA: 11s - loss: 1.3713 - acc: 0.6664 - LRFinder: val_loss: 3.7072 - lr = 0.00833897 \n",
            "313/390 [=======================>......] - ETA: 11s - loss: 1.3710 - acc: 0.6664 - LRFinder: val_loss: 3.7092 - lr = 0.00836000 \n",
            "314/390 [=======================>......] - ETA: 11s - loss: 1.3707 - acc: 0.6667 - LRFinder: val_loss: 3.9022 - lr = 0.00838103 \n",
            "315/390 [=======================>......] - ETA: 11s - loss: 1.3704 - acc: 0.6669 - LRFinder: val_loss: 3.8816 - lr = 0.00840205 \n",
            "316/390 [=======================>......] - ETA: 11s - loss: 1.3710 - acc: 0.6665 - LRFinder: val_loss: 4.0696 - lr = 0.00842308 \n",
            "317/390 [=======================>......] - ETA: 11s - loss: 1.3712 - acc: 0.6665 - LRFinder: val_loss: 3.9872 - lr = 0.00844410 \n",
            "318/390 [=======================>......] - ETA: 11s - loss: 1.3707 - acc: 0.6666 - LRFinder: val_loss: 4.0272 - lr = 0.00846513 \n",
            "319/390 [=======================>......] - ETA: 10s - loss: 1.3708 - acc: 0.6665 - LRFinder: val_loss: 3.9005 - lr = 0.00848615 \n",
            "320/390 [=======================>......] - ETA: 10s - loss: 1.3705 - acc: 0.6668 - LRFinder: val_loss: 4.1456 - lr = 0.00850718 \n",
            "321/390 [=======================>......] - ETA: 10s - loss: 1.3708 - acc: 0.6667 - LRFinder: val_loss: 3.9945 - lr = 0.00852821 \n",
            "322/390 [=======================>......] - ETA: 10s - loss: 1.3705 - acc: 0.6667 - LRFinder: val_loss: 4.2849 - lr = 0.00854923 \n",
            "323/390 [=======================>......] - ETA: 10s - loss: 1.3706 - acc: 0.6667 - LRFinder: val_loss: 4.2139 - lr = 0.00857026 \n",
            "324/390 [=======================>......] - ETA: 10s - loss: 1.3712 - acc: 0.6664 - LRFinder: val_loss: 4.2465 - lr = 0.00859128 \n",
            "325/390 [========================>.....] - ETA: 9s - loss: 1.3712 - acc: 0.6663  - LRFinder: val_loss: 4.0101 - lr = 0.00861231 \n",
            "326/390 [========================>.....] - ETA: 9s - loss: 1.3712 - acc: 0.6664 - LRFinder: val_loss: 4.0173 - lr = 0.00863333 \n",
            "327/390 [========================>.....] - ETA: 9s - loss: 1.3716 - acc: 0.6662 - LRFinder: val_loss: 4.1192 - lr = 0.00865436 \n",
            "328/390 [========================>.....] - ETA: 9s - loss: 1.3712 - acc: 0.6663 - LRFinder: val_loss: 3.9698 - lr = 0.00867538 \n",
            "329/390 [========================>.....] - ETA: 9s - loss: 1.3711 - acc: 0.6663 - LRFinder: val_loss: 3.9839 - lr = 0.00869641 \n",
            "330/390 [========================>.....] - ETA: 9s - loss: 1.3707 - acc: 0.6663 - LRFinder: val_loss: 4.2267 - lr = 0.00871744 \n",
            "331/390 [========================>.....] - ETA: 9s - loss: 1.3708 - acc: 0.6662 - LRFinder: val_loss: 3.9491 - lr = 0.00873846 \n",
            "332/390 [========================>.....] - ETA: 8s - loss: 1.3706 - acc: 0.6663 - LRFinder: val_loss: 4.1396 - lr = 0.00875949 \n",
            "333/390 [========================>.....] - ETA: 8s - loss: 1.3704 - acc: 0.6663 - LRFinder: val_loss: 4.2546 - lr = 0.00878051 \n",
            "334/390 [========================>.....] - ETA: 8s - loss: 1.3703 - acc: 0.6664 - LRFinder: val_loss: 3.9792 - lr = 0.00880154 \n",
            "335/390 [========================>.....] - ETA: 8s - loss: 1.3703 - acc: 0.6665 - LRFinder: val_loss: 3.8876 - lr = 0.00882256 \n",
            "336/390 [========================>.....] - ETA: 8s - loss: 1.3700 - acc: 0.6666 - LRFinder: val_loss: 3.8806 - lr = 0.00884359 \n",
            "337/390 [========================>.....] - ETA: 8s - loss: 1.3695 - acc: 0.6667 - LRFinder: val_loss: 3.9335 - lr = 0.00886462 \n",
            "338/390 [=========================>....] - ETA: 7s - loss: 1.3702 - acc: 0.6665 - LRFinder: val_loss: 4.0596 - lr = 0.00888564 \n",
            "339/390 [=========================>....] - ETA: 7s - loss: 1.3701 - acc: 0.6666 - LRFinder: val_loss: 3.8852 - lr = 0.00890667 \n",
            "340/390 [=========================>....] - ETA: 7s - loss: 1.3701 - acc: 0.6666 - LRFinder: val_loss: 3.8697 - lr = 0.00892769 \n",
            "341/390 [=========================>....] - ETA: 7s - loss: 1.3702 - acc: 0.6665 - LRFinder: val_loss: 3.9043 - lr = 0.00894872 \n",
            "342/390 [=========================>....] - ETA: 7s - loss: 1.3708 - acc: 0.6662 - LRFinder: val_loss: 3.9233 - lr = 0.00896974 \n",
            "343/390 [=========================>....] - ETA: 7s - loss: 1.3708 - acc: 0.6662 - LRFinder: val_loss: 4.0530 - lr = 0.00899077 \n",
            "344/390 [=========================>....] - ETA: 7s - loss: 1.3707 - acc: 0.6663 - LRFinder: val_loss: 3.7064 - lr = 0.00901179 \n",
            "345/390 [=========================>....] - ETA: 6s - loss: 1.3710 - acc: 0.6663 - LRFinder: val_loss: 3.9649 - lr = 0.00903282 \n",
            "346/390 [=========================>....] - ETA: 6s - loss: 1.3713 - acc: 0.6660 - LRFinder: val_loss: 4.0424 - lr = 0.00905385 \n",
            "347/390 [=========================>....] - ETA: 6s - loss: 1.3714 - acc: 0.6659 - LRFinder: val_loss: 3.9537 - lr = 0.00907487 \n",
            "348/390 [=========================>....] - ETA: 6s - loss: 1.3714 - acc: 0.6658 - LRFinder: val_loss: 3.9147 - lr = 0.00909590 \n",
            "349/390 [=========================>....] - ETA: 6s - loss: 1.3712 - acc: 0.6659 - LRFinder: val_loss: 3.9296 - lr = 0.00911692 \n",
            "350/390 [=========================>....] - ETA: 6s - loss: 1.3712 - acc: 0.6658 - LRFinder: val_loss: 4.0572 - lr = 0.00913795 \n",
            "351/390 [==========================>...] - ETA: 5s - loss: 1.3713 - acc: 0.6657 - LRFinder: val_loss: 4.0783 - lr = 0.00915897 \n",
            "352/390 [==========================>...] - ETA: 5s - loss: 1.3713 - acc: 0.6656 - LRFinder: val_loss: 4.1453 - lr = 0.00918000 \n",
            "353/390 [==========================>...] - ETA: 5s - loss: 1.3718 - acc: 0.6654 - LRFinder: val_loss: 3.9638 - lr = 0.00920103 \n",
            "354/390 [==========================>...] - ETA: 5s - loss: 1.3719 - acc: 0.6653 - LRFinder: val_loss: 4.1603 - lr = 0.00922205 \n",
            "355/390 [==========================>...] - ETA: 5s - loss: 1.3714 - acc: 0.6654 - LRFinder: val_loss: 4.0800 - lr = 0.00924308 \n",
            "356/390 [==========================>...] - ETA: 5s - loss: 1.3715 - acc: 0.6655 - LRFinder: val_loss: 4.0678 - lr = 0.00926410 \n",
            "357/390 [==========================>...] - ETA: 5s - loss: 1.3719 - acc: 0.6654 - LRFinder: val_loss: 3.9178 - lr = 0.00928513 \n",
            "358/390 [==========================>...] - ETA: 4s - loss: 1.3721 - acc: 0.6654 - LRFinder: val_loss: 3.8910 - lr = 0.00930615 \n",
            "359/390 [==========================>...] - ETA: 4s - loss: 1.3724 - acc: 0.6654 - LRFinder: val_loss: 3.8127 - lr = 0.00932718 \n",
            "360/390 [==========================>...] - ETA: 4s - loss: 1.3723 - acc: 0.6654 - LRFinder: val_loss: 3.7982 - lr = 0.00934821 \n",
            "361/390 [==========================>...] - ETA: 4s - loss: 1.3723 - acc: 0.6654 - LRFinder: val_loss: 3.7438 - lr = 0.00936923 \n",
            "362/390 [==========================>...] - ETA: 4s - loss: 1.3729 - acc: 0.6652 - LRFinder: val_loss: 3.6852 - lr = 0.00939026 \n",
            "363/390 [==========================>...] - ETA: 4s - loss: 1.3730 - acc: 0.6652 - LRFinder: val_loss: 3.7797 - lr = 0.00941128 \n",
            "364/390 [===========================>..] - ETA: 3s - loss: 1.3731 - acc: 0.6651 - LRFinder: val_loss: 3.7413 - lr = 0.00943231 \n",
            "365/390 [===========================>..] - ETA: 3s - loss: 1.3732 - acc: 0.6650 - LRFinder: val_loss: 3.9731 - lr = 0.00945333 \n",
            "366/390 [===========================>..] - ETA: 3s - loss: 1.3736 - acc: 0.6649 - LRFinder: val_loss: 3.9593 - lr = 0.00947436 \n",
            "367/390 [===========================>..] - ETA: 3s - loss: 1.3736 - acc: 0.6649 - LRFinder: val_loss: 4.0554 - lr = 0.00949538 \n",
            "368/390 [===========================>..] - ETA: 3s - loss: 1.3738 - acc: 0.6648 - LRFinder: val_loss: 4.2934 - lr = 0.00951641 \n",
            "369/390 [===========================>..] - ETA: 3s - loss: 1.3736 - acc: 0.6650 - LRFinder: val_loss: 4.0323 - lr = 0.00953744 \n",
            "370/390 [===========================>..] - ETA: 3s - loss: 1.3740 - acc: 0.6648 - LRFinder: val_loss: 4.1591 - lr = 0.00955846 \n",
            "371/390 [===========================>..] - ETA: 2s - loss: 1.3740 - acc: 0.6647 - LRFinder: val_loss: 4.2683 - lr = 0.00957949 \n",
            "372/390 [===========================>..] - ETA: 2s - loss: 1.3739 - acc: 0.6649 - LRFinder: val_loss: 4.2687 - lr = 0.00960051 \n",
            "373/390 [===========================>..] - ETA: 2s - loss: 1.3737 - acc: 0.6649 - LRFinder: val_loss: 4.3139 - lr = 0.00962154 \n",
            "374/390 [===========================>..] - ETA: 2s - loss: 1.3734 - acc: 0.6650 - LRFinder: val_loss: 4.2481 - lr = 0.00964256 \n",
            "375/390 [===========================>..] - ETA: 2s - loss: 1.3738 - acc: 0.6648 - LRFinder: val_loss: 4.2377 - lr = 0.00966359 \n",
            "376/390 [===========================>..] - ETA: 2s - loss: 1.3739 - acc: 0.6648 - LRFinder: val_loss: 4.5175 - lr = 0.00968462 \n",
            "377/390 [============================>.] - ETA: 1s - loss: 1.3738 - acc: 0.6648 - LRFinder: val_loss: 4.5464 - lr = 0.00970564 \n",
            "378/390 [============================>.] - ETA: 1s - loss: 1.3738 - acc: 0.6648 - LRFinder: val_loss: 4.6131 - lr = 0.00972667 \n",
            "379/390 [============================>.] - ETA: 1s - loss: 1.3735 - acc: 0.6649 - LRFinder: val_loss: 4.4185 - lr = 0.00974769 \n",
            "380/390 [============================>.] - ETA: 1s - loss: 1.3736 - acc: 0.6650 - LRFinder: val_loss: 4.5309 - lr = 0.00976872 \n",
            "381/390 [============================>.] - ETA: 1s - loss: 1.3740 - acc: 0.6648 - LRFinder: val_loss: 4.6529 - lr = 0.00978974 \n",
            "382/390 [============================>.] - ETA: 1s - loss: 1.3740 - acc: 0.6648 - LRFinder: val_loss: 4.3850 - lr = 0.00981077 \n",
            "383/390 [============================>.] - ETA: 1s - loss: 1.3739 - acc: 0.6648 - LRFinder: val_loss: 4.2818 - lr = 0.00983179 \n",
            "384/390 [============================>.] - ETA: 0s - loss: 1.3741 - acc: 0.6648 - LRFinder: val_loss: 4.2316 - lr = 0.00985282 \n",
            "385/390 [============================>.] - ETA: 0s - loss: 1.3740 - acc: 0.6647 - LRFinder: val_loss: 4.1578 - lr = 0.00987385 \n",
            "386/390 [============================>.] - ETA: 0s - loss: 1.3740 - acc: 0.6647 - LRFinder: val_loss: 4.0870 - lr = 0.00989487 \n",
            "387/390 [============================>.] - ETA: 0s - loss: 1.3740 - acc: 0.6646 - LRFinder: val_loss: 4.0528 - lr = 0.00991590 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 1.3742 - acc: 0.6645 - LRFinder: val_loss: 3.9420 - lr = 0.00993692 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.3743 - acc: 0.6645 - LRFinder: val_loss: 3.7500 - lr = 0.00995795 \n",
            " - LRFinder: val_loss: 3.8947 - lr = 0.00997897 \n",
            "390/390 [==============================] - 61s 156ms/step - loss: 1.3744 - acc: 0.6644 - val_loss: 3.8889 - val_acc: 0.1892\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {weights/weight_decay/weight_decay-3e-06/}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAESCAYAAAAR2wXeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NpJuZrplALd",
        "colab_type": "text"
      },
      "source": [
        "#BEST DECAY IS 0.0000003"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWaEbb0BlBK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = SGD(lr=0.0018, decay=0.000003, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EecCH-q0Ege3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specifying the path to store the weights\n",
        "filepath=\"/content/gdrive/My Drive/Assignment13_v3:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=True, mode='max')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSTRlAv4KUuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from clr import OneCycleLR\n",
        "\n",
        "lr_manager = OneCycleLR(epochs=50, batch_size=BATCH_SIZE, samples=n_train, steps=len(train_iterator), max_lr=0.0018,\n",
        "                        end_percentage=0.1, scale=100,\n",
        "                        maximum_momentum=0.95, minimum_momentum=0.9)\n",
        "\n",
        "callbacks = [checkpoint, lr_manager]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaDjaDz3szBx",
        "colab_type": "code",
        "outputId": "d53b1f40-ac2c-423f-d298-2ef30faa401a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "\n",
        "model_info = model.fit_generator(train_iterator,\n",
        "                                 steps_per_epoch = len(train_iterator), nb_epoch = EPOCHS, \n",
        "                                 validation_data = validation_iterator, \n",
        "                                 validation_steps = len(validation_iterator),\n",
        "                                 verbose=1, callbacks=callbacks)\n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n",
        "# plot model history\n",
        "plot_model_history(model_info)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=196, validation_data=<keras_pre..., validation_steps=20, verbose=1, callbacks=[<keras.ca..., epochs=300)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:651: DeprecationWarning: `wait_time` is not used anymore.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "  4/196 [..............................] - ETA: 3:06 - loss: 1.3731 - acc: 0.6602"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.276351). Check your callbacks.\n",
            "  % delta_t_median)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 26s 135ms/step - loss: 1.3439 - acc: 0.6781 - val_loss: 1.2506 - val_acc: 0.7103\n",
            " - lr: 0.00010 - momentum: 0.95 \n",
            "Epoch 2/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 1.3238 - acc: 0.6838 - val_loss: 1.2420 - val_acc: 0.7123\n",
            " - lr: 0.00018 - momentum: 0.95 \n",
            "Epoch 3/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.3126 - acc: 0.6866 - val_loss: 1.2360 - val_acc: 0.7163\n",
            " - lr: 0.00026 - momentum: 0.94 \n",
            "Epoch 4/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.3090 - acc: 0.6886 - val_loss: 1.2261 - val_acc: 0.7189\n",
            " - lr: 0.00033 - momentum: 0.94 \n",
            "Epoch 5/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.3060 - acc: 0.6907 - val_loss: 1.2291 - val_acc: 0.7182\n",
            " - lr: 0.00041 - momentum: 0.94 \n",
            "Epoch 6/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2995 - acc: 0.6930 - val_loss: 1.2250 - val_acc: 0.7198\n",
            " - lr: 0.00049 - momentum: 0.94 \n",
            "Epoch 7/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2933 - acc: 0.6936 - val_loss: 1.2174 - val_acc: 0.7235\n",
            " - lr: 0.00057 - momentum: 0.93 \n",
            "Epoch 8/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2942 - acc: 0.6949 - val_loss: 1.2149 - val_acc: 0.7211\n",
            " - lr: 0.00065 - momentum: 0.93 \n",
            "Epoch 9/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2878 - acc: 0.6973 - val_loss: 1.2124 - val_acc: 0.7259\n",
            " - lr: 0.00073 - momentum: 0.93 \n",
            "Epoch 10/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2868 - acc: 0.6964 - val_loss: 1.2089 - val_acc: 0.7249\n",
            " - lr: 0.00081 - momentum: 0.93 \n",
            "Epoch 11/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2787 - acc: 0.6984 - val_loss: 1.2086 - val_acc: 0.7300\n",
            " - lr: 0.00089 - momentum: 0.93 \n",
            "Epoch 12/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2732 - acc: 0.7010 - val_loss: 1.2002 - val_acc: 0.7272\n",
            " - lr: 0.00097 - momentum: 0.92 \n",
            "Epoch 13/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2711 - acc: 0.7002 - val_loss: 1.1912 - val_acc: 0.7323\n",
            " - lr: 0.00105 - momentum: 0.92 \n",
            "Epoch 14/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2666 - acc: 0.7027 - val_loss: 1.1917 - val_acc: 0.7293\n",
            " - lr: 0.00113 - momentum: 0.92 \n",
            "Epoch 15/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2628 - acc: 0.7025 - val_loss: 1.1922 - val_acc: 0.7314\n",
            " - lr: 0.00121 - momentum: 0.92 \n",
            "Epoch 16/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2555 - acc: 0.7064 - val_loss: 1.1866 - val_acc: 0.7349\n",
            " - lr: 0.00128 - momentum: 0.91 \n",
            "Epoch 17/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2564 - acc: 0.7054 - val_loss: 1.1808 - val_acc: 0.7337\n",
            " - lr: 0.00136 - momentum: 0.91 \n",
            "Epoch 18/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2452 - acc: 0.7098 - val_loss: 1.1803 - val_acc: 0.7326\n",
            " - lr: 0.00144 - momentum: 0.91 \n",
            "Epoch 19/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2391 - acc: 0.7109 - val_loss: 1.2268 - val_acc: 0.7205\n",
            " - lr: 0.00152 - momentum: 0.91 \n",
            "Epoch 20/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2350 - acc: 0.7109 - val_loss: 1.1835 - val_acc: 0.7314\n",
            " - lr: 0.00160 - momentum: 0.91 \n",
            "Epoch 21/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2295 - acc: 0.7118 - val_loss: 1.1908 - val_acc: 0.7320\n",
            " - lr: 0.00168 - momentum: 0.90 \n",
            "Epoch 22/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2239 - acc: 0.7150 - val_loss: 1.1654 - val_acc: 0.7411\n",
            " - lr: 0.00176 - momentum: 0.90 \n",
            "Epoch 23/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.6741 - acc: 0.5543 - val_loss: 3.7704 - val_acc: 0.2349\n",
            " - lr: 0.17604 - momentum: 0.90 \n",
            "Epoch 24/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.6158 - acc: 0.5649 - val_loss: 2.2412 - val_acc: 0.3834\n",
            " - lr: 0.16804 - momentum: 0.90 \n",
            "Epoch 25/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.3565 - acc: 0.6497 - val_loss: 1.3176 - val_acc: 0.6603\n",
            " - lr: 0.16004 - momentum: 0.91 \n",
            "Epoch 26/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.2116 - acc: 0.6921 - val_loss: 1.3831 - val_acc: 0.6249\n",
            " - lr: 0.15204 - momentum: 0.91 \n",
            "Epoch 27/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.0937 - acc: 0.7274 - val_loss: 1.2227 - val_acc: 0.6904\n",
            " - lr: 0.14404 - momentum: 0.91 \n",
            "Epoch 28/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 1.0011 - acc: 0.7555 - val_loss: 1.6036 - val_acc: 0.6201\n",
            " - lr: 0.13605 - momentum: 0.91 \n",
            "Epoch 29/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.9375 - acc: 0.7720 - val_loss: 1.2701 - val_acc: 0.6749\n",
            " - lr: 0.12805 - momentum: 0.91 \n",
            "Epoch 30/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.8747 - acc: 0.7902 - val_loss: 1.1820 - val_acc: 0.7099\n",
            " - lr: 0.12005 - momentum: 0.92 \n",
            "Epoch 31/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.8268 - acc: 0.8040 - val_loss: 1.1174 - val_acc: 0.7141\n",
            " - lr: 0.11205 - momentum: 0.92 \n",
            "Epoch 32/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.7863 - acc: 0.8146 - val_loss: 1.3628 - val_acc: 0.6612\n",
            " - lr: 0.10405 - momentum: 0.92 \n",
            "Epoch 33/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.7476 - acc: 0.8258 - val_loss: 0.9040 - val_acc: 0.7671\n",
            " - lr: 0.09605 - momentum: 0.92 \n",
            "Epoch 34/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.7137 - acc: 0.8330 - val_loss: 1.0482 - val_acc: 0.7241\n",
            " - lr: 0.08805 - momentum: 0.93 \n",
            "Epoch 35/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.6849 - acc: 0.8411 - val_loss: 0.8036 - val_acc: 0.8098\n",
            " - lr: 0.08005 - momentum: 0.93 \n",
            "Epoch 36/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.6554 - acc: 0.8500 - val_loss: 0.8511 - val_acc: 0.8047\n",
            " - lr: 0.07205 - momentum: 0.93 \n",
            "Epoch 37/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.6340 - acc: 0.8579 - val_loss: 0.8498 - val_acc: 0.7943\n",
            " - lr: 0.06405 - momentum: 0.93 \n",
            "Epoch 38/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.6080 - acc: 0.8630 - val_loss: 0.9474 - val_acc: 0.7665\n",
            " - lr: 0.05605 - momentum: 0.93 \n",
            "Epoch 39/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5785 - acc: 0.8712 - val_loss: 0.9406 - val_acc: 0.7815\n",
            " - lr: 0.04805 - momentum: 0.94 \n",
            "Epoch 40/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5539 - acc: 0.8767 - val_loss: 0.6386 - val_acc: 0.8561\n",
            " - lr: 0.04005 - momentum: 0.94 \n",
            "Epoch 41/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5284 - acc: 0.8850 - val_loss: 0.6900 - val_acc: 0.8367\n",
            " - lr: 0.03206 - momentum: 0.94 \n",
            "Epoch 42/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5064 - acc: 0.8899 - val_loss: 0.5836 - val_acc: 0.8753\n",
            " - lr: 0.02406 - momentum: 0.94 \n",
            "Epoch 43/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4717 - acc: 0.9026 - val_loss: 0.5672 - val_acc: 0.8765\n",
            " - lr: 0.01606 - momentum: 0.95 \n",
            "Epoch 44/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4381 - acc: 0.9140 - val_loss: 0.4969 - val_acc: 0.8984\n",
            " - lr: 0.00806 - momentum: 0.95 \n",
            "Epoch 45/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4103 - acc: 0.9226 - val_loss: 0.4617 - val_acc: 0.9085\n",
            " - lr: 0.00006 - momentum: 0.95 \n",
            "Epoch 46/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3927 - acc: 0.9292 - val_loss: 0.4598 - val_acc: 0.9103\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 47/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3924 - acc: 0.9284 - val_loss: 0.4594 - val_acc: 0.9103\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 48/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3955 - acc: 0.9291 - val_loss: 0.4593 - val_acc: 0.9099\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 49/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3932 - acc: 0.9288 - val_loss: 0.4593 - val_acc: 0.9095\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 50/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3966 - acc: 0.9275 - val_loss: 0.4594 - val_acc: 0.9099\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 51/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3929 - acc: 0.9287 - val_loss: 0.4587 - val_acc: 0.9101\n",
            " - lr: 0.00010 - momentum: 0.95 \n",
            "Epoch 52/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3922 - acc: 0.9280 - val_loss: 0.4577 - val_acc: 0.9102\n",
            " - lr: 0.00018 - momentum: 0.95 \n",
            "Epoch 53/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3924 - acc: 0.9283 - val_loss: 0.4572 - val_acc: 0.9104\n",
            " - lr: 0.00026 - momentum: 0.94 \n",
            "Epoch 54/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3881 - acc: 0.9290 - val_loss: 0.4552 - val_acc: 0.9123\n",
            " - lr: 0.00033 - momentum: 0.94 \n",
            "Epoch 55/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3909 - acc: 0.9290 - val_loss: 0.4546 - val_acc: 0.9119\n",
            " - lr: 0.00041 - momentum: 0.94 \n",
            "Epoch 56/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3895 - acc: 0.9297 - val_loss: 0.4545 - val_acc: 0.9113\n",
            " - lr: 0.00049 - momentum: 0.94 \n",
            "Epoch 57/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3844 - acc: 0.9305 - val_loss: 0.4543 - val_acc: 0.9118\n",
            " - lr: 0.00057 - momentum: 0.93 \n",
            "Epoch 58/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3851 - acc: 0.9309 - val_loss: 0.4543 - val_acc: 0.9115\n",
            " - lr: 0.00065 - momentum: 0.93 \n",
            "Epoch 59/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3856 - acc: 0.9309 - val_loss: 0.4535 - val_acc: 0.9120\n",
            " - lr: 0.00073 - momentum: 0.93 \n",
            "Epoch 60/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3876 - acc: 0.9302 - val_loss: 0.4538 - val_acc: 0.9128\n",
            " - lr: 0.00081 - momentum: 0.93 \n",
            "Epoch 61/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3800 - acc: 0.9325 - val_loss: 0.4534 - val_acc: 0.9116\n",
            " - lr: 0.00089 - momentum: 0.93 \n",
            "Epoch 62/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3799 - acc: 0.9318 - val_loss: 0.4529 - val_acc: 0.9127\n",
            " - lr: 0.00097 - momentum: 0.92 \n",
            "Epoch 63/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3836 - acc: 0.9310 - val_loss: 0.4524 - val_acc: 0.9123\n",
            " - lr: 0.00105 - momentum: 0.92 \n",
            "Epoch 64/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3814 - acc: 0.9326 - val_loss: 0.4519 - val_acc: 0.9130\n",
            " - lr: 0.00113 - momentum: 0.92 \n",
            "Epoch 65/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3834 - acc: 0.9310 - val_loss: 0.4523 - val_acc: 0.9115\n",
            " - lr: 0.00121 - momentum: 0.92 \n",
            "Epoch 66/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3761 - acc: 0.9340 - val_loss: 0.4513 - val_acc: 0.9132\n",
            " - lr: 0.00128 - momentum: 0.91 \n",
            "Epoch 67/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3795 - acc: 0.9324 - val_loss: 0.4528 - val_acc: 0.9137\n",
            " - lr: 0.00136 - momentum: 0.91 \n",
            "Epoch 68/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3766 - acc: 0.9343 - val_loss: 0.4498 - val_acc: 0.9139\n",
            " - lr: 0.00144 - momentum: 0.91 \n",
            "Epoch 69/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3775 - acc: 0.9327 - val_loss: 0.4500 - val_acc: 0.9132\n",
            " - lr: 0.00152 - momentum: 0.91 \n",
            "Epoch 70/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3724 - acc: 0.9346 - val_loss: 0.4500 - val_acc: 0.9146\n",
            " - lr: 0.00160 - momentum: 0.91 \n",
            "Epoch 71/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3695 - acc: 0.9345 - val_loss: 0.4491 - val_acc: 0.9135\n",
            " - lr: 0.00168 - momentum: 0.90 \n",
            "Epoch 72/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3721 - acc: 0.9333 - val_loss: 0.4509 - val_acc: 0.9123\n",
            " - lr: 0.00176 - momentum: 0.90 \n",
            "Epoch 73/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5168 - acc: 0.8849 - val_loss: 1.3603 - val_acc: 0.7043\n",
            " - lr: 0.17604 - momentum: 0.90 \n",
            "Epoch 74/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.6546 - acc: 0.8430 - val_loss: 0.9924 - val_acc: 0.7627\n",
            " - lr: 0.16804 - momentum: 0.90 \n",
            "Epoch 75/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.6409 - acc: 0.8507 - val_loss: 0.8341 - val_acc: 0.8050\n",
            " - lr: 0.16004 - momentum: 0.91 \n",
            "Epoch 76/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.6188 - acc: 0.8587 - val_loss: 0.7512 - val_acc: 0.8273\n",
            " - lr: 0.15204 - momentum: 0.91 \n",
            "Epoch 77/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.6096 - acc: 0.8627 - val_loss: 0.7246 - val_acc: 0.8311\n",
            " - lr: 0.14404 - momentum: 0.91 \n",
            "Epoch 78/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5931 - acc: 0.8677 - val_loss: 0.9025 - val_acc: 0.7864\n",
            " - lr: 0.13605 - momentum: 0.91 \n",
            "Epoch 79/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5866 - acc: 0.8705 - val_loss: 0.9211 - val_acc: 0.7786\n",
            " - lr: 0.12805 - momentum: 0.91 \n",
            "Epoch 80/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5680 - acc: 0.8752 - val_loss: 0.6622 - val_acc: 0.8544\n",
            " - lr: 0.12005 - momentum: 0.92 \n",
            "Epoch 81/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5505 - acc: 0.8812 - val_loss: 0.7600 - val_acc: 0.8217\n",
            " - lr: 0.11205 - momentum: 0.92 \n",
            "Epoch 82/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5400 - acc: 0.8854 - val_loss: 0.8137 - val_acc: 0.8074\n",
            " - lr: 0.10405 - momentum: 0.92 \n",
            "Epoch 83/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5268 - acc: 0.8909 - val_loss: 0.8438 - val_acc: 0.8038\n",
            " - lr: 0.09605 - momentum: 0.92 \n",
            "Epoch 84/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5146 - acc: 0.8912 - val_loss: 0.9730 - val_acc: 0.7867\n",
            " - lr: 0.08805 - momentum: 0.93 \n",
            "Epoch 85/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5007 - acc: 0.8982 - val_loss: 0.6923 - val_acc: 0.8409\n",
            " - lr: 0.08005 - momentum: 0.93 \n",
            "Epoch 86/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4887 - acc: 0.8997 - val_loss: 0.5992 - val_acc: 0.8689\n",
            " - lr: 0.07205 - momentum: 0.93 \n",
            "Epoch 87/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4708 - acc: 0.9055 - val_loss: 0.7709 - val_acc: 0.8262\n",
            " - lr: 0.06405 - momentum: 0.93 \n",
            "Epoch 88/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4555 - acc: 0.9091 - val_loss: 0.7275 - val_acc: 0.8313\n",
            " - lr: 0.05605 - momentum: 0.93 \n",
            "Epoch 89/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4357 - acc: 0.9159 - val_loss: 0.5890 - val_acc: 0.8738\n",
            " - lr: 0.04805 - momentum: 0.94 \n",
            "Epoch 90/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4196 - acc: 0.9199 - val_loss: 0.7644 - val_acc: 0.8331\n",
            " - lr: 0.04005 - momentum: 0.94 \n",
            "Epoch 91/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3998 - acc: 0.9258 - val_loss: 0.4689 - val_acc: 0.9106\n",
            " - lr: 0.03206 - momentum: 0.94 \n",
            "Epoch 92/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3740 - acc: 0.9346 - val_loss: 0.4698 - val_acc: 0.9079\n",
            " - lr: 0.02406 - momentum: 0.94 \n",
            "Epoch 93/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3506 - acc: 0.9419 - val_loss: 0.4600 - val_acc: 0.9129\n",
            " - lr: 0.01606 - momentum: 0.95 \n",
            "Epoch 94/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3286 - acc: 0.9489 - val_loss: 0.4410 - val_acc: 0.9210\n",
            " - lr: 0.00806 - momentum: 0.95 \n",
            "Epoch 95/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3082 - acc: 0.9554 - val_loss: 0.4159 - val_acc: 0.9251\n",
            " - lr: 0.00006 - momentum: 0.95 \n",
            "Epoch 96/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2961 - acc: 0.9602 - val_loss: 0.4140 - val_acc: 0.9277\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 97/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2962 - acc: 0.9595 - val_loss: 0.4142 - val_acc: 0.9274\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 98/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3009 - acc: 0.9572 - val_loss: 0.4143 - val_acc: 0.9274\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 99/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2980 - acc: 0.9587 - val_loss: 0.4142 - val_acc: 0.9273\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 100/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2971 - acc: 0.9592 - val_loss: 0.4141 - val_acc: 0.9273\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 101/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2974 - acc: 0.9592 - val_loss: 0.4136 - val_acc: 0.9276\n",
            " - lr: 0.00010 - momentum: 0.95 \n",
            "Epoch 102/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2972 - acc: 0.9597 - val_loss: 0.4131 - val_acc: 0.9276\n",
            " - lr: 0.00018 - momentum: 0.95 \n",
            "Epoch 103/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2965 - acc: 0.9597 - val_loss: 0.4124 - val_acc: 0.9281\n",
            " - lr: 0.00026 - momentum: 0.94 \n",
            "Epoch 104/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2941 - acc: 0.9601 - val_loss: 0.4119 - val_acc: 0.9285\n",
            " - lr: 0.00033 - momentum: 0.94 \n",
            "Epoch 105/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2936 - acc: 0.9603 - val_loss: 0.4114 - val_acc: 0.9286\n",
            " - lr: 0.00041 - momentum: 0.94 \n",
            "Epoch 106/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2910 - acc: 0.9626 - val_loss: 0.4112 - val_acc: 0.9283\n",
            " - lr: 0.00049 - momentum: 0.94 \n",
            "Epoch 107/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2939 - acc: 0.9599 - val_loss: 0.4111 - val_acc: 0.9289\n",
            " - lr: 0.00057 - momentum: 0.93 \n",
            "Epoch 108/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2904 - acc: 0.9610 - val_loss: 0.4108 - val_acc: 0.9292\n",
            " - lr: 0.00065 - momentum: 0.93 \n",
            "Epoch 109/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.2912 - acc: 0.9610 - val_loss: 0.4105 - val_acc: 0.9295\n",
            " - lr: 0.00073 - momentum: 0.93 \n",
            "Epoch 110/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2874 - acc: 0.9626 - val_loss: 0.4107 - val_acc: 0.9286\n",
            " - lr: 0.00081 - momentum: 0.93 \n",
            "Epoch 111/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2894 - acc: 0.9614 - val_loss: 0.4116 - val_acc: 0.9295\n",
            " - lr: 0.00089 - momentum: 0.93 \n",
            "Epoch 112/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2869 - acc: 0.9632 - val_loss: 0.4103 - val_acc: 0.9287\n",
            " - lr: 0.00097 - momentum: 0.92 \n",
            "Epoch 113/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2854 - acc: 0.9629 - val_loss: 0.4105 - val_acc: 0.9289\n",
            " - lr: 0.00105 - momentum: 0.92 \n",
            "Epoch 114/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2853 - acc: 0.9624 - val_loss: 0.4102 - val_acc: 0.9286\n",
            " - lr: 0.00113 - momentum: 0.92 \n",
            "Epoch 115/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2847 - acc: 0.9629 - val_loss: 0.4114 - val_acc: 0.9290\n",
            " - lr: 0.00121 - momentum: 0.92 \n",
            "Epoch 116/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2861 - acc: 0.9630 - val_loss: 0.4118 - val_acc: 0.9284\n",
            " - lr: 0.00128 - momentum: 0.91 \n",
            "Epoch 117/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2824 - acc: 0.9640 - val_loss: 0.4102 - val_acc: 0.9300\n",
            " - lr: 0.00136 - momentum: 0.91 \n",
            "Epoch 118/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2818 - acc: 0.9647 - val_loss: 0.4108 - val_acc: 0.9288\n",
            " - lr: 0.00144 - momentum: 0.91 \n",
            "Epoch 119/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2806 - acc: 0.9643 - val_loss: 0.4104 - val_acc: 0.9292\n",
            " - lr: 0.00152 - momentum: 0.91 \n",
            "Epoch 120/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2814 - acc: 0.9637 - val_loss: 0.4101 - val_acc: 0.9289\n",
            " - lr: 0.00160 - momentum: 0.91 \n",
            "Epoch 121/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2792 - acc: 0.9652 - val_loss: 0.4094 - val_acc: 0.9299\n",
            " - lr: 0.00168 - momentum: 0.90 \n",
            "Epoch 122/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2803 - acc: 0.9642 - val_loss: 0.4111 - val_acc: 0.9291\n",
            " - lr: 0.00176 - momentum: 0.90 \n",
            "Epoch 123/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4213 - acc: 0.9160 - val_loss: 1.7787 - val_acc: 0.6802\n",
            " - lr: 0.17604 - momentum: 0.90 \n",
            "Epoch 124/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5674 - acc: 0.8733 - val_loss: 0.7763 - val_acc: 0.8226\n",
            " - lr: 0.16804 - momentum: 0.90 \n",
            "Epoch 125/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5531 - acc: 0.8811 - val_loss: 0.6882 - val_acc: 0.8496\n",
            " - lr: 0.16004 - momentum: 0.91 \n",
            "Epoch 126/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5440 - acc: 0.8866 - val_loss: 0.7907 - val_acc: 0.8215\n",
            " - lr: 0.15204 - momentum: 0.91 \n",
            "Epoch 127/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5325 - acc: 0.8909 - val_loss: 0.7326 - val_acc: 0.8428\n",
            " - lr: 0.14404 - momentum: 0.91 \n",
            "Epoch 128/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5297 - acc: 0.8916 - val_loss: 0.8797 - val_acc: 0.8076\n",
            " - lr: 0.13605 - momentum: 0.91 \n",
            "Epoch 129/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5195 - acc: 0.8976 - val_loss: 0.6815 - val_acc: 0.8561\n",
            " - lr: 0.12805 - momentum: 0.91 \n",
            "Epoch 130/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5119 - acc: 0.8992 - val_loss: 0.7723 - val_acc: 0.8306\n",
            " - lr: 0.12005 - momentum: 0.92 \n",
            "Epoch 131/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5031 - acc: 0.9028 - val_loss: 0.8204 - val_acc: 0.8165\n",
            " - lr: 0.11205 - momentum: 0.92 \n",
            "Epoch 132/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4931 - acc: 0.9062 - val_loss: 0.6424 - val_acc: 0.8667\n",
            " - lr: 0.10405 - momentum: 0.92 \n",
            "Epoch 133/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4786 - acc: 0.9109 - val_loss: 0.7257 - val_acc: 0.8401\n",
            " - lr: 0.09605 - momentum: 0.92 \n",
            "Epoch 134/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4715 - acc: 0.9127 - val_loss: 1.1540 - val_acc: 0.7670\n",
            " - lr: 0.08805 - momentum: 0.93 \n",
            "Epoch 135/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4583 - acc: 0.9157 - val_loss: 0.7925 - val_acc: 0.8283\n",
            " - lr: 0.08005 - momentum: 0.93 \n",
            "Epoch 136/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4415 - acc: 0.9211 - val_loss: 0.5943 - val_acc: 0.8798\n",
            " - lr: 0.07205 - momentum: 0.93 \n",
            "Epoch 137/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4273 - acc: 0.9254 - val_loss: 0.7736 - val_acc: 0.8416\n",
            " - lr: 0.06405 - momentum: 0.93 \n",
            "Epoch 138/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4154 - acc: 0.9281 - val_loss: 0.9921 - val_acc: 0.7947\n",
            " - lr: 0.05605 - momentum: 0.93 \n",
            "Epoch 139/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3993 - acc: 0.9333 - val_loss: 0.5829 - val_acc: 0.8820\n",
            " - lr: 0.04805 - momentum: 0.94 \n",
            "Epoch 140/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3822 - acc: 0.9386 - val_loss: 0.5875 - val_acc: 0.8884\n",
            " - lr: 0.04005 - momentum: 0.94 \n",
            "Epoch 141/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3599 - acc: 0.9450 - val_loss: 0.4726 - val_acc: 0.9163\n",
            " - lr: 0.03206 - momentum: 0.94 \n",
            "Epoch 142/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3454 - acc: 0.9484 - val_loss: 0.5108 - val_acc: 0.9083\n",
            " - lr: 0.02406 - momentum: 0.94 \n",
            "Epoch 143/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3231 - acc: 0.9544 - val_loss: 0.4533 - val_acc: 0.9233\n",
            " - lr: 0.01606 - momentum: 0.95 \n",
            "Epoch 144/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2975 - acc: 0.9621 - val_loss: 0.4316 - val_acc: 0.9256\n",
            " - lr: 0.00806 - momentum: 0.95 \n",
            "Epoch 145/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2844 - acc: 0.9662 - val_loss: 0.4163 - val_acc: 0.9336\n",
            " - lr: 0.00006 - momentum: 0.95 \n",
            "Epoch 146/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2744 - acc: 0.9709 - val_loss: 0.4156 - val_acc: 0.9343\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 147/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2732 - acc: 0.9716 - val_loss: 0.4158 - val_acc: 0.9342\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 148/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2714 - acc: 0.9715 - val_loss: 0.4157 - val_acc: 0.9341\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 149/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2734 - acc: 0.9714 - val_loss: 0.4158 - val_acc: 0.9342\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 150/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2719 - acc: 0.9714 - val_loss: 0.4156 - val_acc: 0.9341\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 151/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2736 - acc: 0.9716 - val_loss: 0.4150 - val_acc: 0.9342\n",
            " - lr: 0.00010 - momentum: 0.95 \n",
            "Epoch 152/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2731 - acc: 0.9710 - val_loss: 0.4139 - val_acc: 0.9344\n",
            " - lr: 0.00018 - momentum: 0.95 \n",
            "Epoch 153/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2722 - acc: 0.9712 - val_loss: 0.4131 - val_acc: 0.9346\n",
            " - lr: 0.00026 - momentum: 0.94 \n",
            "Epoch 154/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2727 - acc: 0.9715 - val_loss: 0.4120 - val_acc: 0.9354\n",
            " - lr: 0.00033 - momentum: 0.94 \n",
            "Epoch 155/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2714 - acc: 0.9713 - val_loss: 0.4117 - val_acc: 0.9359\n",
            " - lr: 0.00041 - momentum: 0.94 \n",
            "Epoch 156/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2685 - acc: 0.9725 - val_loss: 0.4108 - val_acc: 0.9357\n",
            " - lr: 0.00049 - momentum: 0.94 \n",
            "Epoch 157/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2681 - acc: 0.9728 - val_loss: 0.4101 - val_acc: 0.9356\n",
            " - lr: 0.00057 - momentum: 0.93 \n",
            "Epoch 158/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2684 - acc: 0.9731 - val_loss: 0.4101 - val_acc: 0.9359\n",
            " - lr: 0.00065 - momentum: 0.93 \n",
            "Epoch 159/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2686 - acc: 0.9722 - val_loss: 0.4094 - val_acc: 0.9361\n",
            " - lr: 0.00073 - momentum: 0.93 \n",
            "Epoch 160/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2676 - acc: 0.9733 - val_loss: 0.4084 - val_acc: 0.9362\n",
            " - lr: 0.00081 - momentum: 0.93 \n",
            "Epoch 161/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2684 - acc: 0.9729 - val_loss: 0.4094 - val_acc: 0.9353\n",
            " - lr: 0.00089 - momentum: 0.93 \n",
            "Epoch 162/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2647 - acc: 0.9734 - val_loss: 0.4086 - val_acc: 0.9366\n",
            " - lr: 0.00097 - momentum: 0.92 \n",
            "Epoch 163/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2637 - acc: 0.9745 - val_loss: 0.4079 - val_acc: 0.9361\n",
            " - lr: 0.00105 - momentum: 0.92 \n",
            "Epoch 164/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2647 - acc: 0.9742 - val_loss: 0.4080 - val_acc: 0.9365\n",
            " - lr: 0.00113 - momentum: 0.92 \n",
            "Epoch 165/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2639 - acc: 0.9736 - val_loss: 0.4079 - val_acc: 0.9364\n",
            " - lr: 0.00121 - momentum: 0.92 \n",
            "Epoch 166/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2627 - acc: 0.9735 - val_loss: 0.4073 - val_acc: 0.9363\n",
            " - lr: 0.00128 - momentum: 0.91 \n",
            "Epoch 167/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2602 - acc: 0.9748 - val_loss: 0.4079 - val_acc: 0.9356\n",
            " - lr: 0.00136 - momentum: 0.91 \n",
            "Epoch 168/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2628 - acc: 0.9740 - val_loss: 0.4063 - val_acc: 0.9362\n",
            " - lr: 0.00144 - momentum: 0.91 \n",
            "Epoch 169/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2615 - acc: 0.9749 - val_loss: 0.4076 - val_acc: 0.9372\n",
            " - lr: 0.00152 - momentum: 0.91 \n",
            "Epoch 170/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2589 - acc: 0.9758 - val_loss: 0.4069 - val_acc: 0.9372\n",
            " - lr: 0.00160 - momentum: 0.91 \n",
            "Epoch 171/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2621 - acc: 0.9734 - val_loss: 0.4083 - val_acc: 0.9366\n",
            " - lr: 0.00168 - momentum: 0.90 \n",
            "Epoch 172/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2588 - acc: 0.9752 - val_loss: 0.4090 - val_acc: 0.9376\n",
            " - lr: 0.00176 - momentum: 0.90 \n",
            "Epoch 173/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3879 - acc: 0.9306 - val_loss: 1.6586 - val_acc: 0.6893\n",
            " - lr: 0.17604 - momentum: 0.90 \n",
            "Epoch 174/300\n",
            "196/196 [==============================] - 22s 111ms/step - loss: 0.5395 - acc: 0.8876 - val_loss: 1.1401 - val_acc: 0.7457\n",
            " - lr: 0.16804 - momentum: 0.90 \n",
            "Epoch 175/300\n",
            "196/196 [==============================] - 22s 110ms/step - loss: 0.5293 - acc: 0.8948 - val_loss: 1.0191 - val_acc: 0.7591\n",
            " - lr: 0.16004 - momentum: 0.91 \n",
            "Epoch 176/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5165 - acc: 0.9011 - val_loss: 0.7131 - val_acc: 0.8513\n",
            " - lr: 0.15204 - momentum: 0.91 \n",
            "Epoch 177/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5107 - acc: 0.9025 - val_loss: 0.8840 - val_acc: 0.8041\n",
            " - lr: 0.14404 - momentum: 0.91 \n",
            "Epoch 178/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.4987 - acc: 0.9096 - val_loss: 0.9510 - val_acc: 0.7923\n",
            " - lr: 0.13605 - momentum: 0.91 \n",
            "Epoch 179/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4976 - acc: 0.9079 - val_loss: 0.6664 - val_acc: 0.8665\n",
            " - lr: 0.12805 - momentum: 0.91 \n",
            "Epoch 180/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4882 - acc: 0.9121 - val_loss: 0.6064 - val_acc: 0.8797\n",
            " - lr: 0.12005 - momentum: 0.92 \n",
            "Epoch 181/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4831 - acc: 0.9136 - val_loss: 0.6502 - val_acc: 0.8700\n",
            " - lr: 0.11205 - momentum: 0.92 \n",
            "Epoch 182/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4658 - acc: 0.9199 - val_loss: 0.7540 - val_acc: 0.8464\n",
            " - lr: 0.10405 - momentum: 0.92 \n",
            "Epoch 183/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.4559 - acc: 0.9219 - val_loss: 0.6687 - val_acc: 0.8637\n",
            " - lr: 0.09605 - momentum: 0.92 \n",
            "Epoch 184/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4431 - acc: 0.9257 - val_loss: 0.6689 - val_acc: 0.8724\n",
            " - lr: 0.08805 - momentum: 0.93 \n",
            "Epoch 185/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4406 - acc: 0.9260 - val_loss: 0.6561 - val_acc: 0.8683\n",
            " - lr: 0.08005 - momentum: 0.93 \n",
            "Epoch 186/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4213 - acc: 0.9314 - val_loss: 0.6436 - val_acc: 0.8796\n",
            " - lr: 0.07205 - momentum: 0.93 \n",
            "Epoch 187/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.4102 - acc: 0.9334 - val_loss: 0.5636 - val_acc: 0.8945\n",
            " - lr: 0.06405 - momentum: 0.93 \n",
            "Epoch 188/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3911 - acc: 0.9400 - val_loss: 0.5360 - val_acc: 0.8984\n",
            " - lr: 0.05605 - momentum: 0.93 \n",
            "Epoch 189/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3723 - acc: 0.9436 - val_loss: 0.5290 - val_acc: 0.9080\n",
            " - lr: 0.04805 - momentum: 0.94 \n",
            "Epoch 190/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3595 - acc: 0.9473 - val_loss: 0.5469 - val_acc: 0.9008\n",
            " - lr: 0.04005 - momentum: 0.94 \n",
            "Epoch 191/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3421 - acc: 0.9525 - val_loss: 0.5134 - val_acc: 0.9038\n",
            " - lr: 0.03206 - momentum: 0.94 \n",
            "Epoch 192/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3225 - acc: 0.9584 - val_loss: 0.4694 - val_acc: 0.9192\n",
            " - lr: 0.02406 - momentum: 0.94 \n",
            "Epoch 193/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3010 - acc: 0.9645 - val_loss: 0.4449 - val_acc: 0.9261\n",
            " - lr: 0.01606 - momentum: 0.95 \n",
            "Epoch 194/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2829 - acc: 0.9698 - val_loss: 0.4293 - val_acc: 0.9330\n",
            " - lr: 0.00806 - momentum: 0.95 \n",
            "Epoch 195/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2680 - acc: 0.9744 - val_loss: 0.4103 - val_acc: 0.9343\n",
            " - lr: 0.00006 - momentum: 0.95 \n",
            "Epoch 196/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2620 - acc: 0.9769 - val_loss: 0.4108 - val_acc: 0.9351\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 197/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2620 - acc: 0.9766 - val_loss: 0.4112 - val_acc: 0.9353\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 198/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2614 - acc: 0.9772 - val_loss: 0.4112 - val_acc: 0.9355\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 199/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2613 - acc: 0.9775 - val_loss: 0.4111 - val_acc: 0.9355\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 200/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2631 - acc: 0.9763 - val_loss: 0.4111 - val_acc: 0.9356\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 201/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2635 - acc: 0.9765 - val_loss: 0.4111 - val_acc: 0.9354\n",
            " - lr: 0.00010 - momentum: 0.95 \n",
            "Epoch 202/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.2615 - acc: 0.9760 - val_loss: 0.4112 - val_acc: 0.9355\n",
            " - lr: 0.00018 - momentum: 0.95 \n",
            "Epoch 203/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.2610 - acc: 0.9770 - val_loss: 0.4105 - val_acc: 0.9357\n",
            " - lr: 0.00026 - momentum: 0.94 \n",
            "Epoch 204/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2618 - acc: 0.9760 - val_loss: 0.4104 - val_acc: 0.9357\n",
            " - lr: 0.00033 - momentum: 0.94 \n",
            "Epoch 205/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2582 - acc: 0.9776 - val_loss: 0.4095 - val_acc: 0.9360\n",
            " - lr: 0.00041 - momentum: 0.94 \n",
            "Epoch 206/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2607 - acc: 0.9769 - val_loss: 0.4082 - val_acc: 0.9362\n",
            " - lr: 0.00049 - momentum: 0.94 \n",
            "Epoch 207/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.2578 - acc: 0.9776 - val_loss: 0.4080 - val_acc: 0.9369\n",
            " - lr: 0.00057 - momentum: 0.93 \n",
            "Epoch 208/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2583 - acc: 0.9778 - val_loss: 0.4072 - val_acc: 0.9369\n",
            " - lr: 0.00065 - momentum: 0.93 \n",
            "Epoch 209/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2580 - acc: 0.9775 - val_loss: 0.4068 - val_acc: 0.9370\n",
            " - lr: 0.00073 - momentum: 0.93 \n",
            "Epoch 210/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2564 - acc: 0.9787 - val_loss: 0.4061 - val_acc: 0.9372\n",
            " - lr: 0.00081 - momentum: 0.93 \n",
            "Epoch 211/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2559 - acc: 0.9786 - val_loss: 0.4062 - val_acc: 0.9373\n",
            " - lr: 0.00089 - momentum: 0.93 \n",
            "Epoch 212/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.2552 - acc: 0.9784 - val_loss: 0.4048 - val_acc: 0.9375\n",
            " - lr: 0.00097 - momentum: 0.92 \n",
            "Epoch 213/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2547 - acc: 0.9785 - val_loss: 0.4042 - val_acc: 0.9383\n",
            " - lr: 0.00105 - momentum: 0.92 \n",
            "Epoch 214/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2556 - acc: 0.9787 - val_loss: 0.4045 - val_acc: 0.9375\n",
            " - lr: 0.00113 - momentum: 0.92 \n",
            "Epoch 215/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2523 - acc: 0.9798 - val_loss: 0.4037 - val_acc: 0.9381\n",
            " - lr: 0.00121 - momentum: 0.92 \n",
            "Epoch 216/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2526 - acc: 0.9792 - val_loss: 0.4040 - val_acc: 0.9387\n",
            " - lr: 0.00128 - momentum: 0.91 \n",
            "Epoch 217/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2532 - acc: 0.9786 - val_loss: 0.4037 - val_acc: 0.9382\n",
            " - lr: 0.00136 - momentum: 0.91 \n",
            "Epoch 218/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2507 - acc: 0.9797 - val_loss: 0.4039 - val_acc: 0.9386\n",
            " - lr: 0.00144 - momentum: 0.91 \n",
            "Epoch 219/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2500 - acc: 0.9802 - val_loss: 0.4029 - val_acc: 0.9394\n",
            " - lr: 0.00152 - momentum: 0.91 \n",
            "Epoch 220/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2524 - acc: 0.9790 - val_loss: 0.4017 - val_acc: 0.9390\n",
            " - lr: 0.00160 - momentum: 0.91 \n",
            "Epoch 221/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2498 - acc: 0.9795 - val_loss: 0.4014 - val_acc: 0.9397\n",
            " - lr: 0.00168 - momentum: 0.90 \n",
            "Epoch 222/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.2490 - acc: 0.9803 - val_loss: 0.4000 - val_acc: 0.9406\n",
            " - lr: 0.00176 - momentum: 0.90 \n",
            "Epoch 223/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.3757 - acc: 0.9386 - val_loss: 1.6753 - val_acc: 0.7036\n",
            " - lr: 0.17604 - momentum: 0.90 \n",
            "Epoch 224/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5204 - acc: 0.8939 - val_loss: 0.8521 - val_acc: 0.8183\n",
            " - lr: 0.16804 - momentum: 0.90 \n",
            "Epoch 225/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5071 - acc: 0.9039 - val_loss: 1.0212 - val_acc: 0.7804\n",
            " - lr: 0.16004 - momentum: 0.91 \n",
            "Epoch 226/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5034 - acc: 0.9070 - val_loss: 1.0563 - val_acc: 0.7796\n",
            " - lr: 0.15204 - momentum: 0.91 \n",
            "Epoch 227/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.4893 - acc: 0.9126 - val_loss: 1.1295 - val_acc: 0.7554\n",
            " - lr: 0.14404 - momentum: 0.91 \n",
            "Epoch 228/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.4908 - acc: 0.9147 - val_loss: 0.9861 - val_acc: 0.7949\n",
            " - lr: 0.13605 - momentum: 0.91 \n",
            "Epoch 229/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4760 - acc: 0.9190 - val_loss: 0.8065 - val_acc: 0.8424\n",
            " - lr: 0.12805 - momentum: 0.91 \n",
            "Epoch 230/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4705 - acc: 0.9204 - val_loss: 0.6201 - val_acc: 0.8795\n",
            " - lr: 0.12005 - momentum: 0.92 \n",
            "Epoch 231/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.4665 - acc: 0.9209 - val_loss: 0.7345 - val_acc: 0.8573\n",
            " - lr: 0.11205 - momentum: 0.92 \n",
            "Epoch 232/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4523 - acc: 0.9254 - val_loss: 0.8341 - val_acc: 0.8461\n",
            " - lr: 0.10405 - momentum: 0.92 \n",
            "Epoch 233/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4469 - acc: 0.9286 - val_loss: 0.6204 - val_acc: 0.8849\n",
            " - lr: 0.09605 - momentum: 0.92 \n",
            "Epoch 234/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4330 - acc: 0.9298 - val_loss: 0.6450 - val_acc: 0.8728\n",
            " - lr: 0.08805 - momentum: 0.93 \n",
            "Epoch 235/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.4185 - acc: 0.9352 - val_loss: 0.6139 - val_acc: 0.8859\n",
            " - lr: 0.08005 - momentum: 0.93 \n",
            "Epoch 236/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.4107 - acc: 0.9373 - val_loss: 0.5968 - val_acc: 0.8916\n",
            " - lr: 0.07205 - momentum: 0.93 \n",
            "Epoch 237/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3967 - acc: 0.9394 - val_loss: 0.6353 - val_acc: 0.8776\n",
            " - lr: 0.06405 - momentum: 0.93 \n",
            "Epoch 238/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.3814 - acc: 0.9445 - val_loss: 0.5205 - val_acc: 0.9083\n",
            " - lr: 0.05605 - momentum: 0.93 \n",
            "Epoch 239/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3627 - acc: 0.9494 - val_loss: 0.5600 - val_acc: 0.8969\n",
            " - lr: 0.04805 - momentum: 0.94 \n",
            "Epoch 240/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3480 - acc: 0.9543 - val_loss: 0.4907 - val_acc: 0.9208\n",
            " - lr: 0.04005 - momentum: 0.94 \n",
            "Epoch 241/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3296 - acc: 0.9581 - val_loss: 0.4858 - val_acc: 0.9177\n",
            " - lr: 0.03206 - momentum: 0.94 \n",
            "Epoch 242/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.3110 - acc: 0.9636 - val_loss: 0.4537 - val_acc: 0.9257\n",
            " - lr: 0.02406 - momentum: 0.94 \n",
            "Epoch 243/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2901 - acc: 0.9697 - val_loss: 0.4536 - val_acc: 0.9286\n",
            " - lr: 0.01606 - momentum: 0.95 \n",
            "Epoch 244/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2750 - acc: 0.9738 - val_loss: 0.4201 - val_acc: 0.9369\n",
            " - lr: 0.00806 - momentum: 0.95 \n",
            "Epoch 245/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.2605 - acc: 0.9788 - val_loss: 0.4131 - val_acc: 0.9397\n",
            " - lr: 0.00006 - momentum: 0.95 \n",
            "Epoch 246/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.2561 - acc: 0.9799 - val_loss: 0.4144 - val_acc: 0.9402\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 247/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2587 - acc: 0.9793 - val_loss: 0.4146 - val_acc: 0.9399\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 248/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.2574 - acc: 0.9792 - val_loss: 0.4143 - val_acc: 0.9398\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 249/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2554 - acc: 0.9802 - val_loss: 0.4143 - val_acc: 0.9400\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 250/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.2556 - acc: 0.9804 - val_loss: 0.4145 - val_acc: 0.9398\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 251/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2570 - acc: 0.9797 - val_loss: 0.4141 - val_acc: 0.9398\n",
            " - lr: 0.00010 - momentum: 0.95 \n",
            "Epoch 252/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.2554 - acc: 0.9804 - val_loss: 0.4136 - val_acc: 0.9398\n",
            " - lr: 0.00018 - momentum: 0.95 \n",
            "Epoch 253/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2544 - acc: 0.9805 - val_loss: 0.4128 - val_acc: 0.9398\n",
            " - lr: 0.00026 - momentum: 0.94 \n",
            "Epoch 254/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2549 - acc: 0.9804 - val_loss: 0.4119 - val_acc: 0.9402\n",
            " - lr: 0.00033 - momentum: 0.94 \n",
            "Epoch 255/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2542 - acc: 0.9809 - val_loss: 0.4119 - val_acc: 0.9402\n",
            " - lr: 0.00041 - momentum: 0.94 \n",
            "Epoch 256/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.2525 - acc: 0.9811 - val_loss: 0.4107 - val_acc: 0.9408\n",
            " - lr: 0.00049 - momentum: 0.94 \n",
            "Epoch 257/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2523 - acc: 0.9810 - val_loss: 0.4105 - val_acc: 0.9403\n",
            " - lr: 0.00057 - momentum: 0.93 \n",
            "Epoch 258/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2525 - acc: 0.9813 - val_loss: 0.4098 - val_acc: 0.9408\n",
            " - lr: 0.00065 - momentum: 0.93 \n",
            "Epoch 259/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2517 - acc: 0.9814 - val_loss: 0.4089 - val_acc: 0.9407\n",
            " - lr: 0.00073 - momentum: 0.93 \n",
            "Epoch 260/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.2529 - acc: 0.9812 - val_loss: 0.4092 - val_acc: 0.9406\n",
            " - lr: 0.00081 - momentum: 0.93 \n",
            "Epoch 261/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.2516 - acc: 0.9813 - val_loss: 0.4089 - val_acc: 0.9406\n",
            " - lr: 0.00089 - momentum: 0.93 \n",
            "Epoch 262/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2513 - acc: 0.9816 - val_loss: 0.4086 - val_acc: 0.9401\n",
            " - lr: 0.00097 - momentum: 0.92 \n",
            "Epoch 263/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2508 - acc: 0.9818 - val_loss: 0.4079 - val_acc: 0.9401\n",
            " - lr: 0.00105 - momentum: 0.92 \n",
            "Epoch 264/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2499 - acc: 0.9815 - val_loss: 0.4079 - val_acc: 0.9406\n",
            " - lr: 0.00113 - momentum: 0.92 \n",
            "Epoch 265/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2481 - acc: 0.9816 - val_loss: 0.4076 - val_acc: 0.9411\n",
            " - lr: 0.00121 - momentum: 0.92 \n",
            "Epoch 266/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2486 - acc: 0.9825 - val_loss: 0.4072 - val_acc: 0.9415\n",
            " - lr: 0.00128 - momentum: 0.91 \n",
            "Epoch 267/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2476 - acc: 0.9833 - val_loss: 0.4064 - val_acc: 0.9411\n",
            " - lr: 0.00136 - momentum: 0.91 \n",
            "Epoch 268/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2490 - acc: 0.9823 - val_loss: 0.4068 - val_acc: 0.9403\n",
            " - lr: 0.00144 - momentum: 0.91 \n",
            "Epoch 269/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2481 - acc: 0.9818 - val_loss: 0.4061 - val_acc: 0.9409\n",
            " - lr: 0.00152 - momentum: 0.91 \n",
            "Epoch 270/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2470 - acc: 0.9822 - val_loss: 0.4053 - val_acc: 0.9419\n",
            " - lr: 0.00160 - momentum: 0.91 \n",
            "Epoch 271/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2457 - acc: 0.9832 - val_loss: 0.4049 - val_acc: 0.9414\n",
            " - lr: 0.00168 - momentum: 0.90 \n",
            "Epoch 272/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2466 - acc: 0.9825 - val_loss: 0.4044 - val_acc: 0.9422\n",
            " - lr: 0.00176 - momentum: 0.90 \n",
            "Epoch 273/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3634 - acc: 0.9435 - val_loss: 4.1281 - val_acc: 0.4610\n",
            " - lr: 0.17604 - momentum: 0.90 \n",
            "Epoch 274/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.5152 - acc: 0.8992 - val_loss: 0.7269 - val_acc: 0.8613\n",
            " - lr: 0.16804 - momentum: 0.90 \n",
            "Epoch 275/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4940 - acc: 0.9107 - val_loss: 0.6956 - val_acc: 0.8618\n",
            " - lr: 0.16004 - momentum: 0.91 \n",
            "Epoch 276/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.4882 - acc: 0.9143 - val_loss: 0.6439 - val_acc: 0.8755\n",
            " - lr: 0.15204 - momentum: 0.91 \n",
            "Epoch 277/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4821 - acc: 0.9172 - val_loss: 0.8479 - val_acc: 0.8233\n",
            " - lr: 0.14404 - momentum: 0.91 \n",
            "Epoch 278/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4726 - acc: 0.9207 - val_loss: 0.6647 - val_acc: 0.8723\n",
            " - lr: 0.13605 - momentum: 0.91 \n",
            "Epoch 279/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4696 - acc: 0.9212 - val_loss: 0.7376 - val_acc: 0.8496\n",
            " - lr: 0.12805 - momentum: 0.91 \n",
            "Epoch 280/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4630 - acc: 0.9244 - val_loss: 0.7696 - val_acc: 0.8538\n",
            " - lr: 0.12005 - momentum: 0.92 \n",
            "Epoch 281/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4524 - acc: 0.9285 - val_loss: 0.6683 - val_acc: 0.8727\n",
            " - lr: 0.11205 - momentum: 0.92 \n",
            "Epoch 282/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4395 - acc: 0.9321 - val_loss: 0.8252 - val_acc: 0.8311\n",
            " - lr: 0.10405 - momentum: 0.92 \n",
            "Epoch 283/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4320 - acc: 0.9336 - val_loss: 0.6234 - val_acc: 0.8817\n",
            " - lr: 0.09605 - momentum: 0.92 \n",
            "Epoch 284/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4238 - acc: 0.9346 - val_loss: 0.6619 - val_acc: 0.8769\n",
            " - lr: 0.08805 - momentum: 0.93 \n",
            "Epoch 285/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.4084 - acc: 0.9405 - val_loss: 0.6975 - val_acc: 0.8697\n",
            " - lr: 0.08005 - momentum: 0.93 \n",
            "Epoch 286/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3963 - acc: 0.9427 - val_loss: 0.5694 - val_acc: 0.8948\n",
            " - lr: 0.07205 - momentum: 0.93 \n",
            "Epoch 287/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3850 - acc: 0.9463 - val_loss: 0.5827 - val_acc: 0.8962\n",
            " - lr: 0.06405 - momentum: 0.93 \n",
            "Epoch 288/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3670 - acc: 0.9500 - val_loss: 0.5242 - val_acc: 0.9121\n",
            " - lr: 0.05605 - momentum: 0.93 \n",
            "Epoch 289/300\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 0.3541 - acc: 0.9535 - val_loss: 0.5024 - val_acc: 0.9147\n",
            " - lr: 0.04805 - momentum: 0.94 \n",
            "Epoch 290/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3362 - acc: 0.9586 - val_loss: 0.5268 - val_acc: 0.9056\n",
            " - lr: 0.04005 - momentum: 0.94 \n",
            "Epoch 291/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3214 - acc: 0.9614 - val_loss: 0.4839 - val_acc: 0.9175\n",
            " - lr: 0.03206 - momentum: 0.94 \n",
            "Epoch 292/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.3023 - acc: 0.9673 - val_loss: 0.4655 - val_acc: 0.9256\n",
            " - lr: 0.02406 - momentum: 0.94 \n",
            "Epoch 293/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2852 - acc: 0.9718 - val_loss: 0.4289 - val_acc: 0.9338\n",
            " - lr: 0.01606 - momentum: 0.95 \n",
            "Epoch 294/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2694 - acc: 0.9761 - val_loss: 0.4053 - val_acc: 0.9404\n",
            " - lr: 0.00806 - momentum: 0.95 \n",
            "Epoch 295/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2548 - acc: 0.9810 - val_loss: 0.4029 - val_acc: 0.9407\n",
            " - lr: 0.00006 - momentum: 0.95 \n",
            "Epoch 296/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2505 - acc: 0.9826 - val_loss: 0.4041 - val_acc: 0.9404\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 297/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2511 - acc: 0.9827 - val_loss: 0.4042 - val_acc: 0.9404\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 298/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2519 - acc: 0.9820 - val_loss: 0.4041 - val_acc: 0.9403\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 299/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2505 - acc: 0.9832 - val_loss: 0.4038 - val_acc: 0.9404\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 300/300\n",
            "196/196 [==============================] - 21s 108ms/step - loss: 0.2513 - acc: 0.9828 - val_loss: 0.4041 - val_acc: 0.9405\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Model took 6376.06 seconds to train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3IAAAFICAYAAAAVqcwDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgwIhCP8pe5_",
        "colab_type": "code",
        "outputId": "6bf94ffd-5394-4b87-9378-df06254e8803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# compute test accuracy\n",
        "result = model.evaluate_generator(validation_iterator, steps = len(validation_iterator))\n",
        "print(result)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:651: DeprecationWarning: `wait_time` is not used anymore.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.40174073143005373, 0.9427999999046326]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoGeKh9ny4Y9",
        "colab_type": "text"
      },
      "source": [
        "#Reached 94% val accuracy at\n",
        "\n",
        "Epoch **246**/300\n",
        "196/196 [==============================] - 21s 109ms/step - loss: 0.2561 - acc: 0.9799 - val_loss: 0.4144 - val_acc: **0.9402**\n",
        " - lr: 0.00001 - momentum: 0.95\n",
        "\n",
        "#Reached 90% val accuracy at\n",
        "Epoch **45**/300\n",
        "196/196 [==============================] - 21s 108ms/step - loss: 0.4103 - acc: 0.9226 - val_loss: 0.4617 - val_acc: **0.9085**\n",
        " - lr: 0.00006 - momentum: 0.95"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBKGgGObKeaf",
        "colab_type": "code",
        "outputId": "67c4754c-b4f2-450e-d38c-04971500bd65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "# Plot Learning Rate\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(lr_manager.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAESCAYAAAABl4lHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttq_t-r8KfE6",
        "colab_type": "code",
        "outputId": "96f6d3a4-27ff-4d1d-93dc-c3a6ec490d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "# Plot momentum\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Momentum')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(lr_manager.history['momentum'])\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAESCAYAAAAWtRmOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}