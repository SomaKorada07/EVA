{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA_Assignment13_v3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOPOrH3Bhw-Y",
        "colab_type": "code",
        "outputId": "37473b75-70d3-4399-8ced-b4aec25c4444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Importing NumPy (Numerical Python) which is a library consisting of multidimensional array objects and a collection of routines for processing those arrays. It also gives an alias to the library.\n",
        "import numpy as np\n",
        "\n",
        "% matplotlib inline\n",
        "np.random.seed(2017)\n",
        "\n",
        "from keras import regularizers\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Activation, Flatten, Dropout, GlobalAveragePooling2D, Input, Dense\n",
        "from keras.layers.merge import add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import Callback\n",
        "# Importing the callbacks of Keras.\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.datasets import cifar10\n",
        "from keras import backend as K"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCDoIVT9OOH4",
        "colab_type": "code",
        "outputId": "f36139ca-7095-4744-a10f-1635930d89e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Mounting the Google Drive to save the weights\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5mkJO53h-Zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128 #@param [\"512\", \"256\", \"128\"] {type:\"raw\"}\n",
        "MOMENTUM = 0.9 #@param [\"0.9\", \"0.95\", \"0.975\"] {type:\"raw\"}\n",
        "WEIGHT_DECAY = 5e-4 #@param [\"0.000125\", \"0.00025\", \"0.0005\", \"5e-4\"] {type:\"raw\"}\n",
        "LEARNING_RATE = 0.1 #@param [\"0.4\", \"0.2\", \"0.1\"] {type:\"raw\"}\n",
        "EPOCHS = 300 #@param {type:\"slider\", min:0, max:300, step:1}\n",
        "WARMUP = 5 #@param {type:\"slider\", min:0, max:24, step:1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WR3LO8tidAc",
        "colab_type": "code",
        "outputId": "073ab41c-8390-4006-f3a5-c5d2e5eb934f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# CIFAR10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. Importing CIFAR10 dataset from Keras.\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "# Loading the CIFAR10 60000 Training and 10000 Test data into respective numpy arrays\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "n_train, n_test = X_train.shape[0], X_test.shape[0]\n",
        "img_size = X_train.shape[1]\n",
        "n_classes = y_train.max() + 1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJq-7uMpmU6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')/255\n",
        "X_test = X_test.astype('float32')/255\n",
        "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2smHikv8OChz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7S_Eg5lODYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(validation_iterator, test_y, model):\n",
        "    result = model.evaluate_generator(validation_iterator, steps = len(validation_iterator))\n",
        "    predicted_class = np.argmax(result, axis=1)\n",
        "    true_class = np.argmax(test_y, axis=1)\n",
        "    num_correct = np.sum(predicted_class == true_class) \n",
        "    accuracy = float(num_correct)/result.shape[0]\n",
        "    return (accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMpsqn_O0CvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_examples(X,y,classes):\n",
        "    rows = int(np.ceil(len(X)/5))\n",
        "    if X.shape[1] > 64:\n",
        "        multiplier = 2\n",
        "    else:\n",
        "        multiplier = 1\n",
        "    fig = plt.figure(figsize=(10*multiplier, rows*2*multiplier))\n",
        "    for idx in np.arange(len(X)):\n",
        "        img = X[idx]\n",
        "        assert (len(img.shape)==3 and img.shape[2] in [1,3,4]) or len(img.shape)==2\n",
        "        ax = fig.add_subplot(rows, 5, idx + 1, xticks=[], yticks=[])\n",
        "        cmap = None\n",
        "        if (len(img.shape)==3 and img.shape[2]==1) or len(img.shape)==2:\n",
        "            cmap=\"binary\"\n",
        "        if len(img.shape)==3 and img.shape[2]==1:\n",
        "            img = img.reshape((img.shape[0],img.shape[1]))\n",
        "        ax.imshow(img,cmap=cmap)\n",
        "        ax.set_title(classes[np.argmax(y[idx])])\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFJt6i8e0IzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cifar10_labels():\n",
        "    return ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMBWETcC0wi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def min_max_scale(X):\n",
        "  return (X - np.min(X))/(np.max(X)-np.min(X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDK_7gDlHEFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cutout_eraser_and_padcrop(p=0.5, s_l=0.05, s_h=0.3, r_1=0.3, r_2=1 / 0.3, max_erasures_per_image=1, pixel_level=True, random_crop_size=(32, 32), padding_pixels=4):\n",
        "    \"\"\"\n",
        "    :param p:\n",
        "    :param s_l: Minimum Area Proportion of Original that may be cut\n",
        "    :param s_h: Maximum Area Proportion of Original that may be cut\n",
        "    :param r_1: Min Aspect Ratio\n",
        "    :param r_2: Max Aspect Ratio\n",
        "    :param max_erasures_per_image:\n",
        "    :param pixel_level:\n",
        "    :return: Eraser to be used as Preprocessing Function\n",
        "    \"\"\"\n",
        "    assert max_erasures_per_image >= 1\n",
        "\n",
        "    def eraser(input_img):\n",
        "        v_l = np.min(input_img)\n",
        "        v_h = np.max(input_img)\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        mx = np.random.randint(1, max_erasures_per_image + 1)\n",
        "        # print(\"Erasures = \",mx,end =\", \")\n",
        "        for i in range(mx):\n",
        "            while True:\n",
        "                s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "                r = np.random.uniform(r_1, r_2)\n",
        "                w = int(np.sqrt(s / r))\n",
        "                h = int(np.sqrt(s * r))\n",
        "                left = np.random.randint(0, img_w)\n",
        "                top = np.random.randint(0, img_h)\n",
        "\n",
        "                if left + w <= img_w and top + h <= img_h:\n",
        "                    break\n",
        "\n",
        "            # print(\"W = \",w,\"H = \",h,end =\", \")\n",
        "\n",
        "            if pixel_level:\n",
        "                # print(np.max(img_c),np.min(img_c),v_l,v_h)\n",
        "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "                # print(c.shape,np.min(c),np.max(c),np.median(c))\n",
        "            else:\n",
        "                c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "            input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        # print()\n",
        "        return input_img\n",
        "     \n",
        "    def random_crop(input_image):\n",
        "          # Note: image_data_format is 'channel_last'\n",
        "          assert input_image.shape[2] == 3\n",
        "          \n",
        "          #Pad by 4 pixels\n",
        "          img = cv2.copyMakeBorder(input_image, padding_pixels, padding_pixels, padding_pixels, padding_pixels, cv2.BORDER_REPLICATE)\n",
        "          \n",
        "          height, width = img.shape[0], img.shape[1]\n",
        "          dy, dx = random_crop_size\n",
        "          x = np.random.randint(0, width - dx + 1)\n",
        "          y = np.random.randint(0, height - dy + 1)\n",
        "          return img[y:(y+dy), x:(x+dx), :]\n",
        "        \n",
        "    def preproc_image(input_image):\n",
        "      #return eraser\n",
        "      return eraser(random_crop(input_image))\n",
        "\n",
        "    return preproc_image\n",
        "      \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBPdtlO75o2O",
        "colab_type": "code",
        "outputId": "a0965e1d-8f43-47b9-a88a-e5cd6c5e8f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "   featurewise_center=True,\n",
        "   featurewise_std_normalization=True,\n",
        "   horizontal_flip=True,                 # randomly flip images                                     \n",
        "   preprocessing_function=get_cutout_eraser_and_padcrop(p=0.75, s_l=0.05, s_h=0.2, r_1=0.3, r_2=1 / 0.3, max_erasures_per_image=1, pixel_level=True))\n",
        "\n",
        "_ = datagen.fit(X_train)\n",
        "train_iterator = datagen.flow(X_train, Y_train, batch_size=100,shuffle=False)\n",
        "\n",
        "X_e, Y_e = train_iterator.next()\n",
        "X_e = min_max_scale(X_e)\n",
        "show_examples(X_e[0:10], Y_e[0:10], classes = get_cifar10_labels())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAAD4CAYAAADvq+IEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 720x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo_GNPlGV_Bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen_validation = ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True,)\n",
        "datagen.mean = np.array([0.4914, 0.4822, 0.4465], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "datagen.std = np.array([0.2023, 0.1994, 0.2010], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "datagen.fit(X_train)\n",
        "datagen_validation.fit(X_train)\n",
        "train_iterator = datagen.flow(X_train, Y_train, batch_size=256,shuffle=True)\n",
        "validation_iterator = datagen_validation.flow(X_test, Y_test, batch_size=512,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oETUKntCiqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _conv_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLbFteMhCj1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
        "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        for i in range(repetitions):\n",
        "            init_strides = (1, 1)\n",
        "            if i == 0 and not is_first_layer:\n",
        "                init_strides = (2, 2)\n",
        "            input = block_function(filters=filters, init_strides=init_strides,\n",
        "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
        "        return input\n",
        "\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgobaLQNDEJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
        "                           strides=init_strides,\n",
        "                           padding=\"same\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
        "                                  strides=init_strides)(input)\n",
        "\n",
        "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvmD6gNhHy65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _bn_relu_conv(**conv_params):\n",
        "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
        "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(activation)\n",
        "\n",
        "    return f\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqPiKSNRH-8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _shortcut(input, residual):\n",
        "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "    input_shape = K.int_shape(input)\n",
        "    residual_shape = K.int_shape(residual)\n",
        "    stride_width = int(round(input_shape[1] / residual_shape[1]))\n",
        "    stride_height = int(round(input_shape[2] / residual_shape[2]))\n",
        "    equal_channels = input_shape[3] == residual_shape[3]\n",
        "\n",
        "    shortcut = input\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
        "        shortcut = Conv2D(filters=residual_shape[3],\n",
        "                          kernel_size=(1, 1),\n",
        "                          strides=(stride_width, stride_height),\n",
        "                          padding=\"valid\",\n",
        "                          kernel_initializer=\"he_normal\",\n",
        "                          kernel_regularizer=l2(0.0001))(input)\n",
        "\n",
        "    return add([shortcut, residual])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvUCI7nrDNKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN -> relu block\n",
        "    \"\"\"\n",
        "    norm = BatchNormalization(axis=3)(input)\n",
        "    return Activation(\"relu\")(norm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10HGAQbDDg6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape=(32, 32, 3)\n",
        "num_outputs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMTngMZGDN_T",
        "colab_type": "code",
        "outputId": "4a56a4c0-f999-4e6a-a2d4-b40a674cacb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "#Define ResNet18 model\n",
        "input = Input(shape=input_shape)\n",
        "conv1 = _conv_bn_relu(filters=32, kernel_size=(3, 3), strides=(1, 1))(input)\n",
        "#pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
        "\n",
        "block = conv1\n",
        "\n",
        "#B1\n",
        "filters = 32\n",
        "block = _residual_block(basic_block, filters=filters, repetitions=2, is_first_layer=True)(block)\n",
        "\n",
        "#B2\n",
        "filters = 64\n",
        "block = _residual_block(basic_block, filters=filters, repetitions=2, is_first_layer=False)(block)\n",
        "\n",
        "#B3\n",
        "filters = 128\n",
        "block = _residual_block(basic_block, filters=filters, repetitions=2, is_first_layer=False)(block)\n",
        "\n",
        "#B4\n",
        "filters = 256\n",
        "block = _residual_block(basic_block, filters=filters, repetitions=2, is_first_layer=False)(block)\n",
        "\n",
        "# Last activation\n",
        "block = _bn_relu(block)\n",
        "\n",
        "# Classifier block\n",
        "pool = GlobalAveragePooling2D()(block)\n",
        "dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n",
        "              activation=\"softmax\")(pool)\n",
        "\n",
        "model = Model(inputs=input, outputs=dense)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0802 23:18:14.763872 140655597586304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0802 23:18:14.809560 140655597586304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0802 23:18:14.818019 140655597586304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0802 23:18:14.877016 140655597586304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0802 23:18:14.878256 140655597586304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0802 23:18:17.637888 140655597586304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRKvgcZrFgIj",
        "colab_type": "code",
        "outputId": "47e80fa2-b12a-4c6b-e433-efedc24a7019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 32)   896         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 32)   9248        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 32)   9248        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 32)   0           activation_1[0][0]               \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 32)   9248        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 32)   0           add_1[0][0]                      \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 64)   18496       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 64)   2112        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 64)   36928       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 64)   0           conv2d_8[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 64)   36928       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 64)   36928       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 64)   0           add_3[0][0]                      \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 8, 8, 128)    73856       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 8, 8, 128)    512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 8, 8, 128)    0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 128)    8320        add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 8, 8, 128)    147584      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 128)    0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 8, 8, 128)    512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 8, 8, 128)    0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 128)    147584      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 8, 8, 128)    512         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 8, 8, 128)    0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 128)    147584      activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 128)    0           add_5[0][0]                      \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 128)    512         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 8, 8, 128)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 256)    295168      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 4, 4, 256)    1024        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 4, 4, 256)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 4, 4, 256)    33024       add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 4, 4, 256)    590080      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 4, 4, 256)    0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 256)    1024        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 256)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 4, 4, 256)    590080      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 4, 4, 256)    1024        conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 256)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 4, 4, 256)    590080      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 4, 4, 256)    0           add_7[0][0]                      \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 4, 4, 256)    1024        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 4, 4, 256)    0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 256)          0           activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           2570        global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 2,803,018\n",
            "Trainable params: 2,799,114\n",
            "Non-trainable params: 3,904\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4DmCxcNgEq0",
        "colab_type": "code",
        "outputId": "beeae918-3ef8-4e66-fe0a-92a18543aea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/SomaKorada07/EIP/master/utils/clr.py\""
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-02 23:18:21--  https://raw.githubusercontent.com/SomaKorada07/EIP/master/utils/clr.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7928 (7.7K) [text/plain]\n",
            "Saving to: ‘clr.py’\n",
            "\n",
            "\rclr.py                0%[                    ]       0  --.-KB/s               \rclr.py              100%[===================>]   7.74K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-08-02 23:18:21 (58.5 MB/s) - ‘clr.py’ saved [7928/7928]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNNpKihGPwLJ",
        "colab_type": "code",
        "outputId": "0ce6d91e-d473-4937-ecba-c260fc3f7d83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/SomaKorada07/EIP/master/utils/lr_finder.py\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-02 23:18:24--  https://raw.githubusercontent.com/SomaKorada07/EIP/master/utils/lr_finder.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14968 (15K) [text/plain]\n",
            "Saving to: ‘lr_finder.py’\n",
            "\n",
            "\rlr_finder.py          0%[                    ]       0  --.-KB/s               \rlr_finder.py        100%[===================>]  14.62K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-08-02 23:18:24 (1.19 MB/s) - ‘lr_finder.py’ saved [14968/14968]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obJeeUUCiGaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from lr_finder import LRFinder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRoHy2TUct44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_epoch = 1\n",
        "\n",
        "weights_file = 'weights/model.h5'\n",
        "model_checkpoint = ModelCheckpoint(weights_file, monitor='val_acc', save_best_only=True,\n",
        "                                   save_weights_only=True, mode='max')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl2hw4mZcI2f",
        "colab_type": "code",
        "outputId": "e87d0c1b-4146-475a-a9e3-18fad545bc91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Exponential lr finder\n",
        "# USE THIS FOR A LARGE RANGE SEARCH\n",
        "# Uncomment the validation_data flag to reduce speed but get a better idea of the learning rate\n",
        "#lr_finder = LRFinder(n_train, BATCH_SIZE, minimum_lr=1e-3, maximum_lr=10.,\n",
        "                     #lr_scale='exp',\n",
        "                     #validation_data=(X_test, Y_test),  # use the validation data for losses\n",
        "                     #validation_sample_rate=5,\n",
        "                     #save_dir='weights/', verbose=True)\n",
        "\n",
        "# Linear lr finder\n",
        "# USE THIS FOR A CLOSE SEARCH\n",
        "# Uncomment the validation_data flag to reduce speed but get a better idea of the learning rate\n",
        "lr_finder = LRFinder(n_train, BATCH_SIZE, minimum_lr=5e-4, maximum_lr=1e-2,\n",
        "                      lr_scale='linear',\n",
        "                      validation_data=(X_test, y_test),  # use the validation data for losses\n",
        "                      validation_sample_rate=5,\n",
        "                      save_dir='weights/', verbose=True)\n",
        "\n",
        "\n",
        "# plot the previous values if present\n",
        "LRFinder.plot_schedule_from_file('weights/', clip_beginning=10, clip_endding=5)\n",
        "\n",
        "optimizer = SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE, shuffle=True),\n",
        "                        steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
        "                        validation_data=(X_test, Y_test),\n",
        "                        epochs=nb_epoch, verbose=1,\n",
        "                        callbacks=[lr_finder, model_checkpoint])\n",
        "\n",
        "lr_finder.plot_schedule(clip_beginning=10, clip_endding=5)\n",
        "\n",
        "scores = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)\n",
        "for score, metric_name in zip(scores, model.metrics_names):\n",
        "    print(\"%s : %0.4f\" % (metric_name, score))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0802 23:18:25.896939 140655597586304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "weights/losses.npy and weights/lrs.npy could not be found at directory : {weights/}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0802 23:18:26.392425 140655597586304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 34:47 - loss: 2.8008 - acc: 0.1250 - LRFinder: val_loss: 2.8388 - lr = 0.00050000 \n",
            "  2/390 [..............................] - ETA: 19:30 - loss: 2.8004 - acc: 0.1289 - LRFinder: val_loss: 2.8378 - lr = 0.00052436 \n",
            "  3/390 [..............................] - ETA: 13:42 - loss: 2.8043 - acc: 0.1276 - LRFinder: val_loss: 2.8411 - lr = 0.00054872 \n",
            "  4/390 [..............................] - ETA: 10:48 - loss: 2.8179 - acc: 0.1211 - LRFinder: val_loss: 2.8418 - lr = 0.00057308 \n",
            "  5/390 [..............................] - ETA: 9:04 - loss: 2.8165 - acc: 0.1313  - LRFinder: val_loss: 2.8486 - lr = 0.00059744 \n",
            "  6/390 [..............................] - ETA: 7:54 - loss: 2.8246 - acc: 0.1276 - LRFinder: val_loss: 2.8288 - lr = 0.00062179 \n",
            "  7/390 [..............................] - ETA: 7:04 - loss: 2.8205 - acc: 0.1306 - LRFinder: val_loss: 2.8389 - lr = 0.00064615 \n",
            "  8/390 [..............................] - ETA: 6:26 - loss: 2.8261 - acc: 0.1289 - LRFinder: val_loss: 2.8601 - lr = 0.00067051 \n",
            "  9/390 [..............................] - ETA: 5:57 - loss: 2.8226 - acc: 0.1302 - LRFinder: val_loss: 2.8476 - lr = 0.00069487 \n",
            " 10/390 [..............................] - ETA: 5:33 - loss: 2.8210 - acc: 0.1313 - LRFinder: val_loss: 2.8728 - lr = 0.00071923 \n",
            " 11/390 [..............................] - ETA: 5:14 - loss: 2.8181 - acc: 0.1307 - LRFinder: val_loss: 2.8592 - lr = 0.00074359 \n",
            " 12/390 [..............................] - ETA: 4:58 - loss: 2.8189 - acc: 0.1322 - LRFinder: val_loss: 2.8592 - lr = 0.00076795 \n",
            " 13/390 [>.............................] - ETA: 4:44 - loss: 2.8192 - acc: 0.1292 - LRFinder: val_loss: 2.8613 - lr = 0.00079231 \n",
            " 14/390 [>.............................] - ETA: 4:32 - loss: 2.8170 - acc: 0.1295 - LRFinder: val_loss: 2.8388 - lr = 0.00081667 \n",
            " 15/390 [>.............................] - ETA: 4:22 - loss: 2.8215 - acc: 0.1266 - LRFinder: val_loss: 2.8605 - lr = 0.00084103 \n",
            " 16/390 [>.............................] - ETA: 4:13 - loss: 2.8173 - acc: 0.1279 - LRFinder: val_loss: 2.8704 - lr = 0.00086538 \n",
            " 17/390 [>.............................] - ETA: 4:05 - loss: 2.8130 - acc: 0.1296 - LRFinder: val_loss: 2.8542 - lr = 0.00088974 \n",
            " 18/390 [>.............................] - ETA: 3:58 - loss: 2.8100 - acc: 0.1319 - LRFinder: val_loss: 2.8499 - lr = 0.00091410 \n",
            " 19/390 [>.............................] - ETA: 3:52 - loss: 2.8070 - acc: 0.1349 - LRFinder: val_loss: 2.8292 - lr = 0.00093846 \n",
            " 20/390 [>.............................] - ETA: 3:46 - loss: 2.8069 - acc: 0.1355 - LRFinder: val_loss: 2.8324 - lr = 0.00096282 \n",
            " 21/390 [>.............................] - ETA: 3:40 - loss: 2.8072 - acc: 0.1358 - LRFinder: val_loss: 2.8276 - lr = 0.00098718 \n",
            " 22/390 [>.............................] - ETA: 3:36 - loss: 2.8068 - acc: 0.1357 - LRFinder: val_loss: 2.8411 - lr = 0.00101154 \n",
            " 23/390 [>.............................] - ETA: 3:31 - loss: 2.8075 - acc: 0.1338 - LRFinder: val_loss: 2.8448 - lr = 0.00103590 \n",
            " 24/390 [>.............................] - ETA: 3:27 - loss: 2.8063 - acc: 0.1322 - LRFinder: val_loss: 2.8300 - lr = 0.00106026 \n",
            " 25/390 [>.............................] - ETA: 3:23 - loss: 2.8070 - acc: 0.1303 - LRFinder: val_loss: 2.8386 - lr = 0.00108462 \n",
            " 26/390 [=>............................] - ETA: 3:20 - loss: 2.8054 - acc: 0.1304 - LRFinder: val_loss: 2.8165 - lr = 0.00110897 \n",
            " 27/390 [=>............................] - ETA: 3:16 - loss: 2.8027 - acc: 0.1337 - LRFinder: val_loss: 2.8240 - lr = 0.00113333 \n",
            " 28/390 [=>............................] - ETA: 3:13 - loss: 2.8022 - acc: 0.1334 - LRFinder: val_loss: 2.8397 - lr = 0.00115769 \n",
            " 29/390 [=>............................] - ETA: 3:10 - loss: 2.8006 - acc: 0.1342 - LRFinder: val_loss: 2.8431 - lr = 0.00118205 \n",
            " 30/390 [=>............................] - ETA: 3:07 - loss: 2.7987 - acc: 0.1346 - LRFinder: val_loss: 2.8397 - lr = 0.00120641 \n",
            " 31/390 [=>............................] - ETA: 3:05 - loss: 2.7976 - acc: 0.1348 - LRFinder: val_loss: 2.8231 - lr = 0.00123077 \n",
            " 32/390 [=>............................] - ETA: 3:02 - loss: 2.7968 - acc: 0.1343 - LRFinder: val_loss: 2.8178 - lr = 0.00125513 \n",
            " 33/390 [=>............................] - ETA: 3:00 - loss: 2.7959 - acc: 0.1359 - LRFinder: val_loss: 2.8280 - lr = 0.00127949 \n",
            " 34/390 [=>............................] - ETA: 2:58 - loss: 2.7942 - acc: 0.1376 - LRFinder: val_loss: 2.8251 - lr = 0.00130385 \n",
            " 35/390 [=>............................] - ETA: 2:56 - loss: 2.7943 - acc: 0.1377 - LRFinder: val_loss: 2.8397 - lr = 0.00132821 \n",
            " 36/390 [=>............................] - ETA: 2:53 - loss: 2.7930 - acc: 0.1363 - LRFinder: val_loss: 2.8374 - lr = 0.00135256 \n",
            " 37/390 [=>............................] - ETA: 2:51 - loss: 2.7917 - acc: 0.1362 - LRFinder: val_loss: 2.8358 - lr = 0.00137692 \n",
            " 38/390 [=>............................] - ETA: 2:50 - loss: 2.7907 - acc: 0.1367 - LRFinder: val_loss: 2.8447 - lr = 0.00140128 \n",
            " 39/390 [==>...........................] - ETA: 2:48 - loss: 2.7886 - acc: 0.1380 - LRFinder: val_loss: 2.8314 - lr = 0.00142564 \n",
            " 40/390 [==>...........................] - ETA: 2:46 - loss: 2.7866 - acc: 0.1396 - LRFinder: val_loss: 2.8236 - lr = 0.00145000 \n",
            " 41/390 [==>...........................] - ETA: 2:45 - loss: 2.7858 - acc: 0.1399 - LRFinder: val_loss: 2.8217 - lr = 0.00147436 \n",
            " 42/390 [==>...........................] - ETA: 2:43 - loss: 2.7850 - acc: 0.1399 - LRFinder: val_loss: 2.8509 - lr = 0.00149872 \n",
            " 43/390 [==>...........................] - ETA: 2:41 - loss: 2.7826 - acc: 0.1412 - LRFinder: val_loss: 2.8192 - lr = 0.00152308 \n",
            " 44/390 [==>...........................] - ETA: 2:40 - loss: 2.7819 - acc: 0.1412 - LRFinder: val_loss: 2.8314 - lr = 0.00154744 \n",
            " 45/390 [==>...........................] - ETA: 2:39 - loss: 2.7817 - acc: 0.1408 - LRFinder: val_loss: 2.8256 - lr = 0.00157179 \n",
            " 46/390 [==>...........................] - ETA: 2:37 - loss: 2.7797 - acc: 0.1422 - LRFinder: val_loss: 2.8429 - lr = 0.00159615 \n",
            " 47/390 [==>...........................] - ETA: 2:36 - loss: 2.7773 - acc: 0.1443 - LRFinder: val_loss: 2.8332 - lr = 0.00162051 \n",
            " 48/390 [==>...........................] - ETA: 2:34 - loss: 2.7759 - acc: 0.1453 - LRFinder: val_loss: 2.8554 - lr = 0.00164487 \n",
            " 49/390 [==>...........................] - ETA: 2:33 - loss: 2.7745 - acc: 0.1464 - LRFinder: val_loss: 2.8449 - lr = 0.00166923 \n",
            " 50/390 [==>...........................] - ETA: 2:32 - loss: 2.7724 - acc: 0.1470 - LRFinder: val_loss: 2.8386 - lr = 0.00169359 \n",
            " 51/390 [==>...........................] - ETA: 2:31 - loss: 2.7703 - acc: 0.1492 - LRFinder: val_loss: 2.8497 - lr = 0.00171795 \n",
            " 52/390 [===>..........................] - ETA: 2:30 - loss: 2.7687 - acc: 0.1504 - LRFinder: val_loss: 2.8329 - lr = 0.00174231 \n",
            " 53/390 [===>..........................] - ETA: 2:29 - loss: 2.7665 - acc: 0.1520 - LRFinder: val_loss: 2.8215 - lr = 0.00176667 \n",
            " 54/390 [===>..........................] - ETA: 2:28 - loss: 2.7654 - acc: 0.1526 - LRFinder: val_loss: 2.8717 - lr = 0.00179103 \n",
            " 55/390 [===>..........................] - ETA: 2:26 - loss: 2.7639 - acc: 0.1530 - LRFinder: val_loss: 2.8364 - lr = 0.00181538 \n",
            " 56/390 [===>..........................] - ETA: 2:25 - loss: 2.7626 - acc: 0.1533 - LRFinder: val_loss: 2.8306 - lr = 0.00183974 \n",
            " 57/390 [===>..........................] - ETA: 2:24 - loss: 2.7610 - acc: 0.1541 - LRFinder: val_loss: 2.8741 - lr = 0.00186410 \n",
            " 58/390 [===>..........................] - ETA: 2:23 - loss: 2.7591 - acc: 0.1553 - LRFinder: val_loss: 2.8737 - lr = 0.00188846 \n",
            " 59/390 [===>..........................] - ETA: 2:22 - loss: 2.7582 - acc: 0.1559 - LRFinder: val_loss: 2.8487 - lr = 0.00191282 \n",
            " 60/390 [===>..........................] - ETA: 2:22 - loss: 2.7571 - acc: 0.1561 - LRFinder: val_loss: 2.8665 - lr = 0.00193718 \n",
            " 61/390 [===>..........................] - ETA: 2:21 - loss: 2.7549 - acc: 0.1573 - LRFinder: val_loss: 2.8450 - lr = 0.00196154 \n",
            " 62/390 [===>..........................] - ETA: 2:20 - loss: 2.7535 - acc: 0.1580 - LRFinder: val_loss: 2.8612 - lr = 0.00198590 \n",
            " 63/390 [===>..........................] - ETA: 2:19 - loss: 2.7522 - acc: 0.1587 - LRFinder: val_loss: 2.8405 - lr = 0.00201026 \n",
            " 64/390 [===>..........................] - ETA: 2:18 - loss: 2.7504 - acc: 0.1595 - LRFinder: val_loss: 2.8823 - lr = 0.00203462 \n",
            " 65/390 [====>.........................] - ETA: 2:17 - loss: 2.7492 - acc: 0.1606 - LRFinder: val_loss: 2.8688 - lr = 0.00205897 \n",
            " 66/390 [====>.........................] - ETA: 2:16 - loss: 2.7486 - acc: 0.1602 - LRFinder: val_loss: 2.8413 - lr = 0.00208333 \n",
            " 67/390 [====>.........................] - ETA: 2:15 - loss: 2.7474 - acc: 0.1604 - LRFinder: val_loss: 2.8523 - lr = 0.00210769 \n",
            " 68/390 [====>.........................] - ETA: 2:15 - loss: 2.7462 - acc: 0.1615 - LRFinder: val_loss: 2.8370 - lr = 0.00213205 \n",
            " 69/390 [====>.........................] - ETA: 2:14 - loss: 2.7446 - acc: 0.1623 - LRFinder: val_loss: 2.8398 - lr = 0.00215641 \n",
            " 70/390 [====>.........................] - ETA: 2:13 - loss: 2.7429 - acc: 0.1634 - LRFinder: val_loss: 2.8690 - lr = 0.00218077 \n",
            " 71/390 [====>.........................] - ETA: 2:12 - loss: 2.7412 - acc: 0.1637 - LRFinder: val_loss: 2.8471 - lr = 0.00220513 \n",
            " 72/390 [====>.........................] - ETA: 2:11 - loss: 2.7393 - acc: 0.1651 - LRFinder: val_loss: 2.8372 - lr = 0.00222949 \n",
            " 73/390 [====>.........................] - ETA: 2:11 - loss: 2.7377 - acc: 0.1658 - LRFinder: val_loss: 2.8673 - lr = 0.00225385 \n",
            " 74/390 [====>.........................] - ETA: 2:10 - loss: 2.7359 - acc: 0.1663 - LRFinder: val_loss: 2.9023 - lr = 0.00227821 \n",
            " 75/390 [====>.........................] - ETA: 2:09 - loss: 2.7341 - acc: 0.1674 - LRFinder: val_loss: 2.8849 - lr = 0.00230256 \n",
            " 76/390 [====>.........................] - ETA: 2:09 - loss: 2.7330 - acc: 0.1679 - LRFinder: val_loss: 2.8857 - lr = 0.00232692 \n",
            " 77/390 [====>.........................] - ETA: 2:08 - loss: 2.7316 - acc: 0.1681 - LRFinder: val_loss: 2.8435 - lr = 0.00235128 \n",
            " 78/390 [=====>........................] - ETA: 2:07 - loss: 2.7301 - acc: 0.1683 - LRFinder: val_loss: 2.8454 - lr = 0.00237564 \n",
            " 79/390 [=====>........................] - ETA: 2:07 - loss: 2.7286 - acc: 0.1687 - LRFinder: val_loss: 2.8549 - lr = 0.00240000 \n",
            " 80/390 [=====>........................] - ETA: 2:06 - loss: 2.7273 - acc: 0.1696 - LRFinder: val_loss: 2.8463 - lr = 0.00242436 \n",
            " 81/390 [=====>........................] - ETA: 2:05 - loss: 2.7256 - acc: 0.1705 - LRFinder: val_loss: 2.8921 - lr = 0.00244872 \n",
            " 82/390 [=====>........................] - ETA: 2:05 - loss: 2.7237 - acc: 0.1711 - LRFinder: val_loss: 2.8659 - lr = 0.00247308 \n",
            " 83/390 [=====>........................] - ETA: 2:04 - loss: 2.7218 - acc: 0.1721 - LRFinder: val_loss: 2.8744 - lr = 0.00249744 \n",
            " 84/390 [=====>........................] - ETA: 2:03 - loss: 2.7205 - acc: 0.1723 - LRFinder: val_loss: 2.8495 - lr = 0.00252179 \n",
            " 85/390 [=====>........................] - ETA: 2:03 - loss: 2.7195 - acc: 0.1727 - LRFinder: val_loss: 2.8309 - lr = 0.00254615 \n",
            " 86/390 [=====>........................] - ETA: 2:02 - loss: 2.7186 - acc: 0.1732 - LRFinder: val_loss: 2.8591 - lr = 0.00257051 \n",
            " 87/390 [=====>........................] - ETA: 2:01 - loss: 2.7167 - acc: 0.1739 - LRFinder: val_loss: 2.8472 - lr = 0.00259487 \n",
            " 88/390 [=====>........................] - ETA: 2:01 - loss: 2.7150 - acc: 0.1745 - LRFinder: val_loss: 2.8677 - lr = 0.00261923 \n",
            " 89/390 [=====>........................] - ETA: 2:00 - loss: 2.7133 - acc: 0.1759 - LRFinder: val_loss: 2.8375 - lr = 0.00264359 \n",
            " 90/390 [=====>........................] - ETA: 2:00 - loss: 2.7111 - acc: 0.1777 - LRFinder: val_loss: 2.8532 - lr = 0.00266795 \n",
            " 91/390 [======>.......................] - ETA: 1:59 - loss: 2.7092 - acc: 0.1785 - LRFinder: val_loss: 2.8237 - lr = 0.00269231 \n",
            " 92/390 [======>.......................] - ETA: 1:58 - loss: 2.7077 - acc: 0.1792 - LRFinder: val_loss: 2.8681 - lr = 0.00271667 \n",
            " 93/390 [======>.......................] - ETA: 1:58 - loss: 2.7065 - acc: 0.1795 - LRFinder: val_loss: 2.8464 - lr = 0.00274103 \n",
            " 94/390 [======>.......................] - ETA: 1:57 - loss: 2.7052 - acc: 0.1802 - LRFinder: val_loss: 2.8617 - lr = 0.00276538 \n",
            " 95/390 [======>.......................] - ETA: 1:57 - loss: 2.7046 - acc: 0.1803 - LRFinder: val_loss: 2.8534 - lr = 0.00278974 \n",
            " 96/390 [======>.......................] - ETA: 1:56 - loss: 2.7034 - acc: 0.1807 - LRFinder: val_loss: 2.8848 - lr = 0.00281410 \n",
            " 97/390 [======>.......................] - ETA: 1:55 - loss: 2.7015 - acc: 0.1815 - LRFinder: val_loss: 2.8578 - lr = 0.00283846 \n",
            " 98/390 [======>.......................] - ETA: 1:55 - loss: 2.6996 - acc: 0.1821 - LRFinder: val_loss: 2.8777 - lr = 0.00286282 \n",
            " 99/390 [======>.......................] - ETA: 1:54 - loss: 2.6974 - acc: 0.1828 - LRFinder: val_loss: 2.8754 - lr = 0.00288718 \n",
            "100/390 [======>.......................] - ETA: 1:54 - loss: 2.6952 - acc: 0.1836 - LRFinder: val_loss: 2.8798 - lr = 0.00291154 \n",
            "101/390 [======>.......................] - ETA: 1:53 - loss: 2.6933 - acc: 0.1846 - LRFinder: val_loss: 2.8486 - lr = 0.00293590 \n",
            "102/390 [======>.......................] - ETA: 1:53 - loss: 2.6918 - acc: 0.1845 - LRFinder: val_loss: 2.8702 - lr = 0.00296026 \n",
            "103/390 [======>.......................] - ETA: 1:52 - loss: 2.6901 - acc: 0.1858 - LRFinder: val_loss: 2.9132 - lr = 0.00298462 \n",
            "104/390 [=======>......................] - ETA: 1:52 - loss: 2.6890 - acc: 0.1868 - LRFinder: val_loss: 2.8584 - lr = 0.00300897 \n",
            "105/390 [=======>......................] - ETA: 1:51 - loss: 2.6870 - acc: 0.1879 - LRFinder: val_loss: 2.9036 - lr = 0.00303333 \n",
            "106/390 [=======>......................] - ETA: 1:50 - loss: 2.6852 - acc: 0.1883 - LRFinder: val_loss: 2.8641 - lr = 0.00305769 \n",
            "107/390 [=======>......................] - ETA: 1:50 - loss: 2.6833 - acc: 0.1891 - LRFinder: val_loss: 2.8709 - lr = 0.00308205 \n",
            "108/390 [=======>......................] - ETA: 1:49 - loss: 2.6811 - acc: 0.1905 - LRFinder: val_loss: 2.8543 - lr = 0.00310641 \n",
            "109/390 [=======>......................] - ETA: 1:49 - loss: 2.6798 - acc: 0.1902 - LRFinder: val_loss: 2.9034 - lr = 0.00313077 \n",
            "110/390 [=======>......................] - ETA: 1:48 - loss: 2.6780 - acc: 0.1906 - LRFinder: val_loss: 2.8740 - lr = 0.00315513 \n",
            "111/390 [=======>......................] - ETA: 1:48 - loss: 2.6770 - acc: 0.1909 - LRFinder: val_loss: 2.9331 - lr = 0.00317949 \n",
            "112/390 [=======>......................] - ETA: 1:47 - loss: 2.6753 - acc: 0.1914 - LRFinder: val_loss: 2.8950 - lr = 0.00320385 \n",
            "113/390 [=======>......................] - ETA: 1:47 - loss: 2.6738 - acc: 0.1922 - LRFinder: val_loss: 2.8817 - lr = 0.00322821 \n",
            "114/390 [=======>......................] - ETA: 1:46 - loss: 2.6727 - acc: 0.1925 - LRFinder: val_loss: 2.8998 - lr = 0.00325256 \n",
            "115/390 [=======>......................] - ETA: 1:46 - loss: 2.6708 - acc: 0.1937 - LRFinder: val_loss: 2.9325 - lr = 0.00327692 \n",
            "116/390 [=======>......................] - ETA: 1:45 - loss: 2.6696 - acc: 0.1939 - LRFinder: val_loss: 2.9020 - lr = 0.00330128 \n",
            "117/390 [========>.....................] - ETA: 1:45 - loss: 2.6678 - acc: 0.1947 - LRFinder: val_loss: 2.8740 - lr = 0.00332564 \n",
            "118/390 [========>.....................] - ETA: 1:44 - loss: 2.6662 - acc: 0.1952 - LRFinder: val_loss: 2.9200 - lr = 0.00335000 \n",
            "119/390 [========>.....................] - ETA: 1:44 - loss: 2.6649 - acc: 0.1956 - LRFinder: val_loss: 2.8769 - lr = 0.00337436 \n",
            "120/390 [========>.....................] - ETA: 1:43 - loss: 2.6638 - acc: 0.1958 - LRFinder: val_loss: 2.9754 - lr = 0.00339872 \n",
            "121/390 [========>.....................] - ETA: 1:43 - loss: 2.6619 - acc: 0.1965 - LRFinder: val_loss: 2.9562 - lr = 0.00342308 \n",
            "122/390 [========>.....................] - ETA: 1:42 - loss: 2.6601 - acc: 0.1972 - LRFinder: val_loss: 2.9492 - lr = 0.00344744 \n",
            "123/390 [========>.....................] - ETA: 1:42 - loss: 2.6578 - acc: 0.1983 - LRFinder: val_loss: 2.9291 - lr = 0.00347179 \n",
            "124/390 [========>.....................] - ETA: 1:41 - loss: 2.6561 - acc: 0.1987 - LRFinder: val_loss: 2.9561 - lr = 0.00349615 \n",
            "125/390 [========>.....................] - ETA: 1:41 - loss: 2.6539 - acc: 0.1994 - LRFinder: val_loss: 2.8996 - lr = 0.00352051 \n",
            "126/390 [========>.....................] - ETA: 1:40 - loss: 2.6525 - acc: 0.1997 - LRFinder: val_loss: 2.9470 - lr = 0.00354487 \n",
            "127/390 [========>.....................] - ETA: 1:40 - loss: 2.6510 - acc: 0.1996 - LRFinder: val_loss: 2.9855 - lr = 0.00356923 \n",
            "128/390 [========>.....................] - ETA: 1:39 - loss: 2.6494 - acc: 0.1999 - LRFinder: val_loss: 3.0010 - lr = 0.00359359 \n",
            "129/390 [========>.....................] - ETA: 1:39 - loss: 2.6482 - acc: 0.2005 - LRFinder: val_loss: 2.9582 - lr = 0.00361795 \n",
            "130/390 [=========>....................] - ETA: 1:38 - loss: 2.6467 - acc: 0.2006 - LRFinder: val_loss: 2.9705 - lr = 0.00364231 \n",
            "131/390 [=========>....................] - ETA: 1:38 - loss: 2.6464 - acc: 0.2007 - LRFinder: val_loss: 2.9632 - lr = 0.00366667 \n",
            "132/390 [=========>....................] - ETA: 1:38 - loss: 2.6447 - acc: 0.2013 - LRFinder: val_loss: 2.9641 - lr = 0.00369103 \n",
            "133/390 [=========>....................] - ETA: 1:37 - loss: 2.6437 - acc: 0.2017 - LRFinder: val_loss: 3.0700 - lr = 0.00371538 \n",
            "134/390 [=========>....................] - ETA: 1:37 - loss: 2.6420 - acc: 0.2023 - LRFinder: val_loss: 2.9205 - lr = 0.00373974 \n",
            "135/390 [=========>....................] - ETA: 1:36 - loss: 2.6406 - acc: 0.2026 - LRFinder: val_loss: 3.0309 - lr = 0.00376410 \n",
            "136/390 [=========>....................] - ETA: 1:36 - loss: 2.6391 - acc: 0.2031 - LRFinder: val_loss: 3.0565 - lr = 0.00378846 \n",
            "137/390 [=========>....................] - ETA: 1:35 - loss: 2.6381 - acc: 0.2040 - LRFinder: val_loss: 3.0063 - lr = 0.00381282 \n",
            "138/390 [=========>....................] - ETA: 1:35 - loss: 2.6365 - acc: 0.2045 - LRFinder: val_loss: 3.0574 - lr = 0.00383718 \n",
            "139/390 [=========>....................] - ETA: 1:34 - loss: 2.6349 - acc: 0.2054 - LRFinder: val_loss: 3.1001 - lr = 0.00386154 \n",
            "140/390 [=========>....................] - ETA: 1:34 - loss: 2.6339 - acc: 0.2059 - LRFinder: val_loss: 3.0425 - lr = 0.00388590 \n",
            "141/390 [=========>....................] - ETA: 1:33 - loss: 2.6333 - acc: 0.2062 - LRFinder: val_loss: 3.0133 - lr = 0.00391026 \n",
            "142/390 [=========>....................] - ETA: 1:33 - loss: 2.6323 - acc: 0.2064 - LRFinder: val_loss: 3.0763 - lr = 0.00393462 \n",
            "143/390 [==========>...................] - ETA: 1:33 - loss: 2.6313 - acc: 0.2061 - LRFinder: val_loss: 3.0736 - lr = 0.00395897 \n",
            "144/390 [==========>...................] - ETA: 1:32 - loss: 2.6298 - acc: 0.2069 - LRFinder: val_loss: 3.0975 - lr = 0.00398333 \n",
            "145/390 [==========>...................] - ETA: 1:32 - loss: 2.6278 - acc: 0.2080 - LRFinder: val_loss: 3.0505 - lr = 0.00400769 \n",
            "146/390 [==========>...................] - ETA: 1:31 - loss: 2.6262 - acc: 0.2089 - LRFinder: val_loss: 3.1023 - lr = 0.00403205 \n",
            "147/390 [==========>...................] - ETA: 1:31 - loss: 2.6247 - acc: 0.2093 - LRFinder: val_loss: 3.1037 - lr = 0.00405641 \n",
            "148/390 [==========>...................] - ETA: 1:30 - loss: 2.6239 - acc: 0.2099 - LRFinder: val_loss: 3.1310 - lr = 0.00408077 \n",
            "149/390 [==========>...................] - ETA: 1:30 - loss: 2.6221 - acc: 0.2102 - LRFinder: val_loss: 3.0904 - lr = 0.00410513 \n",
            "150/390 [==========>...................] - ETA: 1:29 - loss: 2.6209 - acc: 0.2103 - LRFinder: val_loss: 3.0267 - lr = 0.00412949 \n",
            "151/390 [==========>...................] - ETA: 1:29 - loss: 2.6193 - acc: 0.2114 - LRFinder: val_loss: 3.0180 - lr = 0.00415385 \n",
            "152/390 [==========>...................] - ETA: 1:29 - loss: 2.6183 - acc: 0.2114 - LRFinder: val_loss: 3.0996 - lr = 0.00417821 \n",
            "153/390 [==========>...................] - ETA: 1:28 - loss: 2.6177 - acc: 0.2118 - LRFinder: val_loss: 3.1581 - lr = 0.00420256 \n",
            "154/390 [==========>...................] - ETA: 1:28 - loss: 2.6160 - acc: 0.2129 - LRFinder: val_loss: 3.1517 - lr = 0.00422692 \n",
            "155/390 [==========>...................] - ETA: 1:27 - loss: 2.6142 - acc: 0.2136 - LRFinder: val_loss: 3.1406 - lr = 0.00425128 \n",
            "156/390 [===========>..................] - ETA: 1:27 - loss: 2.6124 - acc: 0.2141 - LRFinder: val_loss: 3.2035 - lr = 0.00427564 \n",
            "157/390 [===========>..................] - ETA: 1:26 - loss: 2.6115 - acc: 0.2140 - LRFinder: val_loss: 3.0945 - lr = 0.00430000 \n",
            "158/390 [===========>..................] - ETA: 1:26 - loss: 2.6103 - acc: 0.2147 - LRFinder: val_loss: 3.2069 - lr = 0.00432436 \n",
            "159/390 [===========>..................] - ETA: 1:26 - loss: 2.6089 - acc: 0.2151 - LRFinder: val_loss: 3.2997 - lr = 0.00434872 \n",
            "160/390 [===========>..................] - ETA: 1:25 - loss: 2.6077 - acc: 0.2159 - LRFinder: val_loss: 3.1452 - lr = 0.00437308 \n",
            "161/390 [===========>..................] - ETA: 1:25 - loss: 2.6069 - acc: 0.2162 - LRFinder: val_loss: 3.1948 - lr = 0.00439744 \n",
            "162/390 [===========>..................] - ETA: 1:24 - loss: 2.6061 - acc: 0.2166 - LRFinder: val_loss: 3.2299 - lr = 0.00442179 \n",
            "163/390 [===========>..................] - ETA: 1:24 - loss: 2.6046 - acc: 0.2173 - LRFinder: val_loss: 3.2542 - lr = 0.00444615 \n",
            "164/390 [===========>..................] - ETA: 1:24 - loss: 2.6039 - acc: 0.2175 - LRFinder: val_loss: 3.2050 - lr = 0.00447051 \n",
            "165/390 [===========>..................] - ETA: 1:23 - loss: 2.6025 - acc: 0.2179 - LRFinder: val_loss: 3.2584 - lr = 0.00449487 \n",
            "166/390 [===========>..................] - ETA: 1:23 - loss: 2.6013 - acc: 0.2185 - LRFinder: val_loss: 3.2744 - lr = 0.00451923 \n",
            "167/390 [===========>..................] - ETA: 1:22 - loss: 2.6003 - acc: 0.2187 - LRFinder: val_loss: 3.2270 - lr = 0.00454359 \n",
            "168/390 [===========>..................] - ETA: 1:22 - loss: 2.5993 - acc: 0.2193 - LRFinder: val_loss: 3.1583 - lr = 0.00456795 \n",
            "169/390 [============>.................] - ETA: 1:21 - loss: 2.5980 - acc: 0.2198 - LRFinder: val_loss: 3.2526 - lr = 0.00459231 \n",
            "170/390 [============>.................] - ETA: 1:21 - loss: 2.5967 - acc: 0.2205 - LRFinder: val_loss: 3.2839 - lr = 0.00461667 \n",
            "171/390 [============>.................] - ETA: 1:21 - loss: 2.5948 - acc: 0.2215 - LRFinder: val_loss: 3.3165 - lr = 0.00464103 \n",
            "172/390 [============>.................] - ETA: 1:20 - loss: 2.5932 - acc: 0.2219 - LRFinder: val_loss: 3.1295 - lr = 0.00466538 \n",
            "173/390 [============>.................] - ETA: 1:20 - loss: 2.5923 - acc: 0.2219 - LRFinder: val_loss: 3.2581 - lr = 0.00468974 \n",
            "174/390 [============>.................] - ETA: 1:19 - loss: 2.5914 - acc: 0.2221 - LRFinder: val_loss: 3.2285 - lr = 0.00471410 \n",
            "175/390 [============>.................] - ETA: 1:19 - loss: 2.5902 - acc: 0.2223 - LRFinder: val_loss: 3.2115 - lr = 0.00473846 \n",
            "176/390 [============>.................] - ETA: 1:19 - loss: 2.5887 - acc: 0.2232 - LRFinder: val_loss: 3.2204 - lr = 0.00476282 \n",
            "177/390 [============>.................] - ETA: 1:18 - loss: 2.5880 - acc: 0.2235 - LRFinder: val_loss: 3.2836 - lr = 0.00478718 \n",
            "178/390 [============>.................] - ETA: 1:18 - loss: 2.5868 - acc: 0.2240 - LRFinder: val_loss: 3.2752 - lr = 0.00481154 \n",
            "179/390 [============>.................] - ETA: 1:17 - loss: 2.5854 - acc: 0.2247 - LRFinder: val_loss: 3.3023 - lr = 0.00483590 \n",
            "180/390 [============>.................] - ETA: 1:17 - loss: 2.5844 - acc: 0.2253 - LRFinder: val_loss: 3.3187 - lr = 0.00486026 \n",
            "181/390 [============>.................] - ETA: 1:16 - loss: 2.5837 - acc: 0.2254 - LRFinder: val_loss: 3.1919 - lr = 0.00488462 \n",
            "182/390 [=============>................] - ETA: 1:16 - loss: 2.5823 - acc: 0.2257 - LRFinder: val_loss: 3.3121 - lr = 0.00490897 \n",
            "183/390 [=============>................] - ETA: 1:16 - loss: 2.5813 - acc: 0.2260 - LRFinder: val_loss: 3.2837 - lr = 0.00493333 \n",
            "184/390 [=============>................] - ETA: 1:15 - loss: 2.5805 - acc: 0.2259 - LRFinder: val_loss: 3.3461 - lr = 0.00495769 \n",
            "185/390 [=============>................] - ETA: 1:15 - loss: 2.5797 - acc: 0.2262 - LRFinder: val_loss: 3.2219 - lr = 0.00498205 \n",
            "186/390 [=============>................] - ETA: 1:14 - loss: 2.5788 - acc: 0.2265 - LRFinder: val_loss: 3.2215 - lr = 0.00500641 \n",
            "187/390 [=============>................] - ETA: 1:14 - loss: 2.5783 - acc: 0.2265 - LRFinder: val_loss: 3.3264 - lr = 0.00503077 \n",
            "188/390 [=============>................] - ETA: 1:14 - loss: 2.5773 - acc: 0.2269 - LRFinder: val_loss: 3.1165 - lr = 0.00505513 \n",
            "189/390 [=============>................] - ETA: 1:13 - loss: 2.5762 - acc: 0.2273 - LRFinder: val_loss: 3.3773 - lr = 0.00507949 \n",
            "190/390 [=============>................] - ETA: 1:13 - loss: 2.5747 - acc: 0.2278 - LRFinder: val_loss: 3.2606 - lr = 0.00510385 \n",
            "191/390 [=============>................] - ETA: 1:12 - loss: 2.5737 - acc: 0.2283 - LRFinder: val_loss: 3.2820 - lr = 0.00512821 \n",
            "192/390 [=============>................] - ETA: 1:12 - loss: 2.5724 - acc: 0.2289 - LRFinder: val_loss: 3.3526 - lr = 0.00515256 \n",
            "193/390 [=============>................] - ETA: 1:12 - loss: 2.5709 - acc: 0.2293 - LRFinder: val_loss: 3.3292 - lr = 0.00517692 \n",
            "194/390 [=============>................] - ETA: 1:11 - loss: 2.5697 - acc: 0.2296 - LRFinder: val_loss: 3.4305 - lr = 0.00520128 \n",
            "195/390 [==============>...............] - ETA: 1:11 - loss: 2.5683 - acc: 0.2304 - LRFinder: val_loss: 3.4045 - lr = 0.00522564 \n",
            "196/390 [==============>...............] - ETA: 1:10 - loss: 2.5670 - acc: 0.2307 - LRFinder: val_loss: 3.3487 - lr = 0.00525000 \n",
            "197/390 [==============>...............] - ETA: 1:10 - loss: 2.5660 - acc: 0.2309 - LRFinder: val_loss: 3.3214 - lr = 0.00527436 \n",
            "198/390 [==============>...............] - ETA: 1:10 - loss: 2.5650 - acc: 0.2313 - LRFinder: val_loss: 3.4166 - lr = 0.00529872 \n",
            "199/390 [==============>...............] - ETA: 1:09 - loss: 2.5644 - acc: 0.2315 - LRFinder: val_loss: 3.2735 - lr = 0.00532308 \n",
            "200/390 [==============>...............] - ETA: 1:09 - loss: 2.5630 - acc: 0.2320 - LRFinder: val_loss: 3.3923 - lr = 0.00534744 \n",
            "201/390 [==============>...............] - ETA: 1:09 - loss: 2.5622 - acc: 0.2324 - LRFinder: val_loss: 3.2713 - lr = 0.00537179 \n",
            "202/390 [==============>...............] - ETA: 1:08 - loss: 2.5613 - acc: 0.2324 - LRFinder: val_loss: 3.4062 - lr = 0.00539615 \n",
            "203/390 [==============>...............] - ETA: 1:08 - loss: 2.5604 - acc: 0.2328 - LRFinder: val_loss: 3.4335 - lr = 0.00542051 \n",
            "204/390 [==============>...............] - ETA: 1:07 - loss: 2.5592 - acc: 0.2335 - LRFinder: val_loss: 3.4553 - lr = 0.00544487 \n",
            "205/390 [==============>...............] - ETA: 1:07 - loss: 2.5584 - acc: 0.2341 - LRFinder: val_loss: 3.5007 - lr = 0.00546923 \n",
            "206/390 [==============>...............] - ETA: 1:07 - loss: 2.5577 - acc: 0.2345 - LRFinder: val_loss: 3.5084 - lr = 0.00549359 \n",
            "207/390 [==============>...............] - ETA: 1:06 - loss: 2.5558 - acc: 0.2352 - LRFinder: val_loss: 3.4945 - lr = 0.00551795 \n",
            "208/390 [===============>..............] - ETA: 1:06 - loss: 2.5543 - acc: 0.2357 - LRFinder: val_loss: 3.4902 - lr = 0.00554231 \n",
            "209/390 [===============>..............] - ETA: 1:05 - loss: 2.5531 - acc: 0.2361 - LRFinder: val_loss: 3.3785 - lr = 0.00556667 \n",
            "210/390 [===============>..............] - ETA: 1:05 - loss: 2.5521 - acc: 0.2363 - LRFinder: val_loss: 3.4820 - lr = 0.00559103 \n",
            "211/390 [===============>..............] - ETA: 1:05 - loss: 2.5511 - acc: 0.2367 - LRFinder: val_loss: 3.5219 - lr = 0.00561538 \n",
            "212/390 [===============>..............] - ETA: 1:04 - loss: 2.5501 - acc: 0.2370 - LRFinder: val_loss: 3.4073 - lr = 0.00563974 \n",
            "213/390 [===============>..............] - ETA: 1:04 - loss: 2.5492 - acc: 0.2373 - LRFinder: val_loss: 3.5137 - lr = 0.00566410 \n",
            "214/390 [===============>..............] - ETA: 1:03 - loss: 2.5484 - acc: 0.2375 - LRFinder: val_loss: 3.3396 - lr = 0.00568846 \n",
            "215/390 [===============>..............] - ETA: 1:03 - loss: 2.5470 - acc: 0.2380 - LRFinder: val_loss: 3.4352 - lr = 0.00571282 \n",
            "216/390 [===============>..............] - ETA: 1:03 - loss: 2.5459 - acc: 0.2386 - LRFinder: val_loss: 3.4543 - lr = 0.00573718 \n",
            "217/390 [===============>..............] - ETA: 1:02 - loss: 2.5450 - acc: 0.2389 - LRFinder: val_loss: 3.4609 - lr = 0.00576154 \n",
            "218/390 [===============>..............] - ETA: 1:02 - loss: 2.5438 - acc: 0.2393 - LRFinder: val_loss: 3.4492 - lr = 0.00578590 \n",
            "219/390 [===============>..............] - ETA: 1:02 - loss: 2.5432 - acc: 0.2397 - LRFinder: val_loss: 3.4957 - lr = 0.00581026 \n",
            "220/390 [===============>..............] - ETA: 1:01 - loss: 2.5427 - acc: 0.2398 - LRFinder: val_loss: 3.5159 - lr = 0.00583462 \n",
            "221/390 [================>.............] - ETA: 1:01 - loss: 2.5420 - acc: 0.2399 - LRFinder: val_loss: 3.4952 - lr = 0.00585897 \n",
            "222/390 [================>.............] - ETA: 1:00 - loss: 2.5410 - acc: 0.2401 - LRFinder: val_loss: 3.4754 - lr = 0.00588333 \n",
            "223/390 [================>.............] - ETA: 1:00 - loss: 2.5396 - acc: 0.2408 - LRFinder: val_loss: 3.5305 - lr = 0.00590769 \n",
            "224/390 [================>.............] - ETA: 1:00 - loss: 2.5386 - acc: 0.2410 - LRFinder: val_loss: 3.5895 - lr = 0.00593205 \n",
            "225/390 [================>.............] - ETA: 59s - loss: 2.5372 - acc: 0.2415  - LRFinder: val_loss: 3.4438 - lr = 0.00595641 \n",
            "226/390 [================>.............] - ETA: 59s - loss: 2.5355 - acc: 0.2422 - LRFinder: val_loss: 3.4545 - lr = 0.00598077 \n",
            "227/390 [================>.............] - ETA: 59s - loss: 2.5349 - acc: 0.2422 - LRFinder: val_loss: 3.4508 - lr = 0.00600513 \n",
            "228/390 [================>.............] - ETA: 58s - loss: 2.5341 - acc: 0.2427 - LRFinder: val_loss: 3.4038 - lr = 0.00602949 \n",
            "229/390 [================>.............] - ETA: 58s - loss: 2.5326 - acc: 0.2433 - LRFinder: val_loss: 3.5415 - lr = 0.00605385 \n",
            "230/390 [================>.............] - ETA: 57s - loss: 2.5313 - acc: 0.2439 - LRFinder: val_loss: 3.4597 - lr = 0.00607821 \n",
            "231/390 [================>.............] - ETA: 57s - loss: 2.5295 - acc: 0.2447 - LRFinder: val_loss: 3.3464 - lr = 0.00610256 \n",
            "232/390 [================>.............] - ETA: 57s - loss: 2.5282 - acc: 0.2452 - LRFinder: val_loss: 3.4818 - lr = 0.00612692 \n",
            "233/390 [================>.............] - ETA: 56s - loss: 2.5272 - acc: 0.2455 - LRFinder: val_loss: 3.4332 - lr = 0.00615128 \n",
            "234/390 [=================>............] - ETA: 56s - loss: 2.5258 - acc: 0.2461 - LRFinder: val_loss: 3.3346 - lr = 0.00617564 \n",
            "235/390 [=================>............] - ETA: 56s - loss: 2.5247 - acc: 0.2467 - LRFinder: val_loss: 3.6158 - lr = 0.00620000 \n",
            "236/390 [=================>............] - ETA: 55s - loss: 2.5236 - acc: 0.2476 - LRFinder: val_loss: 3.4690 - lr = 0.00622436 \n",
            "237/390 [=================>............] - ETA: 55s - loss: 2.5225 - acc: 0.2481 - LRFinder: val_loss: 3.5143 - lr = 0.00624872 \n",
            "238/390 [=================>............] - ETA: 54s - loss: 2.5211 - acc: 0.2487 - LRFinder: val_loss: 3.4398 - lr = 0.00627308 \n",
            "239/390 [=================>............] - ETA: 54s - loss: 2.5205 - acc: 0.2488 - LRFinder: val_loss: 3.5331 - lr = 0.00629744 \n",
            "240/390 [=================>............] - ETA: 54s - loss: 2.5197 - acc: 0.2489 - LRFinder: val_loss: 3.5817 - lr = 0.00632179 \n",
            "241/390 [=================>............] - ETA: 53s - loss: 2.5185 - acc: 0.2493 - LRFinder: val_loss: 3.5271 - lr = 0.00634615 \n",
            "242/390 [=================>............] - ETA: 53s - loss: 2.5170 - acc: 0.2498 - LRFinder: val_loss: 3.6426 - lr = 0.00637051 \n",
            "243/390 [=================>............] - ETA: 53s - loss: 2.5159 - acc: 0.2502 - LRFinder: val_loss: 3.6727 - lr = 0.00639487 \n",
            "244/390 [=================>............] - ETA: 52s - loss: 2.5150 - acc: 0.2505 - LRFinder: val_loss: 3.5770 - lr = 0.00641923 \n",
            "245/390 [=================>............] - ETA: 52s - loss: 2.5139 - acc: 0.2506 - LRFinder: val_loss: 3.4820 - lr = 0.00644359 \n",
            "246/390 [=================>............] - ETA: 51s - loss: 2.5130 - acc: 0.2510 - LRFinder: val_loss: 3.5787 - lr = 0.00646795 \n",
            "247/390 [==================>...........] - ETA: 51s - loss: 2.5118 - acc: 0.2514 - LRFinder: val_loss: 3.5341 - lr = 0.00649231 \n",
            "248/390 [==================>...........] - ETA: 51s - loss: 2.5112 - acc: 0.2517 - LRFinder: val_loss: 3.5595 - lr = 0.00651667 \n",
            "249/390 [==================>...........] - ETA: 50s - loss: 2.5101 - acc: 0.2518 - LRFinder: val_loss: 3.6505 - lr = 0.00654103 \n",
            "250/390 [==================>...........] - ETA: 50s - loss: 2.5089 - acc: 0.2522 - LRFinder: val_loss: 3.5752 - lr = 0.00656538 \n",
            "251/390 [==================>...........] - ETA: 50s - loss: 2.5074 - acc: 0.2529 - LRFinder: val_loss: 3.5720 - lr = 0.00658974 \n",
            "252/390 [==================>...........] - ETA: 49s - loss: 2.5063 - acc: 0.2530 - LRFinder: val_loss: 3.5795 - lr = 0.00661410 \n",
            "253/390 [==================>...........] - ETA: 49s - loss: 2.5058 - acc: 0.2532 - LRFinder: val_loss: 3.6486 - lr = 0.00663846 \n",
            "254/390 [==================>...........] - ETA: 48s - loss: 2.5045 - acc: 0.2537 - LRFinder: val_loss: 3.5555 - lr = 0.00666282 \n",
            "255/390 [==================>...........] - ETA: 48s - loss: 2.5036 - acc: 0.2539 - LRFinder: val_loss: 3.5385 - lr = 0.00668718 \n",
            "256/390 [==================>...........] - ETA: 48s - loss: 2.5023 - acc: 0.2542 - LRFinder: val_loss: 3.5820 - lr = 0.00671154 \n",
            "257/390 [==================>...........] - ETA: 47s - loss: 2.5011 - acc: 0.2544 - LRFinder: val_loss: 3.6973 - lr = 0.00673590 \n",
            "258/390 [==================>...........] - ETA: 47s - loss: 2.5006 - acc: 0.2547 - LRFinder: val_loss: 3.7259 - lr = 0.00676026 \n",
            "259/390 [==================>...........] - ETA: 47s - loss: 2.4998 - acc: 0.2551 - LRFinder: val_loss: 3.8324 - lr = 0.00678462 \n",
            "260/390 [===================>..........] - ETA: 46s - loss: 2.4989 - acc: 0.2553 - LRFinder: val_loss: 3.7352 - lr = 0.00680897 \n",
            "261/390 [===================>..........] - ETA: 46s - loss: 2.4979 - acc: 0.2556 - LRFinder: val_loss: 3.8193 - lr = 0.00683333 \n",
            "262/390 [===================>..........] - ETA: 45s - loss: 2.4971 - acc: 0.2558 - LRFinder: val_loss: 3.6836 - lr = 0.00685769 \n",
            "263/390 [===================>..........] - ETA: 45s - loss: 2.4960 - acc: 0.2563 - LRFinder: val_loss: 3.7847 - lr = 0.00688205 \n",
            "264/390 [===================>..........] - ETA: 45s - loss: 2.4949 - acc: 0.2566 - LRFinder: val_loss: 3.7276 - lr = 0.00690641 \n",
            "265/390 [===================>..........] - ETA: 44s - loss: 2.4940 - acc: 0.2569 - LRFinder: val_loss: 3.8497 - lr = 0.00693077 \n",
            "266/390 [===================>..........] - ETA: 44s - loss: 2.4932 - acc: 0.2571 - LRFinder: val_loss: 3.8590 - lr = 0.00695513 \n",
            "267/390 [===================>..........] - ETA: 44s - loss: 2.4920 - acc: 0.2575 - LRFinder: val_loss: 3.7587 - lr = 0.00697949 \n",
            "268/390 [===================>..........] - ETA: 43s - loss: 2.4914 - acc: 0.2576 - LRFinder: val_loss: 3.8082 - lr = 0.00700385 \n",
            "269/390 [===================>..........] - ETA: 43s - loss: 2.4904 - acc: 0.2578 - LRFinder: val_loss: 3.7515 - lr = 0.00702821 \n",
            "270/390 [===================>..........] - ETA: 43s - loss: 2.4891 - acc: 0.2582 - LRFinder: val_loss: 3.8839 - lr = 0.00705256 \n",
            "271/390 [===================>..........] - ETA: 42s - loss: 2.4875 - acc: 0.2589 - LRFinder: val_loss: 3.6674 - lr = 0.00707692 \n",
            "272/390 [===================>..........] - ETA: 42s - loss: 2.4863 - acc: 0.2595 - LRFinder: val_loss: 3.7676 - lr = 0.00710128 \n",
            "273/390 [====================>.........] - ETA: 41s - loss: 2.4855 - acc: 0.2598 - LRFinder: val_loss: 3.6497 - lr = 0.00712564 \n",
            "274/390 [====================>.........] - ETA: 41s - loss: 2.4845 - acc: 0.2600 - LRFinder: val_loss: 3.7696 - lr = 0.00715000 \n",
            "275/390 [====================>.........] - ETA: 41s - loss: 2.4838 - acc: 0.2603 - LRFinder: val_loss: 3.8258 - lr = 0.00717436 \n",
            "276/390 [====================>.........] - ETA: 40s - loss: 2.4826 - acc: 0.2606 - LRFinder: val_loss: 3.8664 - lr = 0.00719872 \n",
            "277/390 [====================>.........] - ETA: 40s - loss: 2.4817 - acc: 0.2609 - LRFinder: val_loss: 3.8155 - lr = 0.00722308 \n",
            "278/390 [====================>.........] - ETA: 40s - loss: 2.4805 - acc: 0.2614 - LRFinder: val_loss: 3.8888 - lr = 0.00724744 \n",
            "279/390 [====================>.........] - ETA: 39s - loss: 2.4792 - acc: 0.2618 - LRFinder: val_loss: 3.7924 - lr = 0.00727179 \n",
            "280/390 [====================>.........] - ETA: 39s - loss: 2.4783 - acc: 0.2622 - LRFinder: val_loss: 3.6581 - lr = 0.00729615 \n",
            "281/390 [====================>.........] - ETA: 38s - loss: 2.4774 - acc: 0.2626 - LRFinder: val_loss: 3.7007 - lr = 0.00732051 \n",
            "282/390 [====================>.........] - ETA: 38s - loss: 2.4766 - acc: 0.2629 - LRFinder: val_loss: 3.7043 - lr = 0.00734487 \n",
            "283/390 [====================>.........] - ETA: 38s - loss: 2.4755 - acc: 0.2632 - LRFinder: val_loss: 3.7393 - lr = 0.00736923 \n",
            "284/390 [====================>.........] - ETA: 37s - loss: 2.4747 - acc: 0.2635 - LRFinder: val_loss: 3.7180 - lr = 0.00739359 \n",
            "285/390 [====================>.........] - ETA: 37s - loss: 2.4742 - acc: 0.2638 - LRFinder: val_loss: 3.7593 - lr = 0.00741795 \n",
            "286/390 [=====================>........] - ETA: 37s - loss: 2.4729 - acc: 0.2644 - LRFinder: val_loss: 3.7251 - lr = 0.00744231 \n",
            "287/390 [=====================>........] - ETA: 36s - loss: 2.4717 - acc: 0.2647 - LRFinder: val_loss: 3.6387 - lr = 0.00746667 \n",
            "288/390 [=====================>........] - ETA: 36s - loss: 2.4712 - acc: 0.2649 - LRFinder: val_loss: 3.5237 - lr = 0.00749103 \n",
            "289/390 [=====================>........] - ETA: 36s - loss: 2.4699 - acc: 0.2654 - LRFinder: val_loss: 3.6476 - lr = 0.00751538 \n",
            "290/390 [=====================>........] - ETA: 35s - loss: 2.4695 - acc: 0.2654 - LRFinder: val_loss: 3.7071 - lr = 0.00753974 \n",
            "291/390 [=====================>........] - ETA: 35s - loss: 2.4685 - acc: 0.2658 - LRFinder: val_loss: 3.7229 - lr = 0.00756410 \n",
            "292/390 [=====================>........] - ETA: 34s - loss: 2.4675 - acc: 0.2662 - LRFinder: val_loss: 3.6556 - lr = 0.00758846 \n",
            "293/390 [=====================>........] - ETA: 34s - loss: 2.4666 - acc: 0.2666 - LRFinder: val_loss: 3.8144 - lr = 0.00761282 \n",
            "294/390 [=====================>........] - ETA: 34s - loss: 2.4656 - acc: 0.2669 - LRFinder: val_loss: 3.6926 - lr = 0.00763718 \n",
            "295/390 [=====================>........] - ETA: 33s - loss: 2.4647 - acc: 0.2672 - LRFinder: val_loss: 3.8161 - lr = 0.00766154 \n",
            "296/390 [=====================>........] - ETA: 33s - loss: 2.4638 - acc: 0.2673 - LRFinder: val_loss: 3.9572 - lr = 0.00768590 \n",
            "297/390 [=====================>........] - ETA: 33s - loss: 2.4626 - acc: 0.2677 - LRFinder: val_loss: 3.7186 - lr = 0.00771026 \n",
            "298/390 [=====================>........] - ETA: 32s - loss: 2.4614 - acc: 0.2681 - LRFinder: val_loss: 3.7595 - lr = 0.00773462 \n",
            "299/390 [======================>.......] - ETA: 32s - loss: 2.4603 - acc: 0.2687 - LRFinder: val_loss: 3.7278 - lr = 0.00775897 \n",
            "300/390 [======================>.......] - ETA: 32s - loss: 2.4597 - acc: 0.2690 - LRFinder: val_loss: 3.7310 - lr = 0.00778333 \n",
            "301/390 [======================>.......] - ETA: 31s - loss: 2.4585 - acc: 0.2694 - LRFinder: val_loss: 3.7859 - lr = 0.00780769 \n",
            "302/390 [======================>.......] - ETA: 31s - loss: 2.4576 - acc: 0.2697 - LRFinder: val_loss: 3.7393 - lr = 0.00783205 \n",
            "303/390 [======================>.......] - ETA: 30s - loss: 2.4565 - acc: 0.2700 - LRFinder: val_loss: 3.8214 - lr = 0.00785641 \n",
            "304/390 [======================>.......] - ETA: 30s - loss: 2.4558 - acc: 0.2702 - LRFinder: val_loss: 3.9120 - lr = 0.00788077 \n",
            "305/390 [======================>.......] - ETA: 30s - loss: 2.4545 - acc: 0.2708 - LRFinder: val_loss: 3.9571 - lr = 0.00790513 \n",
            "306/390 [======================>.......] - ETA: 29s - loss: 2.4542 - acc: 0.2709 - LRFinder: val_loss: 3.6843 - lr = 0.00792949 \n",
            "307/390 [======================>.......] - ETA: 29s - loss: 2.4534 - acc: 0.2712 - LRFinder: val_loss: 3.9204 - lr = 0.00795385 \n",
            "308/390 [======================>.......] - ETA: 29s - loss: 2.4526 - acc: 0.2714 - LRFinder: val_loss: 3.8374 - lr = 0.00797821 \n",
            "309/390 [======================>.......] - ETA: 28s - loss: 2.4514 - acc: 0.2720 - LRFinder: val_loss: 3.9454 - lr = 0.00800256 \n",
            "310/390 [======================>.......] - ETA: 28s - loss: 2.4506 - acc: 0.2722 - LRFinder: val_loss: 3.7865 - lr = 0.00802692 \n",
            "311/390 [======================>.......] - ETA: 28s - loss: 2.4499 - acc: 0.2723 - LRFinder: val_loss: 3.7766 - lr = 0.00805128 \n",
            "312/390 [=======================>......] - ETA: 27s - loss: 2.4491 - acc: 0.2726 - LRFinder: val_loss: 3.7756 - lr = 0.00807564 \n",
            "313/390 [=======================>......] - ETA: 27s - loss: 2.4485 - acc: 0.2728 - LRFinder: val_loss: 3.9104 - lr = 0.00810000 \n",
            "314/390 [=======================>......] - ETA: 27s - loss: 2.4477 - acc: 0.2731 - LRFinder: val_loss: 3.9453 - lr = 0.00812436 \n",
            "315/390 [=======================>......] - ETA: 26s - loss: 2.4471 - acc: 0.2732 - LRFinder: val_loss: 3.7389 - lr = 0.00814872 \n",
            "316/390 [=======================>......] - ETA: 26s - loss: 2.4462 - acc: 0.2737 - LRFinder: val_loss: 3.9859 - lr = 0.00817308 \n",
            "317/390 [=======================>......] - ETA: 25s - loss: 2.4454 - acc: 0.2739 - LRFinder: val_loss: 3.8938 - lr = 0.00819744 \n",
            "318/390 [=======================>......] - ETA: 25s - loss: 2.4446 - acc: 0.2742 - LRFinder: val_loss: 4.2024 - lr = 0.00822179 \n",
            "319/390 [=======================>......] - ETA: 25s - loss: 2.4439 - acc: 0.2744 - LRFinder: val_loss: 3.9790 - lr = 0.00824615 \n",
            "320/390 [=======================>......] - ETA: 24s - loss: 2.4432 - acc: 0.2749 - LRFinder: val_loss: 4.1821 - lr = 0.00827051 \n",
            "321/390 [=======================>......] - ETA: 24s - loss: 2.4424 - acc: 0.2753 - LRFinder: val_loss: 3.9981 - lr = 0.00829487 \n",
            "322/390 [=======================>......] - ETA: 24s - loss: 2.4413 - acc: 0.2755 - LRFinder: val_loss: 3.9645 - lr = 0.00831923 \n",
            "323/390 [=======================>......] - ETA: 23s - loss: 2.4402 - acc: 0.2760 - LRFinder: val_loss: 3.9503 - lr = 0.00834359 \n",
            "324/390 [=======================>......] - ETA: 23s - loss: 2.4395 - acc: 0.2763 - LRFinder: val_loss: 4.0100 - lr = 0.00836795 \n",
            "325/390 [========================>.....] - ETA: 23s - loss: 2.4388 - acc: 0.2767 - LRFinder: val_loss: 4.0956 - lr = 0.00839231 \n",
            "326/390 [========================>.....] - ETA: 22s - loss: 2.4380 - acc: 0.2769 - LRFinder: val_loss: 3.9613 - lr = 0.00841667 \n",
            "327/390 [========================>.....] - ETA: 22s - loss: 2.4371 - acc: 0.2771 - LRFinder: val_loss: 3.9699 - lr = 0.00844103 \n",
            "328/390 [========================>.....] - ETA: 21s - loss: 2.4364 - acc: 0.2774 - LRFinder: val_loss: 3.9511 - lr = 0.00846538 \n",
            "329/390 [========================>.....] - ETA: 21s - loss: 2.4358 - acc: 0.2775 - LRFinder: val_loss: 4.1503 - lr = 0.00848974 \n",
            "330/390 [========================>.....] - ETA: 21s - loss: 2.4346 - acc: 0.2780 - LRFinder: val_loss: 3.8906 - lr = 0.00851410 \n",
            "331/390 [========================>.....] - ETA: 20s - loss: 2.4336 - acc: 0.2785 - LRFinder: val_loss: 3.9096 - lr = 0.00853846 \n",
            "332/390 [========================>.....] - ETA: 20s - loss: 2.4330 - acc: 0.2787 - LRFinder: val_loss: 3.8353 - lr = 0.00856282 \n",
            "333/390 [========================>.....] - ETA: 20s - loss: 2.4322 - acc: 0.2790 - LRFinder: val_loss: 3.8069 - lr = 0.00858718 \n",
            "334/390 [========================>.....] - ETA: 19s - loss: 2.4317 - acc: 0.2791 - LRFinder: val_loss: 3.7941 - lr = 0.00861154 \n",
            "335/390 [========================>.....] - ETA: 19s - loss: 2.4305 - acc: 0.2795 - LRFinder: val_loss: 3.7968 - lr = 0.00863590 \n",
            "336/390 [========================>.....] - ETA: 19s - loss: 2.4297 - acc: 0.2797 - LRFinder: val_loss: 3.8507 - lr = 0.00866026 \n",
            "337/390 [========================>.....] - ETA: 18s - loss: 2.4292 - acc: 0.2799 - LRFinder: val_loss: 3.9602 - lr = 0.00868462 \n",
            "338/390 [=========================>....] - ETA: 18s - loss: 2.4285 - acc: 0.2801 - LRFinder: val_loss: 3.8870 - lr = 0.00870897 \n",
            "339/390 [=========================>....] - ETA: 18s - loss: 2.4277 - acc: 0.2805 - LRFinder: val_loss: 3.7959 - lr = 0.00873333 \n",
            "340/390 [=========================>....] - ETA: 17s - loss: 2.4269 - acc: 0.2808 - LRFinder: val_loss: 3.7616 - lr = 0.00875769 \n",
            "341/390 [=========================>....] - ETA: 17s - loss: 2.4259 - acc: 0.2812 - LRFinder: val_loss: 3.8288 - lr = 0.00878205 \n",
            "342/390 [=========================>....] - ETA: 16s - loss: 2.4253 - acc: 0.2813 - LRFinder: val_loss: 3.9730 - lr = 0.00880641 \n",
            "343/390 [=========================>....] - ETA: 16s - loss: 2.4245 - acc: 0.2817 - LRFinder: val_loss: 3.6793 - lr = 0.00883077 \n",
            "344/390 [=========================>....] - ETA: 16s - loss: 2.4241 - acc: 0.2817 - LRFinder: val_loss: 3.9593 - lr = 0.00885513 \n",
            "345/390 [=========================>....] - ETA: 15s - loss: 2.4233 - acc: 0.2820 - LRFinder: val_loss: 3.9678 - lr = 0.00887949 \n",
            "346/390 [=========================>....] - ETA: 15s - loss: 2.4226 - acc: 0.2822 - LRFinder: val_loss: 3.8914 - lr = 0.00890385 \n",
            "347/390 [=========================>....] - ETA: 15s - loss: 2.4217 - acc: 0.2826 - LRFinder: val_loss: 3.8763 - lr = 0.00892821 \n",
            "348/390 [=========================>....] - ETA: 14s - loss: 2.4210 - acc: 0.2828 - LRFinder: val_loss: 3.8017 - lr = 0.00895256 \n",
            "349/390 [=========================>....] - ETA: 14s - loss: 2.4200 - acc: 0.2831 - LRFinder: val_loss: 3.7703 - lr = 0.00897692 \n",
            "350/390 [=========================>....] - ETA: 14s - loss: 2.4188 - acc: 0.2833 - LRFinder: val_loss: 3.8515 - lr = 0.00900128 \n",
            "351/390 [==========================>...] - ETA: 13s - loss: 2.4178 - acc: 0.2838 - LRFinder: val_loss: 3.9149 - lr = 0.00902564 \n",
            "352/390 [==========================>...] - ETA: 13s - loss: 2.4171 - acc: 0.2840 - LRFinder: val_loss: 3.6937 - lr = 0.00905000 \n",
            "353/390 [==========================>...] - ETA: 13s - loss: 2.4161 - acc: 0.2843 - LRFinder: val_loss: 3.8176 - lr = 0.00907436 \n",
            "354/390 [==========================>...] - ETA: 12s - loss: 2.4151 - acc: 0.2846 - LRFinder: val_loss: 3.7194 - lr = 0.00909872 \n",
            "355/390 [==========================>...] - ETA: 12s - loss: 2.4141 - acc: 0.2850 - LRFinder: val_loss: 3.8380 - lr = 0.00912308 \n",
            "356/390 [==========================>...] - ETA: 12s - loss: 2.4135 - acc: 0.2854 - LRFinder: val_loss: 3.8663 - lr = 0.00914744 \n",
            "357/390 [==========================>...] - ETA: 11s - loss: 2.4127 - acc: 0.2856 - LRFinder: val_loss: 3.7559 - lr = 0.00917179 \n",
            "358/390 [==========================>...] - ETA: 11s - loss: 2.4118 - acc: 0.2859 - LRFinder: val_loss: 3.8206 - lr = 0.00919615 \n",
            "359/390 [==========================>...] - ETA: 10s - loss: 2.4113 - acc: 0.2861 - LRFinder: val_loss: 3.7134 - lr = 0.00922051 \n",
            "360/390 [==========================>...] - ETA: 10s - loss: 2.4107 - acc: 0.2864 - LRFinder: val_loss: 3.6635 - lr = 0.00924487 \n",
            "361/390 [==========================>...] - ETA: 10s - loss: 2.4101 - acc: 0.2866 - LRFinder: val_loss: 3.5536 - lr = 0.00926923 \n",
            "362/390 [==========================>...] - ETA: 9s - loss: 2.4096 - acc: 0.2868  - LRFinder: val_loss: 3.5265 - lr = 0.00929359 \n",
            "363/390 [==========================>...] - ETA: 9s - loss: 2.4084 - acc: 0.2872 - LRFinder: val_loss: 3.5358 - lr = 0.00931795 \n",
            "364/390 [===========================>..] - ETA: 9s - loss: 2.4078 - acc: 0.2874 - LRFinder: val_loss: 3.5257 - lr = 0.00934231 \n",
            "365/390 [===========================>..] - ETA: 8s - loss: 2.4070 - acc: 0.2876 - LRFinder: val_loss: 3.6885 - lr = 0.00936667 \n",
            "366/390 [===========================>..] - ETA: 8s - loss: 2.4058 - acc: 0.2881 - LRFinder: val_loss: 3.5232 - lr = 0.00939103 \n",
            "367/390 [===========================>..] - ETA: 8s - loss: 2.4051 - acc: 0.2884 - LRFinder: val_loss: 3.8335 - lr = 0.00941538 \n",
            "368/390 [===========================>..] - ETA: 7s - loss: 2.4048 - acc: 0.2886 - LRFinder: val_loss: 3.6410 - lr = 0.00943974 \n",
            "369/390 [===========================>..] - ETA: 7s - loss: 2.4042 - acc: 0.2889 - LRFinder: val_loss: 3.7527 - lr = 0.00946410 \n",
            "370/390 [===========================>..] - ETA: 7s - loss: 2.4033 - acc: 0.2893 - LRFinder: val_loss: 3.4407 - lr = 0.00948846 \n",
            "371/390 [===========================>..] - ETA: 6s - loss: 2.4024 - acc: 0.2895 - LRFinder: val_loss: 3.7890 - lr = 0.00951282 \n",
            "372/390 [===========================>..] - ETA: 6s - loss: 2.4018 - acc: 0.2898 - LRFinder: val_loss: 3.6894 - lr = 0.00953718 \n",
            "373/390 [===========================>..] - ETA: 5s - loss: 2.4008 - acc: 0.2902 - LRFinder: val_loss: 3.7945 - lr = 0.00956154 \n",
            "374/390 [===========================>..] - ETA: 5s - loss: 2.4000 - acc: 0.2906 - LRFinder: val_loss: 3.7189 - lr = 0.00958590 \n",
            "375/390 [===========================>..] - ETA: 5s - loss: 2.3993 - acc: 0.2908 - LRFinder: val_loss: 3.5348 - lr = 0.00961026 \n",
            "376/390 [===========================>..] - ETA: 4s - loss: 2.3985 - acc: 0.2910 - LRFinder: val_loss: 3.6324 - lr = 0.00963462 \n",
            "377/390 [============================>.] - ETA: 4s - loss: 2.3978 - acc: 0.2911 - LRFinder: val_loss: 3.6674 - lr = 0.00965897 \n",
            "378/390 [============================>.] - ETA: 4s - loss: 2.3967 - acc: 0.2916 - LRFinder: val_loss: 3.6173 - lr = 0.00968333 \n",
            "379/390 [============================>.] - ETA: 3s - loss: 2.3958 - acc: 0.2918 - LRFinder: val_loss: 3.6658 - lr = 0.00970769 \n",
            "380/390 [============================>.] - ETA: 3s - loss: 2.3950 - acc: 0.2921 - LRFinder: val_loss: 3.4809 - lr = 0.00973205 \n",
            "381/390 [============================>.] - ETA: 3s - loss: 2.3942 - acc: 0.2923 - LRFinder: val_loss: 3.6871 - lr = 0.00975641 \n",
            "382/390 [============================>.] - ETA: 2s - loss: 2.3936 - acc: 0.2926 - LRFinder: val_loss: 3.8756 - lr = 0.00978077 \n",
            "383/390 [============================>.] - ETA: 2s - loss: 2.3927 - acc: 0.2929 - LRFinder: val_loss: 3.8992 - lr = 0.00980513 \n",
            "384/390 [============================>.] - ETA: 2s - loss: 2.3917 - acc: 0.2932 - LRFinder: val_loss: 3.8574 - lr = 0.00982949 \n",
            "385/390 [============================>.] - ETA: 1s - loss: 2.3912 - acc: 0.2934 - LRFinder: val_loss: 3.7756 - lr = 0.00985385 \n",
            "386/390 [============================>.] - ETA: 1s - loss: 2.3905 - acc: 0.2936 - LRFinder: val_loss: 3.9461 - lr = 0.00987821 \n",
            "387/390 [============================>.] - ETA: 1s - loss: 2.3897 - acc: 0.2939 - LRFinder: val_loss: 3.9776 - lr = 0.00990256 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 2.3889 - acc: 0.2942 - LRFinder: val_loss: 3.9455 - lr = 0.00992692 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 2.3883 - acc: 0.2946 - LRFinder: val_loss: 3.8408 - lr = 0.00995128 \n",
            " - LRFinder: val_loss: 4.0175 - lr = 0.00997564 \n",
            "390/390 [==============================] - 140s 360ms/step - loss: 2.3877 - acc: 0.2947 - val_loss: 3.8840 - val_acc: 0.1956\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {weights/}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAESCAYAAAAR2wXeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 3s 313us/step\n",
            "loss : 3.8840\n",
            "acc : 0.1956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvdluHr_ibMV",
        "colab_type": "text"
      },
      "source": [
        "#BEST LR IS 0.0018"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_vRk5eWdW8a",
        "colab_type": "code",
        "outputId": "3c3618f8-7392-4d81-931b-597d563c0e2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "MOMENTUMS = [0.9, 0.95, 0.99]\n",
        "nb_epoch = 1\n",
        "\n",
        "for momentum in MOMENTUMS:\n",
        "#     # Learning rate range obtained from `find_lr_schedule.py`\n",
        "#     # NOTE : Minimum is 10x smaller than the max found above !\n",
        "#     # NOTE : It is preferable to use the validation data here to get a correct value\n",
        "      lr_finder = LRFinder(n_train, BATCH_SIZE, minimum_lr=0.0018, maximum_lr=1e-2,\n",
        "                          validation_data=(X_test, Y_test),\n",
        "                          validation_sample_rate=5,\n",
        "                          lr_scale='linear', save_dir='weights/momentum/momentum-%s/' % str(momentum),\n",
        "                          verbose=True)\n",
        "\n",
        "#     # set the weight_decay here !\n",
        "#     # lr doesnt matter as it will be over written by the callback\n",
        "      optimizer = SGD(lr=0.0018, momentum=momentum, nesterov=True)\n",
        "      model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "#         # Fit the model on the batches generated by datagen.flow().\n",
        "      model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE, shuffle=True),\n",
        "                             steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
        "                             validation_data=(X_test, Y_test),\n",
        "                             epochs=nb_epoch, verbose=1,\n",
        "                             callbacks=[lr_finder])\n",
        "\n",
        "# from plot we see, the model isnt impacted by the weight_decay very much at all\n",
        "# so we can use any of them.\n",
        "\n",
        "for momentum in MOMENTUMS:\n",
        "    directory = 'weights/momentum/momentum-%s/' % str(momentum)\n",
        "\n",
        "    losses, lrs = LRFinder.restore_schedule_from_dir(directory, 10, 5)\n",
        "    plt.plot(lrs, losses, label='momentum=%0.2f' % momentum)\n",
        "\n",
        "plt.title(\"Momentum\")\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 15:17 - loss: 2.1587 - acc: 0.4297 - LRFinder: val_loss: 3.7151 - lr = 0.00180000 \n",
            "  2/390 [..............................] - ETA: 10:12 - loss: 2.1272 - acc: 0.4023 - LRFinder: val_loss: 3.8091 - lr = 0.00182103 \n",
            "  3/390 [..............................] - ETA: 7:34 - loss: 2.1035 - acc: 0.4167  - LRFinder: val_loss: 3.9199 - lr = 0.00184205 \n",
            "  4/390 [..............................] - ETA: 6:13 - loss: 2.0869 - acc: 0.4082 - LRFinder: val_loss: 3.8381 - lr = 0.00186308 \n",
            "  5/390 [..............................] - ETA: 5:24 - loss: 2.0960 - acc: 0.3984 - LRFinder: val_loss: 3.7715 - lr = 0.00188410 \n",
            "  6/390 [..............................] - ETA: 4:52 - loss: 2.0901 - acc: 0.4036 - LRFinder: val_loss: 3.9028 - lr = 0.00190513 \n",
            "  7/390 [..............................] - ETA: 4:28 - loss: 2.0889 - acc: 0.4085 - LRFinder: val_loss: 3.7410 - lr = 0.00192615 \n",
            "  8/390 [..............................] - ETA: 4:11 - loss: 2.0897 - acc: 0.4053 - LRFinder: val_loss: 3.8305 - lr = 0.00194718 \n",
            "  9/390 [..............................] - ETA: 3:57 - loss: 2.0934 - acc: 0.4062 - LRFinder: val_loss: 3.7783 - lr = 0.00196821 \n",
            " 10/390 [..............................] - ETA: 3:45 - loss: 2.0933 - acc: 0.4047 - LRFinder: val_loss: 3.8490 - lr = 0.00198923 \n",
            " 11/390 [..............................] - ETA: 3:36 - loss: 2.0924 - acc: 0.4013 - LRFinder: val_loss: 3.7250 - lr = 0.00201026 \n",
            " 12/390 [..............................] - ETA: 3:28 - loss: 2.0883 - acc: 0.4036 - LRFinder: val_loss: 3.8693 - lr = 0.00203128 \n",
            " 13/390 [>.............................] - ETA: 3:22 - loss: 2.0936 - acc: 0.4014 - LRFinder: val_loss: 3.9259 - lr = 0.00205231 \n",
            " 14/390 [>.............................] - ETA: 3:16 - loss: 2.0930 - acc: 0.3968 - LRFinder: val_loss: 3.9092 - lr = 0.00207333 \n",
            " 15/390 [>.............................] - ETA: 3:11 - loss: 2.0960 - acc: 0.3964 - LRFinder: val_loss: 4.0015 - lr = 0.00209436 \n",
            " 16/390 [>.............................] - ETA: 3:07 - loss: 2.0984 - acc: 0.3955 - LRFinder: val_loss: 3.7871 - lr = 0.00211538 \n",
            " 17/390 [>.............................] - ETA: 3:03 - loss: 2.0960 - acc: 0.3952 - LRFinder: val_loss: 4.0550 - lr = 0.00213641 \n",
            " 18/390 [>.............................] - ETA: 2:59 - loss: 2.0998 - acc: 0.3932 - LRFinder: val_loss: 4.0044 - lr = 0.00215744 \n",
            " 19/390 [>.............................] - ETA: 2:56 - loss: 2.1033 - acc: 0.3923 - LRFinder: val_loss: 3.9564 - lr = 0.00217846 \n",
            " 20/390 [>.............................] - ETA: 2:53 - loss: 2.1006 - acc: 0.3930 - LRFinder: val_loss: 4.0445 - lr = 0.00219949 \n",
            " 21/390 [>.............................] - ETA: 2:50 - loss: 2.1038 - acc: 0.3906 - LRFinder: val_loss: 3.8433 - lr = 0.00222051 \n",
            " 22/390 [>.............................] - ETA: 2:48 - loss: 2.1016 - acc: 0.3910 - LRFinder: val_loss: 3.9564 - lr = 0.00224154 \n",
            " 23/390 [>.............................] - ETA: 2:45 - loss: 2.1031 - acc: 0.3896 - LRFinder: val_loss: 3.7566 - lr = 0.00226256 \n",
            " 24/390 [>.............................] - ETA: 2:43 - loss: 2.0998 - acc: 0.3923 - LRFinder: val_loss: 3.8392 - lr = 0.00228359 \n",
            " 25/390 [>.............................] - ETA: 2:41 - loss: 2.0982 - acc: 0.3947 - LRFinder: val_loss: 3.8468 - lr = 0.00230462 \n",
            " 26/390 [=>............................] - ETA: 2:39 - loss: 2.1056 - acc: 0.3945 - LRFinder: val_loss: 3.8643 - lr = 0.00232564 \n",
            " 27/390 [=>............................] - ETA: 2:38 - loss: 2.1066 - acc: 0.3938 - LRFinder: val_loss: 4.0388 - lr = 0.00234667 \n",
            " 28/390 [=>............................] - ETA: 2:36 - loss: 2.1035 - acc: 0.3931 - LRFinder: val_loss: 3.7913 - lr = 0.00236769 \n",
            " 29/390 [=>............................] - ETA: 2:34 - loss: 2.1003 - acc: 0.3939 - LRFinder: val_loss: 3.8805 - lr = 0.00238872 \n",
            " 30/390 [=>............................] - ETA: 2:33 - loss: 2.0945 - acc: 0.3969 - LRFinder: val_loss: 3.7723 - lr = 0.00240974 \n",
            " 31/390 [=>............................] - ETA: 2:31 - loss: 2.0965 - acc: 0.3974 - LRFinder: val_loss: 3.7749 - lr = 0.00243077 \n",
            " 32/390 [=>............................] - ETA: 2:30 - loss: 2.0991 - acc: 0.3970 - LRFinder: val_loss: 3.8891 - lr = 0.00245179 \n",
            " 33/390 [=>............................] - ETA: 2:29 - loss: 2.0973 - acc: 0.3975 - LRFinder: val_loss: 3.7140 - lr = 0.00247282 \n",
            " 34/390 [=>............................] - ETA: 2:28 - loss: 2.0988 - acc: 0.3980 - LRFinder: val_loss: 3.8220 - lr = 0.00249385 \n",
            " 35/390 [=>............................] - ETA: 2:26 - loss: 2.0968 - acc: 0.4004 - LRFinder: val_loss: 3.8037 - lr = 0.00251487 \n",
            " 36/390 [=>............................] - ETA: 2:25 - loss: 2.0937 - acc: 0.4032 - LRFinder: val_loss: 3.7013 - lr = 0.00253590 \n",
            " 37/390 [=>............................] - ETA: 2:24 - loss: 2.0910 - acc: 0.4048 - LRFinder: val_loss: 3.7349 - lr = 0.00255692 \n",
            " 38/390 [=>............................] - ETA: 2:23 - loss: 2.0918 - acc: 0.4036 - LRFinder: val_loss: 3.7656 - lr = 0.00257795 \n",
            " 39/390 [==>...........................] - ETA: 2:22 - loss: 2.0921 - acc: 0.4034 - LRFinder: val_loss: 3.6424 - lr = 0.00259897 \n",
            " 40/390 [==>...........................] - ETA: 2:21 - loss: 2.0890 - acc: 0.4041 - LRFinder: val_loss: 3.8065 - lr = 0.00262000 \n",
            " 41/390 [==>...........................] - ETA: 2:20 - loss: 2.0883 - acc: 0.4049 - LRFinder: val_loss: 3.8814 - lr = 0.00264103 \n",
            " 42/390 [==>...........................] - ETA: 2:20 - loss: 2.0881 - acc: 0.4048 - LRFinder: val_loss: 4.0816 - lr = 0.00266205 \n",
            " 43/390 [==>...........................] - ETA: 2:19 - loss: 2.0887 - acc: 0.4048 - LRFinder: val_loss: 4.0215 - lr = 0.00268308 \n",
            " 44/390 [==>...........................] - ETA: 2:18 - loss: 2.0887 - acc: 0.4048 - LRFinder: val_loss: 3.9708 - lr = 0.00270410 \n",
            " 45/390 [==>...........................] - ETA: 2:17 - loss: 2.0871 - acc: 0.4052 - LRFinder: val_loss: 3.8265 - lr = 0.00272513 \n",
            " 46/390 [==>...........................] - ETA: 2:16 - loss: 2.0850 - acc: 0.4066 - LRFinder: val_loss: 3.7996 - lr = 0.00274615 \n",
            " 47/390 [==>...........................] - ETA: 2:16 - loss: 2.0866 - acc: 0.4056 - LRFinder: val_loss: 3.7842 - lr = 0.00276718 \n",
            " 48/390 [==>...........................] - ETA: 2:15 - loss: 2.0877 - acc: 0.4054 - LRFinder: val_loss: 3.8251 - lr = 0.00278821 \n",
            " 49/390 [==>...........................] - ETA: 2:14 - loss: 2.0889 - acc: 0.4047 - LRFinder: val_loss: 3.7611 - lr = 0.00280923 \n",
            " 50/390 [==>...........................] - ETA: 2:13 - loss: 2.0875 - acc: 0.4050 - LRFinder: val_loss: 3.9036 - lr = 0.00283026 \n",
            " 51/390 [==>...........................] - ETA: 2:12 - loss: 2.0901 - acc: 0.4046 - LRFinder: val_loss: 3.7730 - lr = 0.00285128 \n",
            " 52/390 [===>..........................] - ETA: 2:12 - loss: 2.0899 - acc: 0.4040 - LRFinder: val_loss: 3.9772 - lr = 0.00287231 \n",
            " 53/390 [===>..........................] - ETA: 2:11 - loss: 2.0913 - acc: 0.4030 - LRFinder: val_loss: 3.8035 - lr = 0.00289333 \n",
            " 54/390 [===>..........................] - ETA: 2:10 - loss: 2.0898 - acc: 0.4044 - LRFinder: val_loss: 3.7749 - lr = 0.00291436 \n",
            " 55/390 [===>..........................] - ETA: 2:10 - loss: 2.0878 - acc: 0.4057 - LRFinder: val_loss: 3.6926 - lr = 0.00293538 \n",
            " 56/390 [===>..........................] - ETA: 2:09 - loss: 2.0879 - acc: 0.4050 - LRFinder: val_loss: 3.6738 - lr = 0.00295641 \n",
            " 57/390 [===>..........................] - ETA: 2:08 - loss: 2.0888 - acc: 0.4050 - LRFinder: val_loss: 3.8548 - lr = 0.00297744 \n",
            " 58/390 [===>..........................] - ETA: 2:08 - loss: 2.0879 - acc: 0.4052 - LRFinder: val_loss: 3.7081 - lr = 0.00299846 \n",
            " 59/390 [===>..........................] - ETA: 2:07 - loss: 2.0874 - acc: 0.4053 - LRFinder: val_loss: 3.7667 - lr = 0.00301949 \n",
            " 60/390 [===>..........................] - ETA: 2:07 - loss: 2.0876 - acc: 0.4046 - LRFinder: val_loss: 3.7208 - lr = 0.00304051 \n",
            " 61/390 [===>..........................] - ETA: 2:06 - loss: 2.0875 - acc: 0.4036 - LRFinder: val_loss: 3.7605 - lr = 0.00306154 \n",
            " 62/390 [===>..........................] - ETA: 2:05 - loss: 2.0882 - acc: 0.4031 - LRFinder: val_loss: 3.8194 - lr = 0.00308256 \n",
            " 63/390 [===>..........................] - ETA: 2:05 - loss: 2.0885 - acc: 0.4034 - LRFinder: val_loss: 3.6971 - lr = 0.00310359 \n",
            " 64/390 [===>..........................] - ETA: 2:04 - loss: 2.0890 - acc: 0.4039 - LRFinder: val_loss: 3.7262 - lr = 0.00312462 \n",
            " 65/390 [====>.........................] - ETA: 2:04 - loss: 2.0876 - acc: 0.4046 - LRFinder: val_loss: 3.7912 - lr = 0.00314564 \n",
            " 66/390 [====>.........................] - ETA: 2:03 - loss: 2.0886 - acc: 0.4046 - LRFinder: val_loss: 3.8958 - lr = 0.00316667 \n",
            " 67/390 [====>.........................] - ETA: 2:02 - loss: 2.0886 - acc: 0.4059 - LRFinder: val_loss: 3.7460 - lr = 0.00318769 \n",
            " 68/390 [====>.........................] - ETA: 2:02 - loss: 2.0895 - acc: 0.4053 - LRFinder: val_loss: 3.7776 - lr = 0.00320872 \n",
            " 69/390 [====>.........................] - ETA: 2:01 - loss: 2.0869 - acc: 0.4062 - LRFinder: val_loss: 3.8735 - lr = 0.00322974 \n",
            " 70/390 [====>.........................] - ETA: 2:01 - loss: 2.0864 - acc: 0.4064 - LRFinder: val_loss: 3.9807 - lr = 0.00325077 \n",
            " 71/390 [====>.........................] - ETA: 2:00 - loss: 2.0874 - acc: 0.4058 - LRFinder: val_loss: 3.9104 - lr = 0.00327179 \n",
            " 72/390 [====>.........................] - ETA: 2:00 - loss: 2.0873 - acc: 0.4058 - LRFinder: val_loss: 3.9532 - lr = 0.00329282 \n",
            " 73/390 [====>.........................] - ETA: 1:59 - loss: 2.0862 - acc: 0.4066 - LRFinder: val_loss: 3.8454 - lr = 0.00331385 \n",
            " 74/390 [====>.........................] - ETA: 1:59 - loss: 2.0846 - acc: 0.4070 - LRFinder: val_loss: 3.9610 - lr = 0.00333487 \n",
            " 75/390 [====>.........................] - ETA: 1:58 - loss: 2.0836 - acc: 0.4077 - LRFinder: val_loss: 3.8558 - lr = 0.00335590 \n",
            " 76/390 [====>.........................] - ETA: 1:58 - loss: 2.0836 - acc: 0.4071 - LRFinder: val_loss: 3.8033 - lr = 0.00337692 \n",
            " 77/390 [====>.........................] - ETA: 1:57 - loss: 2.0831 - acc: 0.4074 - LRFinder: val_loss: 3.8134 - lr = 0.00339795 \n",
            " 78/390 [=====>........................] - ETA: 1:57 - loss: 2.0828 - acc: 0.4078 - LRFinder: val_loss: 3.7436 - lr = 0.00341897 \n",
            " 79/390 [=====>........................] - ETA: 1:56 - loss: 2.0828 - acc: 0.4079 - LRFinder: val_loss: 3.8692 - lr = 0.00344000 \n",
            " 80/390 [=====>........................] - ETA: 1:56 - loss: 2.0823 - acc: 0.4080 - LRFinder: val_loss: 3.8325 - lr = 0.00346103 \n",
            " 81/390 [=====>........................] - ETA: 1:55 - loss: 2.0814 - acc: 0.4084 - LRFinder: val_loss: 3.9377 - lr = 0.00348205 \n",
            " 82/390 [=====>........................] - ETA: 1:55 - loss: 2.0797 - acc: 0.4090 - LRFinder: val_loss: 3.6661 - lr = 0.00350308 \n",
            " 83/390 [=====>........................] - ETA: 1:54 - loss: 2.0788 - acc: 0.4099 - LRFinder: val_loss: 3.7889 - lr = 0.00352410 \n",
            " 84/390 [=====>........................] - ETA: 1:54 - loss: 2.0798 - acc: 0.4101 - LRFinder: val_loss: 3.7424 - lr = 0.00354513 \n",
            " 85/390 [=====>........................] - ETA: 1:53 - loss: 2.0793 - acc: 0.4102 - LRFinder: val_loss: 3.6576 - lr = 0.00356615 \n",
            " 86/390 [=====>........................] - ETA: 1:53 - loss: 2.0779 - acc: 0.4105 - LRFinder: val_loss: 3.7944 - lr = 0.00358718 \n",
            " 87/390 [=====>........................] - ETA: 1:52 - loss: 2.0776 - acc: 0.4099 - LRFinder: val_loss: 3.8001 - lr = 0.00360821 \n",
            " 88/390 [=====>........................] - ETA: 1:52 - loss: 2.0767 - acc: 0.4100 - LRFinder: val_loss: 3.6620 - lr = 0.00362923 \n",
            " 89/390 [=====>........................] - ETA: 1:51 - loss: 2.0761 - acc: 0.4099 - LRFinder: val_loss: 3.7829 - lr = 0.00365026 \n",
            " 90/390 [=====>........................] - ETA: 1:51 - loss: 2.0738 - acc: 0.4111 - LRFinder: val_loss: 3.8682 - lr = 0.00367128 \n",
            " 91/390 [======>.......................] - ETA: 1:50 - loss: 2.0710 - acc: 0.4129 - LRFinder: val_loss: 3.9533 - lr = 0.00369231 \n",
            " 92/390 [======>.......................] - ETA: 1:50 - loss: 2.0710 - acc: 0.4132 - LRFinder: val_loss: 3.8562 - lr = 0.00371333 \n",
            " 93/390 [======>.......................] - ETA: 1:49 - loss: 2.0699 - acc: 0.4136 - LRFinder: val_loss: 3.9106 - lr = 0.00373436 \n",
            " 94/390 [======>.......................] - ETA: 1:49 - loss: 2.0694 - acc: 0.4135 - LRFinder: val_loss: 3.9193 - lr = 0.00375538 \n",
            " 95/390 [======>.......................] - ETA: 1:48 - loss: 2.0699 - acc: 0.4136 - LRFinder: val_loss: 3.7095 - lr = 0.00377641 \n",
            " 96/390 [======>.......................] - ETA: 1:48 - loss: 2.0699 - acc: 0.4135 - LRFinder: val_loss: 3.8090 - lr = 0.00379744 \n",
            " 97/390 [======>.......................] - ETA: 1:48 - loss: 2.0692 - acc: 0.4137 - LRFinder: val_loss: 3.8198 - lr = 0.00381846 \n",
            " 98/390 [======>.......................] - ETA: 1:47 - loss: 2.0693 - acc: 0.4142 - LRFinder: val_loss: 3.8601 - lr = 0.00383949 \n",
            " 99/390 [======>.......................] - ETA: 1:47 - loss: 2.0699 - acc: 0.4140 - LRFinder: val_loss: 3.8611 - lr = 0.00386051 \n",
            "100/390 [======>.......................] - ETA: 1:46 - loss: 2.0684 - acc: 0.4148 - LRFinder: val_loss: 3.8572 - lr = 0.00388154 \n",
            "101/390 [======>.......................] - ETA: 1:46 - loss: 2.0668 - acc: 0.4160 - LRFinder: val_loss: 3.8861 - lr = 0.00390256 \n",
            "102/390 [======>.......................] - ETA: 1:45 - loss: 2.0667 - acc: 0.4159 - LRFinder: val_loss: 3.8142 - lr = 0.00392359 \n",
            "103/390 [======>.......................] - ETA: 1:45 - loss: 2.0659 - acc: 0.4162 - LRFinder: val_loss: 3.8271 - lr = 0.00394462 \n",
            "104/390 [=======>......................] - ETA: 1:44 - loss: 2.0659 - acc: 0.4157 - LRFinder: val_loss: 3.7826 - lr = 0.00396564 \n",
            "105/390 [=======>......................] - ETA: 1:44 - loss: 2.0651 - acc: 0.4164 - LRFinder: val_loss: 3.8449 - lr = 0.00398667 \n",
            "106/390 [=======>......................] - ETA: 1:44 - loss: 2.0652 - acc: 0.4163 - LRFinder: val_loss: 4.0090 - lr = 0.00400769 \n",
            "107/390 [=======>......................] - ETA: 1:43 - loss: 2.0653 - acc: 0.4162 - LRFinder: val_loss: 3.8215 - lr = 0.00402872 \n",
            "108/390 [=======>......................] - ETA: 1:43 - loss: 2.0664 - acc: 0.4157 - LRFinder: val_loss: 3.8299 - lr = 0.00404974 \n",
            "109/390 [=======>......................] - ETA: 1:42 - loss: 2.0662 - acc: 0.4160 - LRFinder: val_loss: 3.8017 - lr = 0.00407077 \n",
            "110/390 [=======>......................] - ETA: 1:42 - loss: 2.0664 - acc: 0.4161 - LRFinder: val_loss: 3.9618 - lr = 0.00409179 \n",
            "111/390 [=======>......................] - ETA: 1:41 - loss: 2.0655 - acc: 0.4168 - LRFinder: val_loss: 3.9713 - lr = 0.00411282 \n",
            "112/390 [=======>......................] - ETA: 1:41 - loss: 2.0658 - acc: 0.4170 - LRFinder: val_loss: 3.7897 - lr = 0.00413385 \n",
            "113/390 [=======>......................] - ETA: 1:41 - loss: 2.0663 - acc: 0.4169 - LRFinder: val_loss: 3.9357 - lr = 0.00415487 \n",
            "114/390 [=======>......................] - ETA: 1:40 - loss: 2.0671 - acc: 0.4165 - LRFinder: val_loss: 3.7046 - lr = 0.00417590 \n",
            "115/390 [=======>......................] - ETA: 1:40 - loss: 2.0662 - acc: 0.4166 - LRFinder: val_loss: 3.6902 - lr = 0.00419692 \n",
            "116/390 [=======>......................] - ETA: 1:39 - loss: 2.0661 - acc: 0.4167 - LRFinder: val_loss: 3.8676 - lr = 0.00421795 \n",
            "117/390 [========>.....................] - ETA: 1:39 - loss: 2.0674 - acc: 0.4163 - LRFinder: val_loss: 3.8555 - lr = 0.00423897 \n",
            "118/390 [========>.....................] - ETA: 1:38 - loss: 2.0668 - acc: 0.4164 - LRFinder: val_loss: 3.8182 - lr = 0.00426000 \n",
            "119/390 [========>.....................] - ETA: 1:38 - loss: 2.0665 - acc: 0.4164 - LRFinder: val_loss: 3.7343 - lr = 0.00428103 \n",
            "120/390 [========>.....................] - ETA: 1:38 - loss: 2.0664 - acc: 0.4166 - LRFinder: val_loss: 3.9300 - lr = 0.00430205 \n",
            "121/390 [========>.....................] - ETA: 1:37 - loss: 2.0654 - acc: 0.4172 - LRFinder: val_loss: 3.7187 - lr = 0.00432308 \n",
            "122/390 [========>.....................] - ETA: 1:37 - loss: 2.0643 - acc: 0.4178 - LRFinder: val_loss: 3.9286 - lr = 0.00434410 \n",
            "123/390 [========>.....................] - ETA: 1:36 - loss: 2.0636 - acc: 0.4183 - LRFinder: val_loss: 3.8462 - lr = 0.00436513 \n",
            "124/390 [========>.....................] - ETA: 1:36 - loss: 2.0619 - acc: 0.4186 - LRFinder: val_loss: 3.7704 - lr = 0.00438615 \n",
            "125/390 [========>.....................] - ETA: 1:36 - loss: 2.0605 - acc: 0.4194 - LRFinder: val_loss: 3.8753 - lr = 0.00440718 \n",
            "126/390 [========>.....................] - ETA: 1:35 - loss: 2.0601 - acc: 0.4194 - LRFinder: val_loss: 3.9267 - lr = 0.00442821 \n",
            "127/390 [========>.....................] - ETA: 1:35 - loss: 2.0597 - acc: 0.4197 - LRFinder: val_loss: 3.7266 - lr = 0.00444923 \n",
            "128/390 [========>.....................] - ETA: 1:34 - loss: 2.0597 - acc: 0.4198 - LRFinder: val_loss: 3.6527 - lr = 0.00447026 \n",
            "129/390 [========>.....................] - ETA: 1:34 - loss: 2.0597 - acc: 0.4199 - LRFinder: val_loss: 3.7729 - lr = 0.00449128 \n",
            "130/390 [=========>....................] - ETA: 1:34 - loss: 2.0599 - acc: 0.4201 - LRFinder: val_loss: 3.7349 - lr = 0.00451231 \n",
            "131/390 [=========>....................] - ETA: 1:33 - loss: 2.0592 - acc: 0.4201 - LRFinder: val_loss: 3.8985 - lr = 0.00453333 \n",
            "132/390 [=========>....................] - ETA: 1:33 - loss: 2.0592 - acc: 0.4202 - LRFinder: val_loss: 3.9145 - lr = 0.00455436 \n",
            "133/390 [=========>....................] - ETA: 1:32 - loss: 2.0596 - acc: 0.4199 - LRFinder: val_loss: 3.6246 - lr = 0.00457538 \n",
            "134/390 [=========>....................] - ETA: 1:32 - loss: 2.0599 - acc: 0.4198 - LRFinder: val_loss: 3.7709 - lr = 0.00459641 \n",
            "135/390 [=========>....................] - ETA: 1:32 - loss: 2.0605 - acc: 0.4196 - LRFinder: val_loss: 3.9504 - lr = 0.00461744 \n",
            "136/390 [=========>....................] - ETA: 1:31 - loss: 2.0612 - acc: 0.4191 - LRFinder: val_loss: 3.8624 - lr = 0.00463846 \n",
            "137/390 [=========>....................] - ETA: 1:31 - loss: 2.0611 - acc: 0.4193 - LRFinder: val_loss: 3.7356 - lr = 0.00465949 \n",
            "138/390 [=========>....................] - ETA: 1:30 - loss: 2.0612 - acc: 0.4193 - LRFinder: val_loss: 3.8042 - lr = 0.00468051 \n",
            "139/390 [=========>....................] - ETA: 1:30 - loss: 2.0615 - acc: 0.4191 - LRFinder: val_loss: 3.9066 - lr = 0.00470154 \n",
            "140/390 [=========>....................] - ETA: 1:30 - loss: 2.0626 - acc: 0.4190 - LRFinder: val_loss: 3.7631 - lr = 0.00472256 \n",
            "141/390 [=========>....................] - ETA: 1:29 - loss: 2.0636 - acc: 0.4183 - LRFinder: val_loss: 3.6321 - lr = 0.00474359 \n",
            "142/390 [=========>....................] - ETA: 1:29 - loss: 2.0635 - acc: 0.4182 - LRFinder: val_loss: 3.6055 - lr = 0.00476462 \n",
            "143/390 [==========>...................] - ETA: 1:28 - loss: 2.0632 - acc: 0.4183 - LRFinder: val_loss: 3.7214 - lr = 0.00478564 \n",
            "144/390 [==========>...................] - ETA: 1:28 - loss: 2.0636 - acc: 0.4182 - LRFinder: val_loss: 3.7228 - lr = 0.00480667 \n",
            "145/390 [==========>...................] - ETA: 1:28 - loss: 2.0634 - acc: 0.4183 - LRFinder: val_loss: 3.8016 - lr = 0.00482769 \n",
            "146/390 [==========>...................] - ETA: 1:27 - loss: 2.0628 - acc: 0.4190 - LRFinder: val_loss: 3.8311 - lr = 0.00484872 \n",
            "147/390 [==========>...................] - ETA: 1:27 - loss: 2.0626 - acc: 0.4193 - LRFinder: val_loss: 3.6586 - lr = 0.00486974 \n",
            "148/390 [==========>...................] - ETA: 1:26 - loss: 2.0626 - acc: 0.4193 - LRFinder: val_loss: 3.8130 - lr = 0.00489077 \n",
            "149/390 [==========>...................] - ETA: 1:26 - loss: 2.0625 - acc: 0.4195 - LRFinder: val_loss: 3.9026 - lr = 0.00491179 \n",
            "150/390 [==========>...................] - ETA: 1:26 - loss: 2.0613 - acc: 0.4200 - LRFinder: val_loss: 3.8668 - lr = 0.00493282 \n",
            "151/390 [==========>...................] - ETA: 1:25 - loss: 2.0615 - acc: 0.4201 - LRFinder: val_loss: 3.9118 - lr = 0.00495385 \n",
            "152/390 [==========>...................] - ETA: 1:25 - loss: 2.0621 - acc: 0.4201 - LRFinder: val_loss: 3.9174 - lr = 0.00497487 \n",
            "153/390 [==========>...................] - ETA: 1:25 - loss: 2.0628 - acc: 0.4200 - LRFinder: val_loss: 3.8638 - lr = 0.00499590 \n",
            "154/390 [==========>...................] - ETA: 1:24 - loss: 2.0623 - acc: 0.4200 - LRFinder: val_loss: 3.7036 - lr = 0.00501692 \n",
            "155/390 [==========>...................] - ETA: 1:24 - loss: 2.0624 - acc: 0.4202 - LRFinder: val_loss: 3.6571 - lr = 0.00503795 \n",
            "156/390 [===========>..................] - ETA: 1:23 - loss: 2.0624 - acc: 0.4203 - LRFinder: val_loss: 3.7064 - lr = 0.00505897 \n",
            "157/390 [===========>..................] - ETA: 1:23 - loss: 2.0620 - acc: 0.4204 - LRFinder: val_loss: 3.9537 - lr = 0.00508000 \n",
            "158/390 [===========>..................] - ETA: 1:23 - loss: 2.0616 - acc: 0.4208 - LRFinder: val_loss: 3.6595 - lr = 0.00510103 \n",
            "159/390 [===========>..................] - ETA: 1:22 - loss: 2.0605 - acc: 0.4213 - LRFinder: val_loss: 3.8251 - lr = 0.00512205 \n",
            "160/390 [===========>..................] - ETA: 1:22 - loss: 2.0597 - acc: 0.4218 - LRFinder: val_loss: 3.8769 - lr = 0.00514308 \n",
            "161/390 [===========>..................] - ETA: 1:21 - loss: 2.0588 - acc: 0.4227 - LRFinder: val_loss: 3.7749 - lr = 0.00516410 \n",
            "162/390 [===========>..................] - ETA: 1:21 - loss: 2.0579 - acc: 0.4233 - LRFinder: val_loss: 3.7855 - lr = 0.00518513 \n",
            "163/390 [===========>..................] - ETA: 1:21 - loss: 2.0586 - acc: 0.4227 - LRFinder: val_loss: 3.6549 - lr = 0.00520615 \n",
            "164/390 [===========>..................] - ETA: 1:20 - loss: 2.0580 - acc: 0.4228 - LRFinder: val_loss: 3.8979 - lr = 0.00522718 \n",
            "165/390 [===========>..................] - ETA: 1:20 - loss: 2.0574 - acc: 0.4229 - LRFinder: val_loss: 3.8041 - lr = 0.00524821 \n",
            "166/390 [===========>..................] - ETA: 1:20 - loss: 2.0576 - acc: 0.4227 - LRFinder: val_loss: 3.9644 - lr = 0.00526923 \n",
            "167/390 [===========>..................] - ETA: 1:19 - loss: 2.0577 - acc: 0.4223 - LRFinder: val_loss: 3.8656 - lr = 0.00529026 \n",
            "168/390 [===========>..................] - ETA: 1:19 - loss: 2.0577 - acc: 0.4223 - LRFinder: val_loss: 3.8355 - lr = 0.00531128 \n",
            "169/390 [============>.................] - ETA: 1:18 - loss: 2.0586 - acc: 0.4216 - LRFinder: val_loss: 3.7712 - lr = 0.00533231 \n",
            "170/390 [============>.................] - ETA: 1:18 - loss: 2.0582 - acc: 0.4218 - LRFinder: val_loss: 3.8692 - lr = 0.00535333 \n",
            "171/390 [============>.................] - ETA: 1:18 - loss: 2.0592 - acc: 0.4216 - LRFinder: val_loss: 3.7217 - lr = 0.00537436 \n",
            "172/390 [============>.................] - ETA: 1:17 - loss: 2.0587 - acc: 0.4217 - LRFinder: val_loss: 3.7647 - lr = 0.00539538 \n",
            "173/390 [============>.................] - ETA: 1:17 - loss: 2.0588 - acc: 0.4216 - LRFinder: val_loss: 3.6915 - lr = 0.00541641 \n",
            "174/390 [============>.................] - ETA: 1:17 - loss: 2.0579 - acc: 0.4217 - LRFinder: val_loss: 3.5918 - lr = 0.00543744 \n",
            "175/390 [============>.................] - ETA: 1:16 - loss: 2.0579 - acc: 0.4216 - LRFinder: val_loss: 3.6131 - lr = 0.00545846 \n",
            "176/390 [============>.................] - ETA: 1:16 - loss: 2.0585 - acc: 0.4213 - LRFinder: val_loss: 3.7450 - lr = 0.00547949 \n",
            "177/390 [============>.................] - ETA: 1:15 - loss: 2.0579 - acc: 0.4213 - LRFinder: val_loss: 3.8727 - lr = 0.00550051 \n",
            "178/390 [============>.................] - ETA: 1:15 - loss: 2.0580 - acc: 0.4214 - LRFinder: val_loss: 3.7221 - lr = 0.00552154 \n",
            "179/390 [============>.................] - ETA: 1:15 - loss: 2.0579 - acc: 0.4219 - LRFinder: val_loss: 3.7390 - lr = 0.00554256 \n",
            "180/390 [============>.................] - ETA: 1:14 - loss: 2.0577 - acc: 0.4221 - LRFinder: val_loss: 3.5675 - lr = 0.00556359 \n",
            "181/390 [============>.................] - ETA: 1:14 - loss: 2.0575 - acc: 0.4223 - LRFinder: val_loss: 3.7838 - lr = 0.00558462 \n",
            "182/390 [=============>................] - ETA: 1:14 - loss: 2.0577 - acc: 0.4224 - LRFinder: val_loss: 3.8203 - lr = 0.00560564 \n",
            "183/390 [=============>................] - ETA: 1:13 - loss: 2.0580 - acc: 0.4222 - LRFinder: val_loss: 4.0112 - lr = 0.00562667 \n",
            "184/390 [=============>................] - ETA: 1:13 - loss: 2.0579 - acc: 0.4221 - LRFinder: val_loss: 3.9559 - lr = 0.00564769 \n",
            "185/390 [=============>................] - ETA: 1:12 - loss: 2.0580 - acc: 0.4223 - LRFinder: val_loss: 3.8467 - lr = 0.00566872 \n",
            "186/390 [=============>................] - ETA: 1:12 - loss: 2.0577 - acc: 0.4226 - LRFinder: val_loss: 3.8193 - lr = 0.00568974 \n",
            "187/390 [=============>................] - ETA: 1:12 - loss: 2.0578 - acc: 0.4225 - LRFinder: val_loss: 3.7049 - lr = 0.00571077 \n",
            "188/390 [=============>................] - ETA: 1:11 - loss: 2.0574 - acc: 0.4227 - LRFinder: val_loss: 3.7527 - lr = 0.00573179 \n",
            "189/390 [=============>................] - ETA: 1:11 - loss: 2.0573 - acc: 0.4228 - LRFinder: val_loss: 3.8130 - lr = 0.00575282 \n",
            "190/390 [=============>................] - ETA: 1:11 - loss: 2.0570 - acc: 0.4228 - LRFinder: val_loss: 3.8543 - lr = 0.00577385 \n",
            "191/390 [=============>................] - ETA: 1:10 - loss: 2.0573 - acc: 0.4224 - LRFinder: val_loss: 3.8027 - lr = 0.00579487 \n",
            "192/390 [=============>................] - ETA: 1:10 - loss: 2.0569 - acc: 0.4225 - LRFinder: val_loss: 3.7918 - lr = 0.00581590 \n",
            "193/390 [=============>................] - ETA: 1:10 - loss: 2.0566 - acc: 0.4222 - LRFinder: val_loss: 3.6984 - lr = 0.00583692 \n",
            "194/390 [=============>................] - ETA: 1:09 - loss: 2.0571 - acc: 0.4220 - LRFinder: val_loss: 3.7481 - lr = 0.00585795 \n",
            "195/390 [==============>...............] - ETA: 1:09 - loss: 2.0569 - acc: 0.4220 - LRFinder: val_loss: 3.7835 - lr = 0.00587897 \n",
            "196/390 [==============>...............] - ETA: 1:08 - loss: 2.0571 - acc: 0.4221 - LRFinder: val_loss: 3.5913 - lr = 0.00590000 \n",
            "197/390 [==============>...............] - ETA: 1:08 - loss: 2.0568 - acc: 0.4222 - LRFinder: val_loss: 3.7305 - lr = 0.00592103 \n",
            "198/390 [==============>...............] - ETA: 1:08 - loss: 2.0568 - acc: 0.4223 - LRFinder: val_loss: 3.6210 - lr = 0.00594205 \n",
            "199/390 [==============>...............] - ETA: 1:07 - loss: 2.0566 - acc: 0.4224 - LRFinder: val_loss: 3.6364 - lr = 0.00596308 \n",
            "200/390 [==============>...............] - ETA: 1:07 - loss: 2.0576 - acc: 0.4218 - LRFinder: val_loss: 3.6511 - lr = 0.00598410 \n",
            "201/390 [==============>...............] - ETA: 1:07 - loss: 2.0574 - acc: 0.4218 - LRFinder: val_loss: 3.7848 - lr = 0.00600513 \n",
            "202/390 [==============>...............] - ETA: 1:06 - loss: 2.0570 - acc: 0.4216 - LRFinder: val_loss: 3.8860 - lr = 0.00602615 \n",
            "203/390 [==============>...............] - ETA: 1:06 - loss: 2.0570 - acc: 0.4217 - LRFinder: val_loss: 3.7893 - lr = 0.00604718 \n",
            "204/390 [==============>...............] - ETA: 1:05 - loss: 2.0567 - acc: 0.4217 - LRFinder: val_loss: 3.6314 - lr = 0.00606821 \n",
            "205/390 [==============>...............] - ETA: 1:05 - loss: 2.0554 - acc: 0.4221 - LRFinder: val_loss: 3.6611 - lr = 0.00608923 \n",
            "206/390 [==============>...............] - ETA: 1:05 - loss: 2.0551 - acc: 0.4221 - LRFinder: val_loss: 3.5869 - lr = 0.00611026 \n",
            "207/390 [==============>...............] - ETA: 1:04 - loss: 2.0548 - acc: 0.4224 - LRFinder: val_loss: 3.6957 - lr = 0.00613128 \n",
            "208/390 [===============>..............] - ETA: 1:04 - loss: 2.0545 - acc: 0.4224 - LRFinder: val_loss: 3.5954 - lr = 0.00615231 \n",
            "209/390 [===============>..............] - ETA: 1:04 - loss: 2.0548 - acc: 0.4221 - LRFinder: val_loss: 3.6336 - lr = 0.00617333 \n",
            "210/390 [===============>..............] - ETA: 1:03 - loss: 2.0547 - acc: 0.4222 - LRFinder: val_loss: 3.6330 - lr = 0.00619436 \n",
            "211/390 [===============>..............] - ETA: 1:03 - loss: 2.0540 - acc: 0.4227 - LRFinder: val_loss: 3.6366 - lr = 0.00621538 \n",
            "212/390 [===============>..............] - ETA: 1:03 - loss: 2.0540 - acc: 0.4224 - LRFinder: val_loss: 3.6816 - lr = 0.00623641 \n",
            "213/390 [===============>..............] - ETA: 1:02 - loss: 2.0537 - acc: 0.4227 - LRFinder: val_loss: 3.7319 - lr = 0.00625744 \n",
            "214/390 [===============>..............] - ETA: 1:02 - loss: 2.0538 - acc: 0.4227 - LRFinder: val_loss: 3.7601 - lr = 0.00627846 \n",
            "215/390 [===============>..............] - ETA: 1:01 - loss: 2.0530 - acc: 0.4228 - LRFinder: val_loss: 3.7781 - lr = 0.00629949 \n",
            "216/390 [===============>..............] - ETA: 1:01 - loss: 2.0523 - acc: 0.4231 - LRFinder: val_loss: 3.7786 - lr = 0.00632051 \n",
            "217/390 [===============>..............] - ETA: 1:01 - loss: 2.0524 - acc: 0.4228 - LRFinder: val_loss: 3.8762 - lr = 0.00634154 \n",
            "218/390 [===============>..............] - ETA: 1:00 - loss: 2.0526 - acc: 0.4229 - LRFinder: val_loss: 3.8358 - lr = 0.00636256 \n",
            "219/390 [===============>..............] - ETA: 1:00 - loss: 2.0517 - acc: 0.4235 - LRFinder: val_loss: 3.8220 - lr = 0.00638359 \n",
            "220/390 [===============>..............] - ETA: 1:00 - loss: 2.0514 - acc: 0.4235 - LRFinder: val_loss: 3.9930 - lr = 0.00640462 \n",
            "221/390 [================>.............] - ETA: 59s - loss: 2.0511 - acc: 0.4237  - LRFinder: val_loss: 3.7457 - lr = 0.00642564 \n",
            "222/390 [================>.............] - ETA: 59s - loss: 2.0507 - acc: 0.4238 - LRFinder: val_loss: 3.9634 - lr = 0.00644667 \n",
            "223/390 [================>.............] - ETA: 59s - loss: 2.0512 - acc: 0.4235 - LRFinder: val_loss: 3.6802 - lr = 0.00646769 \n",
            "224/390 [================>.............] - ETA: 58s - loss: 2.0512 - acc: 0.4237 - LRFinder: val_loss: 3.8063 - lr = 0.00648872 \n",
            "225/390 [================>.............] - ETA: 58s - loss: 2.0506 - acc: 0.4235 - LRFinder: val_loss: 3.8255 - lr = 0.00650974 \n",
            "226/390 [================>.............] - ETA: 57s - loss: 2.0497 - acc: 0.4236 - LRFinder: val_loss: 3.7498 - lr = 0.00653077 \n",
            "227/390 [================>.............] - ETA: 57s - loss: 2.0496 - acc: 0.4237 - LRFinder: val_loss: 3.7535 - lr = 0.00655179 \n",
            "228/390 [================>.............] - ETA: 57s - loss: 2.0499 - acc: 0.4237 - LRFinder: val_loss: 3.7096 - lr = 0.00657282 \n",
            "229/390 [================>.............] - ETA: 56s - loss: 2.0494 - acc: 0.4243 - LRFinder: val_loss: 3.7470 - lr = 0.00659385 \n",
            "230/390 [================>.............] - ETA: 56s - loss: 2.0497 - acc: 0.4240 - LRFinder: val_loss: 3.6681 - lr = 0.00661487 \n",
            "231/390 [================>.............] - ETA: 56s - loss: 2.0492 - acc: 0.4244 - LRFinder: val_loss: 3.7926 - lr = 0.00663590 \n",
            "232/390 [================>.............] - ETA: 55s - loss: 2.0489 - acc: 0.4242 - LRFinder: val_loss: 3.6984 - lr = 0.00665692 \n",
            "233/390 [================>.............] - ETA: 55s - loss: 2.0485 - acc: 0.4244 - LRFinder: val_loss: 3.7096 - lr = 0.00667795 \n",
            "234/390 [=================>............] - ETA: 55s - loss: 2.0485 - acc: 0.4246 - LRFinder: val_loss: 4.0004 - lr = 0.00669897 \n",
            "235/390 [=================>............] - ETA: 54s - loss: 2.0484 - acc: 0.4245 - LRFinder: val_loss: 3.8930 - lr = 0.00672000 \n",
            "236/390 [=================>............] - ETA: 54s - loss: 2.0480 - acc: 0.4246 - LRFinder: val_loss: 3.7768 - lr = 0.00674103 \n",
            "237/390 [=================>............] - ETA: 53s - loss: 2.0479 - acc: 0.4248 - LRFinder: val_loss: 3.7413 - lr = 0.00676205 \n",
            "238/390 [=================>............] - ETA: 53s - loss: 2.0481 - acc: 0.4248 - LRFinder: val_loss: 3.8480 - lr = 0.00678308 \n",
            "239/390 [=================>............] - ETA: 53s - loss: 2.0482 - acc: 0.4249 - LRFinder: val_loss: 3.8813 - lr = 0.00680410 \n",
            "240/390 [=================>............] - ETA: 52s - loss: 2.0480 - acc: 0.4251 - LRFinder: val_loss: 3.7889 - lr = 0.00682513 \n",
            "241/390 [=================>............] - ETA: 52s - loss: 2.0481 - acc: 0.4249 - LRFinder: val_loss: 3.9268 - lr = 0.00684615 \n",
            "242/390 [=================>............] - ETA: 52s - loss: 2.0481 - acc: 0.4249 - LRFinder: val_loss: 3.8259 - lr = 0.00686718 \n",
            "243/390 [=================>............] - ETA: 51s - loss: 2.0483 - acc: 0.4247 - LRFinder: val_loss: 3.9685 - lr = 0.00688821 \n",
            "244/390 [=================>............] - ETA: 51s - loss: 2.0490 - acc: 0.4243 - LRFinder: val_loss: 3.8936 - lr = 0.00690923 \n",
            "245/390 [=================>............] - ETA: 51s - loss: 2.0489 - acc: 0.4246 - LRFinder: val_loss: 3.8701 - lr = 0.00693026 \n",
            "246/390 [=================>............] - ETA: 50s - loss: 2.0483 - acc: 0.4250 - LRFinder: val_loss: 3.9116 - lr = 0.00695128 \n",
            "247/390 [==================>...........] - ETA: 50s - loss: 2.0480 - acc: 0.4252 - LRFinder: val_loss: 3.8636 - lr = 0.00697231 \n",
            "248/390 [==================>...........] - ETA: 50s - loss: 2.0477 - acc: 0.4253 - LRFinder: val_loss: 3.8364 - lr = 0.00699333 \n",
            "249/390 [==================>...........] - ETA: 49s - loss: 2.0477 - acc: 0.4252 - LRFinder: val_loss: 3.8295 - lr = 0.00701436 \n",
            "250/390 [==================>...........] - ETA: 49s - loss: 2.0477 - acc: 0.4252 - LRFinder: val_loss: 3.9250 - lr = 0.00703538 \n",
            "251/390 [==================>...........] - ETA: 48s - loss: 2.0480 - acc: 0.4253 - LRFinder: val_loss: 3.7088 - lr = 0.00705641 \n",
            "252/390 [==================>...........] - ETA: 48s - loss: 2.0474 - acc: 0.4253 - LRFinder: val_loss: 3.6291 - lr = 0.00707744 \n",
            "253/390 [==================>...........] - ETA: 48s - loss: 2.0475 - acc: 0.4251 - LRFinder: val_loss: 3.6617 - lr = 0.00709846 \n",
            "254/390 [==================>...........] - ETA: 47s - loss: 2.0472 - acc: 0.4254 - LRFinder: val_loss: 3.5277 - lr = 0.00711949 \n",
            "255/390 [==================>...........] - ETA: 47s - loss: 2.0467 - acc: 0.4257 - LRFinder: val_loss: 3.7403 - lr = 0.00714051 \n",
            "256/390 [==================>...........] - ETA: 47s - loss: 2.0467 - acc: 0.4256 - LRFinder: val_loss: 3.7993 - lr = 0.00716154 \n",
            "257/390 [==================>...........] - ETA: 46s - loss: 2.0463 - acc: 0.4258 - LRFinder: val_loss: 3.7968 - lr = 0.00718256 \n",
            "258/390 [==================>...........] - ETA: 46s - loss: 2.0462 - acc: 0.4259 - LRFinder: val_loss: 3.6589 - lr = 0.00720359 \n",
            "259/390 [==================>...........] - ETA: 46s - loss: 2.0459 - acc: 0.4260 - LRFinder: val_loss: 3.7782 - lr = 0.00722462 \n",
            "260/390 [===================>..........] - ETA: 45s - loss: 2.0456 - acc: 0.4262 - LRFinder: val_loss: 3.6543 - lr = 0.00724564 \n",
            "261/390 [===================>..........] - ETA: 45s - loss: 2.0455 - acc: 0.4260 - LRFinder: val_loss: 3.6357 - lr = 0.00726667 \n",
            "262/390 [===================>..........] - ETA: 45s - loss: 2.0450 - acc: 0.4262 - LRFinder: val_loss: 3.5128 - lr = 0.00728769 \n",
            "263/390 [===================>..........] - ETA: 44s - loss: 2.0448 - acc: 0.4262 - LRFinder: val_loss: 3.6655 - lr = 0.00730872 \n",
            "264/390 [===================>..........] - ETA: 44s - loss: 2.0442 - acc: 0.4264 - LRFinder: val_loss: 3.4965 - lr = 0.00732974 \n",
            "265/390 [===================>..........] - ETA: 43s - loss: 2.0442 - acc: 0.4267 - LRFinder: val_loss: 3.5376 - lr = 0.00735077 \n",
            "266/390 [===================>..........] - ETA: 43s - loss: 2.0447 - acc: 0.4268 - LRFinder: val_loss: 3.4720 - lr = 0.00737179 \n",
            "267/390 [===================>..........] - ETA: 43s - loss: 2.0440 - acc: 0.4270 - LRFinder: val_loss: 3.5763 - lr = 0.00739282 \n",
            "268/390 [===================>..........] - ETA: 42s - loss: 2.0434 - acc: 0.4271 - LRFinder: val_loss: 3.6256 - lr = 0.00741385 \n",
            "269/390 [===================>..........] - ETA: 42s - loss: 2.0438 - acc: 0.4269 - LRFinder: val_loss: 3.4754 - lr = 0.00743487 \n",
            "270/390 [===================>..........] - ETA: 42s - loss: 2.0433 - acc: 0.4272 - LRFinder: val_loss: 3.5455 - lr = 0.00745590 \n",
            "271/390 [===================>..........] - ETA: 41s - loss: 2.0431 - acc: 0.4271 - LRFinder: val_loss: 3.4099 - lr = 0.00747692 \n",
            "272/390 [===================>..........] - ETA: 41s - loss: 2.0426 - acc: 0.4272 - LRFinder: val_loss: 3.4027 - lr = 0.00749795 \n",
            "273/390 [====================>.........] - ETA: 41s - loss: 2.0424 - acc: 0.4273 - LRFinder: val_loss: 3.3242 - lr = 0.00751897 \n",
            "274/390 [====================>.........] - ETA: 40s - loss: 2.0415 - acc: 0.4276 - LRFinder: val_loss: 3.3244 - lr = 0.00754000 \n",
            "275/390 [====================>.........] - ETA: 40s - loss: 2.0414 - acc: 0.4277 - LRFinder: val_loss: 3.3885 - lr = 0.00756103 \n",
            "276/390 [====================>.........] - ETA: 40s - loss: 2.0412 - acc: 0.4277 - LRFinder: val_loss: 3.4449 - lr = 0.00758205 \n",
            "277/390 [====================>.........] - ETA: 39s - loss: 2.0405 - acc: 0.4280 - LRFinder: val_loss: 3.6440 - lr = 0.00760308 \n",
            "278/390 [====================>.........] - ETA: 39s - loss: 2.0401 - acc: 0.4279 - LRFinder: val_loss: 3.4757 - lr = 0.00762410 \n",
            "279/390 [====================>.........] - ETA: 38s - loss: 2.0393 - acc: 0.4281 - LRFinder: val_loss: 3.5012 - lr = 0.00764513 \n",
            "280/390 [====================>.........] - ETA: 38s - loss: 2.0390 - acc: 0.4281 - LRFinder: val_loss: 3.4687 - lr = 0.00766615 \n",
            "281/390 [====================>.........] - ETA: 38s - loss: 2.0385 - acc: 0.4282 - LRFinder: val_loss: 3.4116 - lr = 0.00768718 \n",
            "282/390 [====================>.........] - ETA: 37s - loss: 2.0386 - acc: 0.4281 - LRFinder: val_loss: 3.3398 - lr = 0.00770821 \n",
            "283/390 [====================>.........] - ETA: 37s - loss: 2.0380 - acc: 0.4283 - LRFinder: val_loss: 3.3225 - lr = 0.00772923 \n",
            "284/390 [====================>.........] - ETA: 37s - loss: 2.0379 - acc: 0.4281 - LRFinder: val_loss: 3.4601 - lr = 0.00775026 \n",
            "285/390 [====================>.........] - ETA: 36s - loss: 2.0377 - acc: 0.4281 - LRFinder: val_loss: 3.4710 - lr = 0.00777128 \n",
            "286/390 [=====================>........] - ETA: 36s - loss: 2.0377 - acc: 0.4281 - LRFinder: val_loss: 3.6182 - lr = 0.00779231 \n",
            "287/390 [=====================>........] - ETA: 36s - loss: 2.0375 - acc: 0.4284 - LRFinder: val_loss: 3.6799 - lr = 0.00781333 \n",
            "288/390 [=====================>........] - ETA: 35s - loss: 2.0372 - acc: 0.4284 - LRFinder: val_loss: 3.7200 - lr = 0.00783436 \n",
            "289/390 [=====================>........] - ETA: 35s - loss: 2.0369 - acc: 0.4285 - LRFinder: val_loss: 3.4388 - lr = 0.00785538 \n",
            "290/390 [=====================>........] - ETA: 35s - loss: 2.0366 - acc: 0.4287 - LRFinder: val_loss: 3.4803 - lr = 0.00787641 \n",
            "291/390 [=====================>........] - ETA: 34s - loss: 2.0367 - acc: 0.4287 - LRFinder: val_loss: 3.4609 - lr = 0.00789744 \n",
            "292/390 [=====================>........] - ETA: 34s - loss: 2.0361 - acc: 0.4290 - LRFinder: val_loss: 3.4217 - lr = 0.00791846 \n",
            "293/390 [=====================>........] - ETA: 34s - loss: 2.0359 - acc: 0.4292 - LRFinder: val_loss: 3.5169 - lr = 0.00793949 \n",
            "294/390 [=====================>........] - ETA: 33s - loss: 2.0360 - acc: 0.4290 - LRFinder: val_loss: 3.5526 - lr = 0.00796051 \n",
            "295/390 [=====================>........] - ETA: 33s - loss: 2.0360 - acc: 0.4291 - LRFinder: val_loss: 3.4986 - lr = 0.00798154 \n",
            "296/390 [=====================>........] - ETA: 32s - loss: 2.0359 - acc: 0.4293 - LRFinder: val_loss: 3.6776 - lr = 0.00800256 \n",
            "297/390 [=====================>........] - ETA: 32s - loss: 2.0358 - acc: 0.4294 - LRFinder: val_loss: 3.5519 - lr = 0.00802359 \n",
            "298/390 [=====================>........] - ETA: 32s - loss: 2.0357 - acc: 0.4293 - LRFinder: val_loss: 3.5437 - lr = 0.00804462 \n",
            "299/390 [======================>.......] - ETA: 31s - loss: 2.0354 - acc: 0.4294 - LRFinder: val_loss: 3.6077 - lr = 0.00806564 \n",
            "300/390 [======================>.......] - ETA: 31s - loss: 2.0350 - acc: 0.4294 - LRFinder: val_loss: 3.7418 - lr = 0.00808667 \n",
            "301/390 [======================>.......] - ETA: 31s - loss: 2.0343 - acc: 0.4297 - LRFinder: val_loss: 3.7180 - lr = 0.00810769 \n",
            "302/390 [======================>.......] - ETA: 30s - loss: 2.0337 - acc: 0.4300 - LRFinder: val_loss: 3.8361 - lr = 0.00812872 \n",
            "303/390 [======================>.......] - ETA: 30s - loss: 2.0338 - acc: 0.4300 - LRFinder: val_loss: 3.5691 - lr = 0.00814974 \n",
            "304/390 [======================>.......] - ETA: 30s - loss: 2.0340 - acc: 0.4300 - LRFinder: val_loss: 3.5626 - lr = 0.00817077 \n",
            "305/390 [======================>.......] - ETA: 29s - loss: 2.0342 - acc: 0.4300 - LRFinder: val_loss: 3.4398 - lr = 0.00819179 \n",
            "306/390 [======================>.......] - ETA: 29s - loss: 2.0337 - acc: 0.4301 - LRFinder: val_loss: 3.4843 - lr = 0.00821282 \n",
            "307/390 [======================>.......] - ETA: 29s - loss: 2.0333 - acc: 0.4305 - LRFinder: val_loss: 3.6840 - lr = 0.00823385 \n",
            "308/390 [======================>.......] - ETA: 28s - loss: 2.0332 - acc: 0.4304 - LRFinder: val_loss: 3.5066 - lr = 0.00825487 \n",
            "309/390 [======================>.......] - ETA: 28s - loss: 2.0331 - acc: 0.4306 - LRFinder: val_loss: 3.4279 - lr = 0.00827590 \n",
            "310/390 [======================>.......] - ETA: 28s - loss: 2.0324 - acc: 0.4311 - LRFinder: val_loss: 3.3759 - lr = 0.00829692 \n",
            "311/390 [======================>.......] - ETA: 27s - loss: 2.0321 - acc: 0.4312 - LRFinder: val_loss: 3.2904 - lr = 0.00831795 \n",
            "312/390 [=======================>......] - ETA: 27s - loss: 2.0318 - acc: 0.4312 - LRFinder: val_loss: 3.2767 - lr = 0.00833897 \n",
            "313/390 [=======================>......] - ETA: 26s - loss: 2.0316 - acc: 0.4313 - LRFinder: val_loss: 3.2634 - lr = 0.00836000 \n",
            "314/390 [=======================>......] - ETA: 26s - loss: 2.0316 - acc: 0.4312 - LRFinder: val_loss: 3.2788 - lr = 0.00838103 \n",
            "315/390 [=======================>......] - ETA: 26s - loss: 2.0314 - acc: 0.4313 - LRFinder: val_loss: 3.2240 - lr = 0.00840205 \n",
            "316/390 [=======================>......] - ETA: 25s - loss: 2.0311 - acc: 0.4312 - LRFinder: val_loss: 3.2519 - lr = 0.00842308 \n",
            "317/390 [=======================>......] - ETA: 25s - loss: 2.0309 - acc: 0.4314 - LRFinder: val_loss: 3.2027 - lr = 0.00844410 \n",
            "318/390 [=======================>......] - ETA: 25s - loss: 2.0307 - acc: 0.4314 - LRFinder: val_loss: 3.3184 - lr = 0.00846513 \n",
            "319/390 [=======================>......] - ETA: 24s - loss: 2.0304 - acc: 0.4316 - LRFinder: val_loss: 3.4336 - lr = 0.00848615 \n",
            "320/390 [=======================>......] - ETA: 24s - loss: 2.0303 - acc: 0.4315 - LRFinder: val_loss: 3.4638 - lr = 0.00850718 \n",
            "321/390 [=======================>......] - ETA: 24s - loss: 2.0306 - acc: 0.4314 - LRFinder: val_loss: 3.4019 - lr = 0.00852821 \n",
            "322/390 [=======================>......] - ETA: 23s - loss: 2.0301 - acc: 0.4317 - LRFinder: val_loss: 3.4365 - lr = 0.00854923 \n",
            "323/390 [=======================>......] - ETA: 23s - loss: 2.0297 - acc: 0.4317 - LRFinder: val_loss: 3.2247 - lr = 0.00857026 \n",
            "324/390 [=======================>......] - ETA: 23s - loss: 2.0291 - acc: 0.4320 - LRFinder: val_loss: 3.3001 - lr = 0.00859128 \n",
            "325/390 [========================>.....] - ETA: 22s - loss: 2.0287 - acc: 0.4320 - LRFinder: val_loss: 3.2505 - lr = 0.00861231 \n",
            "326/390 [========================>.....] - ETA: 22s - loss: 2.0284 - acc: 0.4322 - LRFinder: val_loss: 3.5340 - lr = 0.00863333 \n",
            "327/390 [========================>.....] - ETA: 22s - loss: 2.0282 - acc: 0.4321 - LRFinder: val_loss: 3.5116 - lr = 0.00865436 \n",
            "328/390 [========================>.....] - ETA: 21s - loss: 2.0278 - acc: 0.4323 - LRFinder: val_loss: 3.5038 - lr = 0.00867538 \n",
            "329/390 [========================>.....] - ETA: 21s - loss: 2.0283 - acc: 0.4321 - LRFinder: val_loss: 3.5052 - lr = 0.00869641 \n",
            "330/390 [========================>.....] - ETA: 20s - loss: 2.0286 - acc: 0.4319 - LRFinder: val_loss: 3.5466 - lr = 0.00871744 \n",
            "331/390 [========================>.....] - ETA: 20s - loss: 2.0283 - acc: 0.4320 - LRFinder: val_loss: 3.3928 - lr = 0.00873846 \n",
            "332/390 [========================>.....] - ETA: 20s - loss: 2.0284 - acc: 0.4322 - LRFinder: val_loss: 3.4199 - lr = 0.00875949 \n",
            "333/390 [========================>.....] - ETA: 19s - loss: 2.0278 - acc: 0.4323 - LRFinder: val_loss: 3.3834 - lr = 0.00878051 \n",
            "334/390 [========================>.....] - ETA: 19s - loss: 2.0277 - acc: 0.4324 - LRFinder: val_loss: 3.5291 - lr = 0.00880154 \n",
            "335/390 [========================>.....] - ETA: 19s - loss: 2.0273 - acc: 0.4326 - LRFinder: val_loss: 3.4230 - lr = 0.00882256 \n",
            "336/390 [========================>.....] - ETA: 18s - loss: 2.0271 - acc: 0.4326 - LRFinder: val_loss: 3.3291 - lr = 0.00884359 \n",
            "337/390 [========================>.....] - ETA: 18s - loss: 2.0271 - acc: 0.4325 - LRFinder: val_loss: 3.3286 - lr = 0.00886462 \n",
            "338/390 [=========================>....] - ETA: 18s - loss: 2.0270 - acc: 0.4323 - LRFinder: val_loss: 3.3524 - lr = 0.00888564 \n",
            "339/390 [=========================>....] - ETA: 17s - loss: 2.0271 - acc: 0.4322 - LRFinder: val_loss: 3.4781 - lr = 0.00890667 \n",
            "340/390 [=========================>....] - ETA: 17s - loss: 2.0271 - acc: 0.4322 - LRFinder: val_loss: 3.3253 - lr = 0.00892769 \n",
            "341/390 [=========================>....] - ETA: 17s - loss: 2.0267 - acc: 0.4324 - LRFinder: val_loss: 3.3198 - lr = 0.00894872 \n",
            "342/390 [=========================>....] - ETA: 16s - loss: 2.0265 - acc: 0.4325 - LRFinder: val_loss: 3.3179 - lr = 0.00896974 \n",
            "343/390 [=========================>....] - ETA: 16s - loss: 2.0262 - acc: 0.4326 - LRFinder: val_loss: 3.4884 - lr = 0.00899077 \n",
            "344/390 [=========================>....] - ETA: 16s - loss: 2.0255 - acc: 0.4330 - LRFinder: val_loss: 3.5091 - lr = 0.00901179 \n",
            "345/390 [=========================>....] - ETA: 15s - loss: 2.0254 - acc: 0.4331 - LRFinder: val_loss: 3.5743 - lr = 0.00903282 \n",
            "346/390 [=========================>....] - ETA: 15s - loss: 2.0248 - acc: 0.4334 - LRFinder: val_loss: 3.5997 - lr = 0.00905385 \n",
            "347/390 [=========================>....] - ETA: 15s - loss: 2.0246 - acc: 0.4335 - LRFinder: val_loss: 3.7106 - lr = 0.00907487 \n",
            "348/390 [=========================>....] - ETA: 14s - loss: 2.0245 - acc: 0.4336 - LRFinder: val_loss: 3.8593 - lr = 0.00909590 \n",
            "349/390 [=========================>....] - ETA: 14s - loss: 2.0243 - acc: 0.4336 - LRFinder: val_loss: 3.7384 - lr = 0.00911692 \n",
            "350/390 [=========================>....] - ETA: 13s - loss: 2.0241 - acc: 0.4337 - LRFinder: val_loss: 3.7284 - lr = 0.00913795 \n",
            "351/390 [==========================>...] - ETA: 13s - loss: 2.0245 - acc: 0.4337 - LRFinder: val_loss: 3.4856 - lr = 0.00915897 \n",
            "352/390 [==========================>...] - ETA: 13s - loss: 2.0243 - acc: 0.4339 - LRFinder: val_loss: 3.8063 - lr = 0.00918000 \n",
            "353/390 [==========================>...] - ETA: 12s - loss: 2.0241 - acc: 0.4340 - LRFinder: val_loss: 3.7835 - lr = 0.00920103 \n",
            "354/390 [==========================>...] - ETA: 12s - loss: 2.0238 - acc: 0.4340 - LRFinder: val_loss: 3.7941 - lr = 0.00922205 \n",
            "355/390 [==========================>...] - ETA: 12s - loss: 2.0235 - acc: 0.4341 - LRFinder: val_loss: 3.7436 - lr = 0.00924308 \n",
            "356/390 [==========================>...] - ETA: 11s - loss: 2.0233 - acc: 0.4343 - LRFinder: val_loss: 4.0423 - lr = 0.00926410 \n",
            "357/390 [==========================>...] - ETA: 11s - loss: 2.0227 - acc: 0.4346 - LRFinder: val_loss: 4.2606 - lr = 0.00928513 \n",
            "358/390 [==========================>...] - ETA: 11s - loss: 2.0226 - acc: 0.4348 - LRFinder: val_loss: 3.9724 - lr = 0.00930615 \n",
            "359/390 [==========================>...] - ETA: 10s - loss: 2.0224 - acc: 0.4348 - LRFinder: val_loss: 3.9445 - lr = 0.00932718 \n",
            "360/390 [==========================>...] - ETA: 10s - loss: 2.0222 - acc: 0.4349 - LRFinder: val_loss: 3.8876 - lr = 0.00934821 \n",
            "361/390 [==========================>...] - ETA: 10s - loss: 2.0221 - acc: 0.4351 - LRFinder: val_loss: 3.7955 - lr = 0.00936923 \n",
            "362/390 [==========================>...] - ETA: 9s - loss: 2.0220 - acc: 0.4352  - LRFinder: val_loss: 3.7895 - lr = 0.00939026 \n",
            "363/390 [==========================>...] - ETA: 9s - loss: 2.0220 - acc: 0.4353 - LRFinder: val_loss: 3.7806 - lr = 0.00941128 \n",
            "364/390 [===========================>..] - ETA: 9s - loss: 2.0221 - acc: 0.4352 - LRFinder: val_loss: 3.7190 - lr = 0.00943231 \n",
            "365/390 [===========================>..] - ETA: 8s - loss: 2.0216 - acc: 0.4354 - LRFinder: val_loss: 3.6487 - lr = 0.00945333 \n",
            "366/390 [===========================>..] - ETA: 8s - loss: 2.0213 - acc: 0.4355 - LRFinder: val_loss: 3.7030 - lr = 0.00947436 \n",
            "367/390 [===========================>..] - ETA: 8s - loss: 2.0211 - acc: 0.4355 - LRFinder: val_loss: 3.7653 - lr = 0.00949538 \n",
            "368/390 [===========================>..] - ETA: 7s - loss: 2.0207 - acc: 0.4357 - LRFinder: val_loss: 3.5125 - lr = 0.00951641 \n",
            "369/390 [===========================>..] - ETA: 7s - loss: 2.0203 - acc: 0.4359 - LRFinder: val_loss: 3.6447 - lr = 0.00953744 \n",
            "370/390 [===========================>..] - ETA: 6s - loss: 2.0202 - acc: 0.4360 - LRFinder: val_loss: 3.6102 - lr = 0.00955846 \n",
            "371/390 [===========================>..] - ETA: 6s - loss: 2.0203 - acc: 0.4359 - LRFinder: val_loss: 3.7298 - lr = 0.00957949 \n",
            "372/390 [===========================>..] - ETA: 6s - loss: 2.0200 - acc: 0.4361 - LRFinder: val_loss: 3.7580 - lr = 0.00960051 \n",
            "373/390 [===========================>..] - ETA: 5s - loss: 2.0199 - acc: 0.4360 - LRFinder: val_loss: 3.6281 - lr = 0.00962154 \n",
            "374/390 [===========================>..] - ETA: 5s - loss: 2.0195 - acc: 0.4362 - LRFinder: val_loss: 3.4103 - lr = 0.00964256 \n",
            "375/390 [===========================>..] - ETA: 5s - loss: 2.0192 - acc: 0.4361 - LRFinder: val_loss: 3.6964 - lr = 0.00966359 \n",
            "376/390 [===========================>..] - ETA: 4s - loss: 2.0190 - acc: 0.4362 - LRFinder: val_loss: 3.6806 - lr = 0.00968462 \n",
            "377/390 [============================>.] - ETA: 4s - loss: 2.0187 - acc: 0.4363 - LRFinder: val_loss: 3.7665 - lr = 0.00970564 \n",
            "378/390 [============================>.] - ETA: 4s - loss: 2.0185 - acc: 0.4365 - LRFinder: val_loss: 3.6758 - lr = 0.00972667 \n",
            "379/390 [============================>.] - ETA: 3s - loss: 2.0182 - acc: 0.4366 - LRFinder: val_loss: 3.6478 - lr = 0.00974769 \n",
            "380/390 [============================>.] - ETA: 3s - loss: 2.0180 - acc: 0.4366 - LRFinder: val_loss: 3.5876 - lr = 0.00976872 \n",
            "381/390 [============================>.] - ETA: 3s - loss: 2.0179 - acc: 0.4367 - LRFinder: val_loss: 3.6336 - lr = 0.00978974 \n",
            "382/390 [============================>.] - ETA: 2s - loss: 2.0175 - acc: 0.4370 - LRFinder: val_loss: 3.4627 - lr = 0.00981077 \n",
            "383/390 [============================>.] - ETA: 2s - loss: 2.0174 - acc: 0.4370 - LRFinder: val_loss: 3.3543 - lr = 0.00983179 \n",
            "384/390 [============================>.] - ETA: 2s - loss: 2.0170 - acc: 0.4371 - LRFinder: val_loss: 3.5454 - lr = 0.00985282 \n",
            "385/390 [============================>.] - ETA: 1s - loss: 2.0166 - acc: 0.4373 - LRFinder: val_loss: 3.3366 - lr = 0.00987385 \n",
            "386/390 [============================>.] - ETA: 1s - loss: 2.0169 - acc: 0.4371 - LRFinder: val_loss: 3.4168 - lr = 0.00989487 \n",
            "387/390 [============================>.] - ETA: 1s - loss: 2.0167 - acc: 0.4373 - LRFinder: val_loss: 3.2971 - lr = 0.00991590 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 2.0166 - acc: 0.4373 - LRFinder: val_loss: 3.3761 - lr = 0.00993692 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 2.0162 - acc: 0.4374 - LRFinder: val_loss: 3.2863 - lr = 0.00995795 \n",
            " - LRFinder: val_loss: 3.3411 - lr = 0.00997897 \n",
            "390/390 [==============================] - 139s 357ms/step - loss: 2.0158 - acc: 0.4375 - val_loss: 3.4034 - val_acc: 0.1580\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {weights/momentum/momentum-0.9/}\n",
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 15:37 - loss: 1.9894 - acc: 0.4688 - LRFinder: val_loss: 3.4040 - lr = 0.00180000 \n",
            "  2/390 [..............................] - ETA: 10:39 - loss: 1.9568 - acc: 0.4766 - LRFinder: val_loss: 3.4929 - lr = 0.00182103 \n",
            "  3/390 [..............................] - ETA: 7:54 - loss: 1.9507 - acc: 0.4635  - LRFinder: val_loss: 3.3209 - lr = 0.00184205 \n",
            "  4/390 [..............................] - ETA: 6:28 - loss: 1.9345 - acc: 0.4688 - LRFinder: val_loss: 3.5347 - lr = 0.00186308 \n",
            "  5/390 [..............................] - ETA: 5:35 - loss: 1.9710 - acc: 0.4531 - LRFinder: val_loss: 3.5317 - lr = 0.00188410 \n",
            "  6/390 [..............................] - ETA: 5:00 - loss: 1.9672 - acc: 0.4466 - LRFinder: val_loss: 3.5002 - lr = 0.00190513 \n",
            "  7/390 [..............................] - ETA: 4:36 - loss: 1.9708 - acc: 0.4475 - LRFinder: val_loss: 3.4459 - lr = 0.00192615 \n",
            "  8/390 [..............................] - ETA: 4:17 - loss: 1.9576 - acc: 0.4609 - LRFinder: val_loss: 3.5751 - lr = 0.00194718 \n",
            "  9/390 [..............................] - ETA: 4:02 - loss: 1.9450 - acc: 0.4714 - LRFinder: val_loss: 3.6686 - lr = 0.00196821 \n",
            " 10/390 [..............................] - ETA: 3:50 - loss: 1.9340 - acc: 0.4688 - LRFinder: val_loss: 3.6454 - lr = 0.00198923 \n",
            " 11/390 [..............................] - ETA: 3:41 - loss: 1.9372 - acc: 0.4680 - LRFinder: val_loss: 3.7209 - lr = 0.00201026 \n",
            " 12/390 [..............................] - ETA: 3:32 - loss: 1.9300 - acc: 0.4694 - LRFinder: val_loss: 3.6359 - lr = 0.00203128 \n",
            " 13/390 [>.............................] - ETA: 3:25 - loss: 1.9331 - acc: 0.4633 - LRFinder: val_loss: 3.6609 - lr = 0.00205231 \n",
            " 14/390 [>.............................] - ETA: 3:19 - loss: 1.9256 - acc: 0.4648 - LRFinder: val_loss: 3.5721 - lr = 0.00207333 \n",
            " 15/390 [>.............................] - ETA: 3:14 - loss: 1.9279 - acc: 0.4677 - LRFinder: val_loss: 3.6229 - lr = 0.00209436 \n",
            " 16/390 [>.............................] - ETA: 3:09 - loss: 1.9224 - acc: 0.4722 - LRFinder: val_loss: 3.7046 - lr = 0.00211538 \n",
            " 17/390 [>.............................] - ETA: 3:05 - loss: 1.9158 - acc: 0.4738 - LRFinder: val_loss: 3.5391 - lr = 0.00213641 \n",
            " 18/390 [>.............................] - ETA: 3:02 - loss: 1.9117 - acc: 0.4753 - LRFinder: val_loss: 3.6173 - lr = 0.00215744 \n",
            " 19/390 [>.............................] - ETA: 2:58 - loss: 1.9134 - acc: 0.4725 - LRFinder: val_loss: 3.6340 - lr = 0.00217846 \n",
            " 20/390 [>.............................] - ETA: 2:55 - loss: 1.9109 - acc: 0.4730 - LRFinder: val_loss: 3.6926 - lr = 0.00219949 \n",
            " 21/390 [>.............................] - ETA: 2:52 - loss: 1.9089 - acc: 0.4725 - LRFinder: val_loss: 3.6463 - lr = 0.00222051 \n",
            " 22/390 [>.............................] - ETA: 2:50 - loss: 1.9127 - acc: 0.4719 - LRFinder: val_loss: 3.6748 - lr = 0.00224154 \n",
            " 23/390 [>.............................] - ETA: 2:47 - loss: 1.9111 - acc: 0.4752 - LRFinder: val_loss: 3.7248 - lr = 0.00226256 \n",
            " 24/390 [>.............................] - ETA: 2:45 - loss: 1.9081 - acc: 0.4772 - LRFinder: val_loss: 3.7305 - lr = 0.00228359 \n",
            " 25/390 [>.............................] - ETA: 2:43 - loss: 1.9034 - acc: 0.4797 - LRFinder: val_loss: 3.6863 - lr = 0.00230462 \n",
            " 26/390 [=>............................] - ETA: 2:41 - loss: 1.9019 - acc: 0.4820 - LRFinder: val_loss: 3.7701 - lr = 0.00232564 \n",
            " 27/390 [=>............................] - ETA: 2:39 - loss: 1.8972 - acc: 0.4835 - LRFinder: val_loss: 3.7921 - lr = 0.00234667 \n",
            " 28/390 [=>............................] - ETA: 2:38 - loss: 1.8936 - acc: 0.4872 - LRFinder: val_loss: 3.8107 - lr = 0.00236769 \n",
            " 29/390 [=>............................] - ETA: 2:36 - loss: 1.8908 - acc: 0.4873 - LRFinder: val_loss: 3.9144 - lr = 0.00238872 \n",
            " 30/390 [=>............................] - ETA: 2:34 - loss: 1.8898 - acc: 0.4878 - LRFinder: val_loss: 3.8912 - lr = 0.00240974 \n",
            " 31/390 [=>............................] - ETA: 2:33 - loss: 1.8921 - acc: 0.4851 - LRFinder: val_loss: 3.8820 - lr = 0.00243077 \n",
            " 32/390 [=>............................] - ETA: 2:32 - loss: 1.8929 - acc: 0.4839 - LRFinder: val_loss: 3.7267 - lr = 0.00245179 \n",
            " 33/390 [=>............................] - ETA: 2:30 - loss: 1.8921 - acc: 0.4834 - LRFinder: val_loss: 3.8572 - lr = 0.00247282 \n",
            " 34/390 [=>............................] - ETA: 2:29 - loss: 1.8896 - acc: 0.4844 - LRFinder: val_loss: 3.8160 - lr = 0.00249385 \n",
            " 35/390 [=>............................] - ETA: 2:28 - loss: 1.8887 - acc: 0.4844 - LRFinder: val_loss: 3.8804 - lr = 0.00251487 \n",
            " 36/390 [=>............................] - ETA: 2:27 - loss: 1.8854 - acc: 0.4844 - LRFinder: val_loss: 3.8725 - lr = 0.00253590 \n",
            " 37/390 [=>............................] - ETA: 2:26 - loss: 1.8884 - acc: 0.4816 - LRFinder: val_loss: 3.8721 - lr = 0.00255692 \n",
            " 38/390 [=>............................] - ETA: 2:24 - loss: 1.8891 - acc: 0.4799 - LRFinder: val_loss: 3.8087 - lr = 0.00257795 \n",
            " 39/390 [==>...........................] - ETA: 2:23 - loss: 1.8908 - acc: 0.4772 - LRFinder: val_loss: 3.9035 - lr = 0.00259897 \n",
            " 40/390 [==>...........................] - ETA: 2:22 - loss: 1.8912 - acc: 0.4770 - LRFinder: val_loss: 3.6533 - lr = 0.00262000 \n",
            " 41/390 [==>...........................] - ETA: 2:22 - loss: 1.8878 - acc: 0.4788 - LRFinder: val_loss: 3.7348 - lr = 0.00264103 \n",
            " 42/390 [==>...........................] - ETA: 2:21 - loss: 1.8890 - acc: 0.4788 - LRFinder: val_loss: 3.8299 - lr = 0.00266205 \n",
            " 43/390 [==>...........................] - ETA: 2:20 - loss: 1.8881 - acc: 0.4787 - LRFinder: val_loss: 3.8045 - lr = 0.00268308 \n",
            " 44/390 [==>...........................] - ETA: 2:19 - loss: 1.8910 - acc: 0.4783 - LRFinder: val_loss: 3.9612 - lr = 0.00270410 \n",
            " 45/390 [==>...........................] - ETA: 2:18 - loss: 1.8930 - acc: 0.4785 - LRFinder: val_loss: 3.9651 - lr = 0.00272513 \n",
            " 46/390 [==>...........................] - ETA: 2:17 - loss: 1.8951 - acc: 0.4781 - LRFinder: val_loss: 3.8180 - lr = 0.00274615 \n",
            " 47/390 [==>...........................] - ETA: 2:16 - loss: 1.8953 - acc: 0.4792 - LRFinder: val_loss: 4.0540 - lr = 0.00276718 \n",
            " 48/390 [==>...........................] - ETA: 2:15 - loss: 1.8942 - acc: 0.4805 - LRFinder: val_loss: 3.8982 - lr = 0.00278821 \n",
            " 49/390 [==>...........................] - ETA: 2:15 - loss: 1.8933 - acc: 0.4815 - LRFinder: val_loss: 3.8927 - lr = 0.00280923 \n",
            " 50/390 [==>...........................] - ETA: 2:14 - loss: 1.8918 - acc: 0.4811 - LRFinder: val_loss: 3.9275 - lr = 0.00283026 \n",
            " 51/390 [==>...........................] - ETA: 2:13 - loss: 1.8935 - acc: 0.4798 - LRFinder: val_loss: 3.8257 - lr = 0.00285128 \n",
            " 52/390 [===>..........................] - ETA: 2:13 - loss: 1.8907 - acc: 0.4791 - LRFinder: val_loss: 3.8334 - lr = 0.00287231 \n",
            " 53/390 [===>..........................] - ETA: 2:12 - loss: 1.8927 - acc: 0.4777 - LRFinder: val_loss: 3.8020 - lr = 0.00289333 \n",
            " 54/390 [===>..........................] - ETA: 2:11 - loss: 1.8926 - acc: 0.4769 - LRFinder: val_loss: 3.7273 - lr = 0.00291436 \n",
            " 55/390 [===>..........................] - ETA: 2:10 - loss: 1.8952 - acc: 0.4756 - LRFinder: val_loss: 3.6392 - lr = 0.00293538 \n",
            " 56/390 [===>..........................] - ETA: 2:10 - loss: 1.8968 - acc: 0.4760 - LRFinder: val_loss: 3.6194 - lr = 0.00295641 \n",
            " 57/390 [===>..........................] - ETA: 2:09 - loss: 1.8983 - acc: 0.4749 - LRFinder: val_loss: 3.6410 - lr = 0.00297744 \n",
            " 58/390 [===>..........................] - ETA: 2:08 - loss: 1.8976 - acc: 0.4756 - LRFinder: val_loss: 3.7021 - lr = 0.00299846 \n",
            " 59/390 [===>..........................] - ETA: 2:08 - loss: 1.8970 - acc: 0.4758 - LRFinder: val_loss: 3.6115 - lr = 0.00301949 \n",
            " 60/390 [===>..........................] - ETA: 2:07 - loss: 1.8954 - acc: 0.4767 - LRFinder: val_loss: 3.5433 - lr = 0.00304051 \n",
            " 61/390 [===>..........................] - ETA: 2:07 - loss: 1.8981 - acc: 0.4764 - LRFinder: val_loss: 3.4299 - lr = 0.00306154 \n",
            " 62/390 [===>..........................] - ETA: 2:06 - loss: 1.8966 - acc: 0.4772 - LRFinder: val_loss: 3.4336 - lr = 0.00308256 \n",
            " 63/390 [===>..........................] - ETA: 2:05 - loss: 1.8957 - acc: 0.4785 - LRFinder: val_loss: 3.4828 - lr = 0.00310359 \n",
            " 64/390 [===>..........................] - ETA: 2:05 - loss: 1.8950 - acc: 0.4781 - LRFinder: val_loss: 3.4217 - lr = 0.00312462 \n",
            " 65/390 [====>.........................] - ETA: 2:04 - loss: 1.8923 - acc: 0.4802 - LRFinder: val_loss: 3.5907 - lr = 0.00314564 \n",
            " 66/390 [====>.........................] - ETA: 2:03 - loss: 1.8913 - acc: 0.4807 - LRFinder: val_loss: 3.4910 - lr = 0.00316667 \n",
            " 67/390 [====>.........................] - ETA: 2:03 - loss: 1.8922 - acc: 0.4794 - LRFinder: val_loss: 3.5461 - lr = 0.00318769 \n",
            " 68/390 [====>.........................] - ETA: 2:02 - loss: 1.8910 - acc: 0.4805 - LRFinder: val_loss: 3.7552 - lr = 0.00320872 \n",
            " 69/390 [====>.........................] - ETA: 2:02 - loss: 1.8910 - acc: 0.4803 - LRFinder: val_loss: 3.6906 - lr = 0.00322974 \n",
            " 70/390 [====>.........................] - ETA: 2:01 - loss: 1.8929 - acc: 0.4801 - LRFinder: val_loss: 3.7153 - lr = 0.00325077 \n",
            " 71/390 [====>.........................] - ETA: 2:01 - loss: 1.8931 - acc: 0.4794 - LRFinder: val_loss: 3.7452 - lr = 0.00327179 \n",
            " 72/390 [====>.........................] - ETA: 2:00 - loss: 1.8950 - acc: 0.4795 - LRFinder: val_loss: 3.7269 - lr = 0.00329282 \n",
            " 73/390 [====>.........................] - ETA: 2:00 - loss: 1.8949 - acc: 0.4798 - LRFinder: val_loss: 3.5979 - lr = 0.00331385 \n",
            " 74/390 [====>.........................] - ETA: 1:59 - loss: 1.8933 - acc: 0.4802 - LRFinder: val_loss: 3.5870 - lr = 0.00333487 \n",
            " 75/390 [====>.........................] - ETA: 1:58 - loss: 1.8910 - acc: 0.4819 - LRFinder: val_loss: 3.5892 - lr = 0.00335590 \n",
            " 76/390 [====>.........................] - ETA: 1:58 - loss: 1.8902 - acc: 0.4815 - LRFinder: val_loss: 3.4493 - lr = 0.00337692 \n",
            " 77/390 [====>.........................] - ETA: 1:57 - loss: 1.8917 - acc: 0.4807 - LRFinder: val_loss: 3.5743 - lr = 0.00339795 \n",
            " 78/390 [=====>........................] - ETA: 1:57 - loss: 1.8926 - acc: 0.4810 - LRFinder: val_loss: 3.4592 - lr = 0.00341897 \n",
            " 79/390 [=====>........................] - ETA: 1:56 - loss: 1.8923 - acc: 0.4811 - LRFinder: val_loss: 3.5502 - lr = 0.00344000 \n",
            " 80/390 [=====>........................] - ETA: 1:56 - loss: 1.8915 - acc: 0.4817 - LRFinder: val_loss: 3.6291 - lr = 0.00346103 \n",
            " 81/390 [=====>........................] - ETA: 1:55 - loss: 1.8913 - acc: 0.4823 - LRFinder: val_loss: 3.5736 - lr = 0.00348205 \n",
            " 82/390 [=====>........................] - ETA: 1:55 - loss: 1.8905 - acc: 0.4818 - LRFinder: val_loss: 3.7190 - lr = 0.00350308 \n",
            " 83/390 [=====>........................] - ETA: 1:54 - loss: 1.8905 - acc: 0.4823 - LRFinder: val_loss: 3.6122 - lr = 0.00352410 \n",
            " 84/390 [=====>........................] - ETA: 1:54 - loss: 1.8910 - acc: 0.4823 - LRFinder: val_loss: 3.6463 - lr = 0.00354513 \n",
            " 85/390 [=====>........................] - ETA: 1:53 - loss: 1.8916 - acc: 0.4823 - LRFinder: val_loss: 3.6178 - lr = 0.00356615 \n",
            " 86/390 [=====>........................] - ETA: 1:53 - loss: 1.8912 - acc: 0.4835 - LRFinder: val_loss: 3.6124 - lr = 0.00358718 \n",
            " 87/390 [=====>........................] - ETA: 1:52 - loss: 1.8903 - acc: 0.4837 - LRFinder: val_loss: 3.6256 - lr = 0.00360821 \n",
            " 88/390 [=====>........................] - ETA: 1:52 - loss: 1.8890 - acc: 0.4838 - LRFinder: val_loss: 3.4877 - lr = 0.00362923 \n",
            " 89/390 [=====>........................] - ETA: 1:51 - loss: 1.8885 - acc: 0.4844 - LRFinder: val_loss: 3.5456 - lr = 0.00365026 \n",
            " 90/390 [=====>........................] - ETA: 1:51 - loss: 1.8869 - acc: 0.4849 - LRFinder: val_loss: 3.5598 - lr = 0.00367128 \n",
            " 91/390 [======>.......................] - ETA: 1:51 - loss: 1.8871 - acc: 0.4848 - LRFinder: val_loss: 3.4455 - lr = 0.00369231 \n",
            " 92/390 [======>.......................] - ETA: 1:50 - loss: 1.8891 - acc: 0.4845 - LRFinder: val_loss: 3.3304 - lr = 0.00371333 \n",
            " 93/390 [======>.......................] - ETA: 1:50 - loss: 1.8874 - acc: 0.4853 - LRFinder: val_loss: 3.6182 - lr = 0.00373436 \n",
            " 94/390 [======>.......................] - ETA: 1:49 - loss: 1.8871 - acc: 0.4856 - LRFinder: val_loss: 3.4947 - lr = 0.00375538 \n",
            " 95/390 [======>.......................] - ETA: 1:49 - loss: 1.8859 - acc: 0.4860 - LRFinder: val_loss: 3.5896 - lr = 0.00377641 \n",
            " 96/390 [======>.......................] - ETA: 1:48 - loss: 1.8846 - acc: 0.4866 - LRFinder: val_loss: 3.6380 - lr = 0.00379744 \n",
            " 97/390 [======>.......................] - ETA: 1:48 - loss: 1.8862 - acc: 0.4862 - LRFinder: val_loss: 3.5906 - lr = 0.00381846 \n",
            " 98/390 [======>.......................] - ETA: 1:47 - loss: 1.8857 - acc: 0.4865 - LRFinder: val_loss: 3.6001 - lr = 0.00383949 \n",
            " 99/390 [======>.......................] - ETA: 1:47 - loss: 1.8857 - acc: 0.4869 - LRFinder: val_loss: 3.6208 - lr = 0.00386051 \n",
            "100/390 [======>.......................] - ETA: 1:46 - loss: 1.8865 - acc: 0.4865 - LRFinder: val_loss: 3.6589 - lr = 0.00388154 \n",
            "101/390 [======>.......................] - ETA: 1:46 - loss: 1.8858 - acc: 0.4872 - LRFinder: val_loss: 3.6456 - lr = 0.00390256 \n",
            "102/390 [======>.......................] - ETA: 1:46 - loss: 1.8863 - acc: 0.4867 - LRFinder: val_loss: 3.6446 - lr = 0.00392359 \n",
            "103/390 [======>.......................] - ETA: 1:45 - loss: 1.8857 - acc: 0.4873 - LRFinder: val_loss: 3.7517 - lr = 0.00394462 \n",
            "104/390 [=======>......................] - ETA: 1:45 - loss: 1.8850 - acc: 0.4873 - LRFinder: val_loss: 3.6009 - lr = 0.00396564 \n",
            "105/390 [=======>......................] - ETA: 1:44 - loss: 1.8857 - acc: 0.4868 - LRFinder: val_loss: 3.7746 - lr = 0.00398667 \n",
            "106/390 [=======>......................] - ETA: 1:44 - loss: 1.8865 - acc: 0.4866 - LRFinder: val_loss: 3.7676 - lr = 0.00400769 \n",
            "107/390 [=======>......................] - ETA: 1:43 - loss: 1.8866 - acc: 0.4864 - LRFinder: val_loss: 3.9111 - lr = 0.00402872 \n",
            "108/390 [=======>......................] - ETA: 1:43 - loss: 1.8867 - acc: 0.4860 - LRFinder: val_loss: 3.7533 - lr = 0.00404974 \n",
            "109/390 [=======>......................] - ETA: 1:43 - loss: 1.8860 - acc: 0.4865 - LRFinder: val_loss: 3.8143 - lr = 0.00407077 \n",
            "110/390 [=======>......................] - ETA: 1:42 - loss: 1.8840 - acc: 0.4869 - LRFinder: val_loss: 3.6720 - lr = 0.00409179 \n",
            "111/390 [=======>......................] - ETA: 1:42 - loss: 1.8838 - acc: 0.4871 - LRFinder: val_loss: 3.7555 - lr = 0.00411282 \n",
            "112/390 [=======>......................] - ETA: 1:41 - loss: 1.8837 - acc: 0.4865 - LRFinder: val_loss: 3.6423 - lr = 0.00413385 \n",
            "113/390 [=======>......................] - ETA: 1:41 - loss: 1.8851 - acc: 0.4857 - LRFinder: val_loss: 3.6914 - lr = 0.00415487 \n",
            "114/390 [=======>......................] - ETA: 1:40 - loss: 1.8843 - acc: 0.4857 - LRFinder: val_loss: 3.5863 - lr = 0.00417590 \n",
            "115/390 [=======>......................] - ETA: 1:40 - loss: 1.8837 - acc: 0.4857 - LRFinder: val_loss: 3.5877 - lr = 0.00419692 \n",
            "116/390 [=======>......................] - ETA: 1:40 - loss: 1.8835 - acc: 0.4858 - LRFinder: val_loss: 3.6562 - lr = 0.00421795 \n",
            "117/390 [========>.....................] - ETA: 1:39 - loss: 1.8836 - acc: 0.4857 - LRFinder: val_loss: 3.4343 - lr = 0.00423897 \n",
            "118/390 [========>.....................] - ETA: 1:39 - loss: 1.8828 - acc: 0.4858 - LRFinder: val_loss: 3.5418 - lr = 0.00426000 \n",
            "119/390 [========>.....................] - ETA: 1:38 - loss: 1.8841 - acc: 0.4850 - LRFinder: val_loss: 3.4691 - lr = 0.00428103 \n",
            "120/390 [========>.....................] - ETA: 1:38 - loss: 1.8843 - acc: 0.4851 - LRFinder: val_loss: 3.5108 - lr = 0.00430205 \n",
            "121/390 [========>.....................] - ETA: 1:37 - loss: 1.8825 - acc: 0.4858 - LRFinder: val_loss: 3.7037 - lr = 0.00432308 \n",
            "122/390 [========>.....................] - ETA: 1:37 - loss: 1.8819 - acc: 0.4853 - LRFinder: val_loss: 3.8062 - lr = 0.00434410 \n",
            "123/390 [========>.....................] - ETA: 1:37 - loss: 1.8825 - acc: 0.4848 - LRFinder: val_loss: 3.7750 - lr = 0.00436513 \n",
            "124/390 [========>.....................] - ETA: 1:36 - loss: 1.8823 - acc: 0.4849 - LRFinder: val_loss: 3.7558 - lr = 0.00438615 \n",
            "125/390 [========>.....................] - ETA: 1:36 - loss: 1.8833 - acc: 0.4847 - LRFinder: val_loss: 3.5124 - lr = 0.00440718 \n",
            "126/390 [========>.....................] - ETA: 1:35 - loss: 1.8833 - acc: 0.4849 - LRFinder: val_loss: 3.7003 - lr = 0.00442821 \n",
            "127/390 [========>.....................] - ETA: 1:35 - loss: 1.8835 - acc: 0.4851 - LRFinder: val_loss: 3.8061 - lr = 0.00444923 \n",
            "128/390 [========>.....................] - ETA: 1:35 - loss: 1.8829 - acc: 0.4850 - LRFinder: val_loss: 3.7088 - lr = 0.00447026 \n",
            "129/390 [========>.....................] - ETA: 1:34 - loss: 1.8836 - acc: 0.4852 - LRFinder: val_loss: 3.7078 - lr = 0.00449128 \n",
            "130/390 [=========>....................] - ETA: 1:34 - loss: 1.8834 - acc: 0.4852 - LRFinder: val_loss: 3.6028 - lr = 0.00451231 \n",
            "131/390 [=========>....................] - ETA: 1:33 - loss: 1.8828 - acc: 0.4854 - LRFinder: val_loss: 3.4860 - lr = 0.00453333 \n",
            "132/390 [=========>....................] - ETA: 1:33 - loss: 1.8830 - acc: 0.4853 - LRFinder: val_loss: 3.3135 - lr = 0.00455436 \n",
            "133/390 [=========>....................] - ETA: 1:33 - loss: 1.8827 - acc: 0.4855 - LRFinder: val_loss: 3.3111 - lr = 0.00457538 \n",
            "134/390 [=========>....................] - ETA: 1:32 - loss: 1.8830 - acc: 0.4856 - LRFinder: val_loss: 3.3401 - lr = 0.00459641 \n",
            "135/390 [=========>....................] - ETA: 1:32 - loss: 1.8837 - acc: 0.4854 - LRFinder: val_loss: 3.4148 - lr = 0.00461744 \n",
            "136/390 [=========>....................] - ETA: 1:31 - loss: 1.8828 - acc: 0.4859 - LRFinder: val_loss: 3.4219 - lr = 0.00463846 \n",
            "137/390 [=========>....................] - ETA: 1:31 - loss: 1.8819 - acc: 0.4859 - LRFinder: val_loss: 3.3802 - lr = 0.00465949 \n",
            "138/390 [=========>....................] - ETA: 1:31 - loss: 1.8824 - acc: 0.4857 - LRFinder: val_loss: 3.4666 - lr = 0.00468051 \n",
            "139/390 [=========>....................] - ETA: 1:30 - loss: 1.8838 - acc: 0.4854 - LRFinder: val_loss: 3.7032 - lr = 0.00470154 \n",
            "140/390 [=========>....................] - ETA: 1:30 - loss: 1.8827 - acc: 0.4854 - LRFinder: val_loss: 3.7164 - lr = 0.00472256 \n",
            "141/390 [=========>....................] - ETA: 1:29 - loss: 1.8827 - acc: 0.4856 - LRFinder: val_loss: 3.6715 - lr = 0.00474359 \n",
            "142/390 [=========>....................] - ETA: 1:29 - loss: 1.8826 - acc: 0.4858 - LRFinder: val_loss: 3.5386 - lr = 0.00476462 \n",
            "143/390 [==========>...................] - ETA: 1:29 - loss: 1.8826 - acc: 0.4860 - LRFinder: val_loss: 3.4925 - lr = 0.00478564 \n",
            "144/390 [==========>...................] - ETA: 1:28 - loss: 1.8823 - acc: 0.4859 - LRFinder: val_loss: 3.4109 - lr = 0.00480667 \n",
            "145/390 [==========>...................] - ETA: 1:28 - loss: 1.8821 - acc: 0.4860 - LRFinder: val_loss: 3.3334 - lr = 0.00482769 \n",
            "146/390 [==========>...................] - ETA: 1:27 - loss: 1.8816 - acc: 0.4864 - LRFinder: val_loss: 3.3291 - lr = 0.00484872 \n",
            "147/390 [==========>...................] - ETA: 1:27 - loss: 1.8817 - acc: 0.4864 - LRFinder: val_loss: 3.4273 - lr = 0.00486974 \n",
            "148/390 [==========>...................] - ETA: 1:27 - loss: 1.8819 - acc: 0.4864 - LRFinder: val_loss: 3.2850 - lr = 0.00489077 \n",
            "149/390 [==========>...................] - ETA: 1:26 - loss: 1.8813 - acc: 0.4867 - LRFinder: val_loss: 3.3992 - lr = 0.00491179 \n",
            "150/390 [==========>...................] - ETA: 1:26 - loss: 1.8805 - acc: 0.4870 - LRFinder: val_loss: 3.4129 - lr = 0.00493282 \n",
            "151/390 [==========>...................] - ETA: 1:25 - loss: 1.8803 - acc: 0.4873 - LRFinder: val_loss: 3.4701 - lr = 0.00495385 \n",
            "152/390 [==========>...................] - ETA: 1:25 - loss: 1.8796 - acc: 0.4871 - LRFinder: val_loss: 3.6378 - lr = 0.00497487 \n",
            "153/390 [==========>...................] - ETA: 1:25 - loss: 1.8791 - acc: 0.4876 - LRFinder: val_loss: 3.6702 - lr = 0.00499590 \n",
            "154/390 [==========>...................] - ETA: 1:24 - loss: 1.8788 - acc: 0.4876 - LRFinder: val_loss: 3.6892 - lr = 0.00501692 \n",
            "155/390 [==========>...................] - ETA: 1:24 - loss: 1.8781 - acc: 0.4880 - LRFinder: val_loss: 3.7246 - lr = 0.00503795 \n",
            "156/390 [===========>..................] - ETA: 1:24 - loss: 1.8782 - acc: 0.4879 - LRFinder: val_loss: 3.7761 - lr = 0.00505897 \n",
            "157/390 [===========>..................] - ETA: 1:23 - loss: 1.8773 - acc: 0.4879 - LRFinder: val_loss: 3.6119 - lr = 0.00508000 \n",
            "158/390 [===========>..................] - ETA: 1:23 - loss: 1.8772 - acc: 0.4878 - LRFinder: val_loss: 3.5257 - lr = 0.00510103 \n",
            "159/390 [===========>..................] - ETA: 1:22 - loss: 1.8771 - acc: 0.4878 - LRFinder: val_loss: 3.4892 - lr = 0.00512205 \n",
            "160/390 [===========>..................] - ETA: 1:22 - loss: 1.8778 - acc: 0.4876 - LRFinder: val_loss: 3.6165 - lr = 0.00514308 \n",
            "161/390 [===========>..................] - ETA: 1:22 - loss: 1.8785 - acc: 0.4874 - LRFinder: val_loss: 3.5618 - lr = 0.00516410 \n",
            "162/390 [===========>..................] - ETA: 1:21 - loss: 1.8775 - acc: 0.4877 - LRFinder: val_loss: 3.6132 - lr = 0.00518513 \n",
            "163/390 [===========>..................] - ETA: 1:21 - loss: 1.8771 - acc: 0.4880 - LRFinder: val_loss: 3.6124 - lr = 0.00520615 \n",
            "164/390 [===========>..................] - ETA: 1:20 - loss: 1.8773 - acc: 0.4881 - LRFinder: val_loss: 3.7568 - lr = 0.00522718 \n",
            "165/390 [===========>..................] - ETA: 1:20 - loss: 1.8771 - acc: 0.4884 - LRFinder: val_loss: 3.7179 - lr = 0.00524821 \n",
            "166/390 [===========>..................] - ETA: 1:20 - loss: 1.8767 - acc: 0.4888 - LRFinder: val_loss: 3.6974 - lr = 0.00526923 \n",
            "167/390 [===========>..................] - ETA: 1:19 - loss: 1.8761 - acc: 0.4887 - LRFinder: val_loss: 3.5350 - lr = 0.00529026 \n",
            "168/390 [===========>..................] - ETA: 1:19 - loss: 1.8760 - acc: 0.4887 - LRFinder: val_loss: 3.6014 - lr = 0.00531128 \n",
            "169/390 [============>.................] - ETA: 1:19 - loss: 1.8761 - acc: 0.4886 - LRFinder: val_loss: 3.5480 - lr = 0.00533231 \n",
            "170/390 [============>.................] - ETA: 1:18 - loss: 1.8758 - acc: 0.4886 - LRFinder: val_loss: 3.4682 - lr = 0.00535333 \n",
            "171/390 [============>.................] - ETA: 1:18 - loss: 1.8769 - acc: 0.4882 - LRFinder: val_loss: 3.6652 - lr = 0.00537436 \n",
            "172/390 [============>.................] - ETA: 1:17 - loss: 1.8766 - acc: 0.4887 - LRFinder: val_loss: 3.5922 - lr = 0.00539538 \n",
            "173/390 [============>.................] - ETA: 1:17 - loss: 1.8770 - acc: 0.4884 - LRFinder: val_loss: 3.4745 - lr = 0.00541641 \n",
            "174/390 [============>.................] - ETA: 1:17 - loss: 1.8765 - acc: 0.4885 - LRFinder: val_loss: 3.5947 - lr = 0.00543744 \n",
            "175/390 [============>.................] - ETA: 1:16 - loss: 1.8762 - acc: 0.4888 - LRFinder: val_loss: 3.5891 - lr = 0.00545846 \n",
            "176/390 [============>.................] - ETA: 1:16 - loss: 1.8770 - acc: 0.4885 - LRFinder: val_loss: 3.5212 - lr = 0.00547949 \n",
            "177/390 [============>.................] - ETA: 1:16 - loss: 1.8770 - acc: 0.4886 - LRFinder: val_loss: 3.6211 - lr = 0.00550051 \n",
            "178/390 [============>.................] - ETA: 1:15 - loss: 1.8768 - acc: 0.4881 - LRFinder: val_loss: 3.8875 - lr = 0.00552154 \n",
            "179/390 [============>.................] - ETA: 1:15 - loss: 1.8766 - acc: 0.4881 - LRFinder: val_loss: 3.7494 - lr = 0.00554256 \n",
            "180/390 [============>.................] - ETA: 1:14 - loss: 1.8762 - acc: 0.4878 - LRFinder: val_loss: 3.8871 - lr = 0.00556359 \n",
            "181/390 [============>.................] - ETA: 1:14 - loss: 1.8765 - acc: 0.4877 - LRFinder: val_loss: 3.9400 - lr = 0.00558462 \n",
            "182/390 [=============>................] - ETA: 1:14 - loss: 1.8770 - acc: 0.4876 - LRFinder: val_loss: 3.7674 - lr = 0.00560564 \n",
            "183/390 [=============>................] - ETA: 1:13 - loss: 1.8770 - acc: 0.4876 - LRFinder: val_loss: 3.7138 - lr = 0.00562667 \n",
            "184/390 [=============>................] - ETA: 1:13 - loss: 1.8770 - acc: 0.4879 - LRFinder: val_loss: 3.7233 - lr = 0.00564769 \n",
            "185/390 [=============>................] - ETA: 1:13 - loss: 1.8764 - acc: 0.4883 - LRFinder: val_loss: 3.8998 - lr = 0.00566872 \n",
            "186/390 [=============>................] - ETA: 1:12 - loss: 1.8762 - acc: 0.4884 - LRFinder: val_loss: 3.8330 - lr = 0.00568974 \n",
            "187/390 [=============>................] - ETA: 1:12 - loss: 1.8758 - acc: 0.4886 - LRFinder: val_loss: 4.0331 - lr = 0.00571077 \n",
            "188/390 [=============>................] - ETA: 1:11 - loss: 1.8757 - acc: 0.4887 - LRFinder: val_loss: 3.8366 - lr = 0.00573179 \n",
            "189/390 [=============>................] - ETA: 1:11 - loss: 1.8763 - acc: 0.4886 - LRFinder: val_loss: 3.9681 - lr = 0.00575282 \n",
            "190/390 [=============>................] - ETA: 1:11 - loss: 1.8762 - acc: 0.4886 - LRFinder: val_loss: 3.8808 - lr = 0.00577385 \n",
            "191/390 [=============>................] - ETA: 1:10 - loss: 1.8761 - acc: 0.4887 - LRFinder: val_loss: 3.8534 - lr = 0.00579487 \n",
            "192/390 [=============>................] - ETA: 1:10 - loss: 1.8758 - acc: 0.4889 - LRFinder: val_loss: 4.1065 - lr = 0.00581590 \n",
            "193/390 [=============>................] - ETA: 1:10 - loss: 1.8751 - acc: 0.4893 - LRFinder: val_loss: 3.9681 - lr = 0.00583692 \n",
            "194/390 [=============>................] - ETA: 1:09 - loss: 1.8759 - acc: 0.4889 - LRFinder: val_loss: 3.8610 - lr = 0.00585795 \n",
            "195/390 [==============>...............] - ETA: 1:09 - loss: 1.8755 - acc: 0.4891 - LRFinder: val_loss: 3.9094 - lr = 0.00587897 \n",
            "196/390 [==============>...............] - ETA: 1:08 - loss: 1.8748 - acc: 0.4894 - LRFinder: val_loss: 4.0758 - lr = 0.00590000 \n",
            "197/390 [==============>...............] - ETA: 1:08 - loss: 1.8749 - acc: 0.4893 - LRFinder: val_loss: 4.0200 - lr = 0.00592103 \n",
            "198/390 [==============>...............] - ETA: 1:08 - loss: 1.8751 - acc: 0.4893 - LRFinder: val_loss: 4.1211 - lr = 0.00594205 \n",
            "199/390 [==============>...............] - ETA: 1:07 - loss: 1.8752 - acc: 0.4890 - LRFinder: val_loss: 4.0887 - lr = 0.00596308 \n",
            "200/390 [==============>...............] - ETA: 1:07 - loss: 1.8754 - acc: 0.4887 - LRFinder: val_loss: 4.1670 - lr = 0.00598410 \n",
            "201/390 [==============>...............] - ETA: 1:07 - loss: 1.8756 - acc: 0.4883 - LRFinder: val_loss: 4.0113 - lr = 0.00600513 \n",
            "202/390 [==============>...............] - ETA: 1:06 - loss: 1.8746 - acc: 0.4886 - LRFinder: val_loss: 4.1046 - lr = 0.00602615 \n",
            "203/390 [==============>...............] - ETA: 1:06 - loss: 1.8738 - acc: 0.4887 - LRFinder: val_loss: 4.3150 - lr = 0.00604718 \n",
            "204/390 [==============>...............] - ETA: 1:06 - loss: 1.8731 - acc: 0.4891 - LRFinder: val_loss: 4.3380 - lr = 0.00606821 \n",
            "205/390 [==============>...............] - ETA: 1:05 - loss: 1.8735 - acc: 0.4886 - LRFinder: val_loss: 4.0273 - lr = 0.00608923 \n",
            "206/390 [==============>...............] - ETA: 1:05 - loss: 1.8736 - acc: 0.4888 - LRFinder: val_loss: 4.1828 - lr = 0.00611026 \n",
            "207/390 [==============>...............] - ETA: 1:04 - loss: 1.8734 - acc: 0.4890 - LRFinder: val_loss: 4.0931 - lr = 0.00613128 \n",
            "208/390 [===============>..............] - ETA: 1:04 - loss: 1.8738 - acc: 0.4890 - LRFinder: val_loss: 3.9050 - lr = 0.00615231 \n",
            "209/390 [===============>..............] - ETA: 1:04 - loss: 1.8736 - acc: 0.4891 - LRFinder: val_loss: 4.0794 - lr = 0.00617333 \n",
            "210/390 [===============>..............] - ETA: 1:03 - loss: 1.8733 - acc: 0.4892 - LRFinder: val_loss: 4.1307 - lr = 0.00619436 \n",
            "211/390 [===============>..............] - ETA: 1:03 - loss: 1.8728 - acc: 0.4896 - LRFinder: val_loss: 4.0792 - lr = 0.00621538 \n",
            "212/390 [===============>..............] - ETA: 1:03 - loss: 1.8727 - acc: 0.4896 - LRFinder: val_loss: 4.1903 - lr = 0.00623641 \n",
            "213/390 [===============>..............] - ETA: 1:02 - loss: 1.8723 - acc: 0.4898 - LRFinder: val_loss: 4.4865 - lr = 0.00625744 \n",
            "214/390 [===============>..............] - ETA: 1:02 - loss: 1.8725 - acc: 0.4896 - LRFinder: val_loss: 4.4623 - lr = 0.00627846 \n",
            "215/390 [===============>..............] - ETA: 1:02 - loss: 1.8726 - acc: 0.4895 - LRFinder: val_loss: 4.1696 - lr = 0.00629949 \n",
            "216/390 [===============>..............] - ETA: 1:01 - loss: 1.8723 - acc: 0.4895 - LRFinder: val_loss: 4.2753 - lr = 0.00632051 \n",
            "217/390 [===============>..............] - ETA: 1:01 - loss: 1.8718 - acc: 0.4899 - LRFinder: val_loss: 4.3282 - lr = 0.00634154 \n",
            "218/390 [===============>..............] - ETA: 1:00 - loss: 1.8713 - acc: 0.4899 - LRFinder: val_loss: 4.2474 - lr = 0.00636256 \n",
            "219/390 [===============>..............] - ETA: 1:00 - loss: 1.8707 - acc: 0.4899 - LRFinder: val_loss: 4.3500 - lr = 0.00638359 \n",
            "220/390 [===============>..............] - ETA: 1:00 - loss: 1.8703 - acc: 0.4903 - LRFinder: val_loss: 4.1776 - lr = 0.00640462 \n",
            "221/390 [================>.............] - ETA: 59s - loss: 1.8707 - acc: 0.4902  - LRFinder: val_loss: 4.1916 - lr = 0.00642564 \n",
            "222/390 [================>.............] - ETA: 59s - loss: 1.8706 - acc: 0.4903 - LRFinder: val_loss: 4.3096 - lr = 0.00644667 \n",
            "223/390 [================>.............] - ETA: 59s - loss: 1.8697 - acc: 0.4905 - LRFinder: val_loss: 4.2704 - lr = 0.00646769 \n",
            "224/390 [================>.............] - ETA: 58s - loss: 1.8698 - acc: 0.4905 - LRFinder: val_loss: 4.4075 - lr = 0.00648872 \n",
            "225/390 [================>.............] - ETA: 58s - loss: 1.8701 - acc: 0.4903 - LRFinder: val_loss: 4.3746 - lr = 0.00650974 \n",
            "226/390 [================>.............] - ETA: 58s - loss: 1.8704 - acc: 0.4901 - LRFinder: val_loss: 4.4523 - lr = 0.00653077 \n",
            "227/390 [================>.............] - ETA: 57s - loss: 1.8694 - acc: 0.4904 - LRFinder: val_loss: 4.2332 - lr = 0.00655179 \n",
            "228/390 [================>.............] - ETA: 57s - loss: 1.8694 - acc: 0.4903 - LRFinder: val_loss: 4.0689 - lr = 0.00657282 \n",
            "229/390 [================>.............] - ETA: 56s - loss: 1.8689 - acc: 0.4905 - LRFinder: val_loss: 4.1890 - lr = 0.00659385 \n",
            "230/390 [================>.............] - ETA: 56s - loss: 1.8683 - acc: 0.4908 - LRFinder: val_loss: 4.0271 - lr = 0.00661487 \n",
            "231/390 [================>.............] - ETA: 56s - loss: 1.8679 - acc: 0.4909 - LRFinder: val_loss: 4.2502 - lr = 0.00663590 \n",
            "232/390 [================>.............] - ETA: 55s - loss: 1.8672 - acc: 0.4912 - LRFinder: val_loss: 4.0961 - lr = 0.00665692 \n",
            "233/390 [================>.............] - ETA: 55s - loss: 1.8670 - acc: 0.4912 - LRFinder: val_loss: 3.9160 - lr = 0.00667795 \n",
            "234/390 [=================>............] - ETA: 55s - loss: 1.8679 - acc: 0.4909 - LRFinder: val_loss: 3.9184 - lr = 0.00669897 \n",
            "235/390 [=================>............] - ETA: 54s - loss: 1.8677 - acc: 0.4907 - LRFinder: val_loss: 3.7655 - lr = 0.00672000 \n",
            "236/390 [=================>............] - ETA: 54s - loss: 1.8666 - acc: 0.4913 - LRFinder: val_loss: 3.7339 - lr = 0.00674103 \n",
            "237/390 [=================>............] - ETA: 54s - loss: 1.8674 - acc: 0.4909 - LRFinder: val_loss: 3.7365 - lr = 0.00676205 \n",
            "238/390 [=================>............] - ETA: 53s - loss: 1.8676 - acc: 0.4909 - LRFinder: val_loss: 3.6319 - lr = 0.00678308 \n",
            "239/390 [=================>............] - ETA: 53s - loss: 1.8671 - acc: 0.4909 - LRFinder: val_loss: 3.5935 - lr = 0.00680410 \n",
            "240/390 [=================>............] - ETA: 52s - loss: 1.8670 - acc: 0.4910 - LRFinder: val_loss: 3.6327 - lr = 0.00682513 \n",
            "241/390 [=================>............] - ETA: 52s - loss: 1.8671 - acc: 0.4910 - LRFinder: val_loss: 3.7790 - lr = 0.00684615 \n",
            "242/390 [=================>............] - ETA: 52s - loss: 1.8675 - acc: 0.4909 - LRFinder: val_loss: 3.8908 - lr = 0.00686718 \n",
            "243/390 [=================>............] - ETA: 51s - loss: 1.8678 - acc: 0.4907 - LRFinder: val_loss: 3.6536 - lr = 0.00688821 \n",
            "244/390 [=================>............] - ETA: 51s - loss: 1.8681 - acc: 0.4908 - LRFinder: val_loss: 3.7197 - lr = 0.00690923 \n",
            "245/390 [=================>............] - ETA: 51s - loss: 1.8687 - acc: 0.4908 - LRFinder: val_loss: 4.0062 - lr = 0.00693026 \n",
            "246/390 [=================>............] - ETA: 50s - loss: 1.8684 - acc: 0.4908 - LRFinder: val_loss: 3.6794 - lr = 0.00695128 \n",
            "247/390 [==================>...........] - ETA: 50s - loss: 1.8681 - acc: 0.4910 - LRFinder: val_loss: 3.9681 - lr = 0.00697231 \n",
            "248/390 [==================>...........] - ETA: 50s - loss: 1.8675 - acc: 0.4913 - LRFinder: val_loss: 4.0576 - lr = 0.00699333 \n",
            "249/390 [==================>...........] - ETA: 49s - loss: 1.8673 - acc: 0.4915 - LRFinder: val_loss: 4.0588 - lr = 0.00701436 \n",
            "250/390 [==================>...........] - ETA: 49s - loss: 1.8668 - acc: 0.4919 - LRFinder: val_loss: 4.0910 - lr = 0.00703538 \n",
            "251/390 [==================>...........] - ETA: 49s - loss: 1.8668 - acc: 0.4918 - LRFinder: val_loss: 3.9153 - lr = 0.00705641 \n",
            "252/390 [==================>...........] - ETA: 48s - loss: 1.8671 - acc: 0.4916 - LRFinder: val_loss: 3.9463 - lr = 0.00707744 \n",
            "253/390 [==================>...........] - ETA: 48s - loss: 1.8673 - acc: 0.4913 - LRFinder: val_loss: 3.8633 - lr = 0.00709846 \n",
            "254/390 [==================>...........] - ETA: 47s - loss: 1.8671 - acc: 0.4913 - LRFinder: val_loss: 3.7987 - lr = 0.00711949 \n",
            "255/390 [==================>...........] - ETA: 47s - loss: 1.8667 - acc: 0.4915 - LRFinder: val_loss: 3.7331 - lr = 0.00714051 \n",
            "256/390 [==================>...........] - ETA: 47s - loss: 1.8671 - acc: 0.4911 - LRFinder: val_loss: 3.7463 - lr = 0.00716154 \n",
            "257/390 [==================>...........] - ETA: 46s - loss: 1.8666 - acc: 0.4912 - LRFinder: val_loss: 3.8418 - lr = 0.00718256 \n",
            "258/390 [==================>...........] - ETA: 46s - loss: 1.8661 - acc: 0.4912 - LRFinder: val_loss: 3.8285 - lr = 0.00720359 \n",
            "259/390 [==================>...........] - ETA: 46s - loss: 1.8656 - acc: 0.4916 - LRFinder: val_loss: 3.8643 - lr = 0.00722462 \n",
            "260/390 [===================>..........] - ETA: 45s - loss: 1.8656 - acc: 0.4917 - LRFinder: val_loss: 3.7458 - lr = 0.00724564 \n",
            "261/390 [===================>..........] - ETA: 45s - loss: 1.8648 - acc: 0.4919 - LRFinder: val_loss: 3.7916 - lr = 0.00726667 \n",
            "262/390 [===================>..........] - ETA: 45s - loss: 1.8644 - acc: 0.4919 - LRFinder: val_loss: 3.6919 - lr = 0.00728769 \n",
            "263/390 [===================>..........] - ETA: 44s - loss: 1.8646 - acc: 0.4920 - LRFinder: val_loss: 3.5890 - lr = 0.00730872 \n",
            "264/390 [===================>..........] - ETA: 44s - loss: 1.8642 - acc: 0.4920 - LRFinder: val_loss: 3.4902 - lr = 0.00732974 \n",
            "265/390 [===================>..........] - ETA: 44s - loss: 1.8638 - acc: 0.4920 - LRFinder: val_loss: 3.3323 - lr = 0.00735077 \n",
            "266/390 [===================>..........] - ETA: 43s - loss: 1.8635 - acc: 0.4921 - LRFinder: val_loss: 3.4553 - lr = 0.00737179 \n",
            "267/390 [===================>..........] - ETA: 43s - loss: 1.8630 - acc: 0.4924 - LRFinder: val_loss: 3.6953 - lr = 0.00739282 \n",
            "268/390 [===================>..........] - ETA: 42s - loss: 1.8628 - acc: 0.4924 - LRFinder: val_loss: 3.5500 - lr = 0.00741385 \n",
            "269/390 [===================>..........] - ETA: 42s - loss: 1.8625 - acc: 0.4924 - LRFinder: val_loss: 3.5923 - lr = 0.00743487 \n",
            "270/390 [===================>..........] - ETA: 42s - loss: 1.8629 - acc: 0.4924 - LRFinder: val_loss: 3.7040 - lr = 0.00745590 \n",
            "271/390 [===================>..........] - ETA: 41s - loss: 1.8628 - acc: 0.4924 - LRFinder: val_loss: 3.8057 - lr = 0.00747692 \n",
            "272/390 [===================>..........] - ETA: 41s - loss: 1.8622 - acc: 0.4928 - LRFinder: val_loss: 3.8428 - lr = 0.00749795 \n",
            "273/390 [====================>.........] - ETA: 41s - loss: 1.8621 - acc: 0.4928 - LRFinder: val_loss: 3.7042 - lr = 0.00751897 \n",
            "274/390 [====================>.........] - ETA: 40s - loss: 1.8623 - acc: 0.4928 - LRFinder: val_loss: 3.8285 - lr = 0.00754000 \n",
            "275/390 [====================>.........] - ETA: 40s - loss: 1.8620 - acc: 0.4929 - LRFinder: val_loss: 3.6899 - lr = 0.00756103 \n",
            "276/390 [====================>.........] - ETA: 40s - loss: 1.8619 - acc: 0.4930 - LRFinder: val_loss: 3.6937 - lr = 0.00758205 \n",
            "277/390 [====================>.........] - ETA: 39s - loss: 1.8623 - acc: 0.4928 - LRFinder: val_loss: 3.7365 - lr = 0.00760308 \n",
            "278/390 [====================>.........] - ETA: 39s - loss: 1.8617 - acc: 0.4931 - LRFinder: val_loss: 3.6679 - lr = 0.00762410 \n",
            "279/390 [====================>.........] - ETA: 39s - loss: 1.8615 - acc: 0.4932 - LRFinder: val_loss: 3.7383 - lr = 0.00764513 \n",
            "280/390 [====================>.........] - ETA: 38s - loss: 1.8613 - acc: 0.4932 - LRFinder: val_loss: 3.7790 - lr = 0.00766615 \n",
            "281/390 [====================>.........] - ETA: 38s - loss: 1.8612 - acc: 0.4932 - LRFinder: val_loss: 3.7693 - lr = 0.00768718 \n",
            "282/390 [====================>.........] - ETA: 37s - loss: 1.8615 - acc: 0.4933 - LRFinder: val_loss: 3.8778 - lr = 0.00770821 \n",
            "283/390 [====================>.........] - ETA: 37s - loss: 1.8614 - acc: 0.4933 - LRFinder: val_loss: 3.9500 - lr = 0.00772923 \n",
            "284/390 [====================>.........] - ETA: 37s - loss: 1.8612 - acc: 0.4935 - LRFinder: val_loss: 3.8555 - lr = 0.00775026 \n",
            "285/390 [====================>.........] - ETA: 36s - loss: 1.8612 - acc: 0.4935 - LRFinder: val_loss: 3.8970 - lr = 0.00777128 \n",
            "286/390 [=====================>........] - ETA: 36s - loss: 1.8610 - acc: 0.4936 - LRFinder: val_loss: 3.8449 - lr = 0.00779231 \n",
            "287/390 [=====================>........] - ETA: 36s - loss: 1.8611 - acc: 0.4935 - LRFinder: val_loss: 3.8762 - lr = 0.00781333 \n",
            "288/390 [=====================>........] - ETA: 35s - loss: 1.8609 - acc: 0.4936 - LRFinder: val_loss: 3.9203 - lr = 0.00783436 \n",
            "289/390 [=====================>........] - ETA: 35s - loss: 1.8605 - acc: 0.4936 - LRFinder: val_loss: 3.9199 - lr = 0.00785538 \n",
            "290/390 [=====================>........] - ETA: 35s - loss: 1.8605 - acc: 0.4935 - LRFinder: val_loss: 3.9649 - lr = 0.00787641 \n",
            "291/390 [=====================>........] - ETA: 34s - loss: 1.8596 - acc: 0.4937 - LRFinder: val_loss: 4.0307 - lr = 0.00789744 \n",
            "292/390 [=====================>........] - ETA: 34s - loss: 1.8593 - acc: 0.4938 - LRFinder: val_loss: 3.8036 - lr = 0.00791846 \n",
            "293/390 [=====================>........] - ETA: 34s - loss: 1.8595 - acc: 0.4938 - LRFinder: val_loss: 3.8739 - lr = 0.00793949 \n",
            "294/390 [=====================>........] - ETA: 33s - loss: 1.8592 - acc: 0.4940 - LRFinder: val_loss: 3.7955 - lr = 0.00796051 \n",
            "295/390 [=====================>........] - ETA: 33s - loss: 1.8595 - acc: 0.4938 - LRFinder: val_loss: 3.7806 - lr = 0.00798154 \n",
            "296/390 [=====================>........] - ETA: 33s - loss: 1.8595 - acc: 0.4937 - LRFinder: val_loss: 3.5893 - lr = 0.00800256 \n",
            "297/390 [=====================>........] - ETA: 32s - loss: 1.8592 - acc: 0.4939 - LRFinder: val_loss: 3.5554 - lr = 0.00802359 \n",
            "298/390 [=====================>........] - ETA: 32s - loss: 1.8589 - acc: 0.4941 - LRFinder: val_loss: 3.6771 - lr = 0.00804462 \n",
            "299/390 [======================>.......] - ETA: 31s - loss: 1.8588 - acc: 0.4940 - LRFinder: val_loss: 3.7733 - lr = 0.00806564 \n",
            "300/390 [======================>.......] - ETA: 31s - loss: 1.8589 - acc: 0.4938 - LRFinder: val_loss: 4.0138 - lr = 0.00808667 \n",
            "301/390 [======================>.......] - ETA: 31s - loss: 1.8587 - acc: 0.4938 - LRFinder: val_loss: 3.9025 - lr = 0.00810769 \n",
            "302/390 [======================>.......] - ETA: 30s - loss: 1.8585 - acc: 0.4939 - LRFinder: val_loss: 3.9649 - lr = 0.00812872 \n",
            "303/390 [======================>.......] - ETA: 30s - loss: 1.8581 - acc: 0.4941 - LRFinder: val_loss: 3.9145 - lr = 0.00814974 \n",
            "304/390 [======================>.......] - ETA: 30s - loss: 1.8573 - acc: 0.4943 - LRFinder: val_loss: 3.8414 - lr = 0.00817077 \n",
            "305/390 [======================>.......] - ETA: 29s - loss: 1.8571 - acc: 0.4945 - LRFinder: val_loss: 3.9325 - lr = 0.00819179 \n",
            "306/390 [======================>.......] - ETA: 29s - loss: 1.8568 - acc: 0.4946 - LRFinder: val_loss: 3.8850 - lr = 0.00821282 \n",
            "307/390 [======================>.......] - ETA: 29s - loss: 1.8566 - acc: 0.4947 - LRFinder: val_loss: 4.0566 - lr = 0.00823385 \n",
            "308/390 [======================>.......] - ETA: 28s - loss: 1.8563 - acc: 0.4948 - LRFinder: val_loss: 4.0026 - lr = 0.00825487 \n",
            "309/390 [======================>.......] - ETA: 28s - loss: 1.8558 - acc: 0.4948 - LRFinder: val_loss: 3.9003 - lr = 0.00827590 \n",
            "310/390 [======================>.......] - ETA: 28s - loss: 1.8555 - acc: 0.4949 - LRFinder: val_loss: 3.9661 - lr = 0.00829692 \n",
            "311/390 [======================>.......] - ETA: 27s - loss: 1.8556 - acc: 0.4949 - LRFinder: val_loss: 3.8678 - lr = 0.00831795 \n",
            "312/390 [=======================>......] - ETA: 27s - loss: 1.8557 - acc: 0.4949 - LRFinder: val_loss: 3.8925 - lr = 0.00833897 \n",
            "313/390 [=======================>......] - ETA: 26s - loss: 1.8554 - acc: 0.4951 - LRFinder: val_loss: 3.9826 - lr = 0.00836000 \n",
            "314/390 [=======================>......] - ETA: 26s - loss: 1.8555 - acc: 0.4952 - LRFinder: val_loss: 3.8564 - lr = 0.00838103 \n",
            "315/390 [=======================>......] - ETA: 26s - loss: 1.8549 - acc: 0.4954 - LRFinder: val_loss: 3.9674 - lr = 0.00840205 \n",
            "316/390 [=======================>......] - ETA: 25s - loss: 1.8550 - acc: 0.4954 - LRFinder: val_loss: 4.0736 - lr = 0.00842308 \n",
            "317/390 [=======================>......] - ETA: 25s - loss: 1.8549 - acc: 0.4953 - LRFinder: val_loss: 4.0637 - lr = 0.00844410 \n",
            "318/390 [=======================>......] - ETA: 25s - loss: 1.8550 - acc: 0.4954 - LRFinder: val_loss: 4.1091 - lr = 0.00846513 \n",
            "319/390 [=======================>......] - ETA: 24s - loss: 1.8553 - acc: 0.4954 - LRFinder: val_loss: 3.9743 - lr = 0.00848615 \n",
            "320/390 [=======================>......] - ETA: 24s - loss: 1.8550 - acc: 0.4954 - LRFinder: val_loss: 3.9358 - lr = 0.00850718 \n",
            "321/390 [=======================>......] - ETA: 24s - loss: 1.8548 - acc: 0.4954 - LRFinder: val_loss: 4.0821 - lr = 0.00852821 \n",
            "322/390 [=======================>......] - ETA: 23s - loss: 1.8542 - acc: 0.4956 - LRFinder: val_loss: 4.0627 - lr = 0.00854923 \n",
            "323/390 [=======================>......] - ETA: 23s - loss: 1.8542 - acc: 0.4956 - LRFinder: val_loss: 4.3756 - lr = 0.00857026 \n",
            "324/390 [=======================>......] - ETA: 23s - loss: 1.8541 - acc: 0.4958 - LRFinder: val_loss: 4.2216 - lr = 0.00859128 \n",
            "325/390 [========================>.....] - ETA: 22s - loss: 1.8539 - acc: 0.4958 - LRFinder: val_loss: 4.3263 - lr = 0.00861231 \n",
            "326/390 [========================>.....] - ETA: 22s - loss: 1.8539 - acc: 0.4959 - LRFinder: val_loss: 4.2023 - lr = 0.00863333 \n",
            "327/390 [========================>.....] - ETA: 22s - loss: 1.8538 - acc: 0.4958 - LRFinder: val_loss: 3.9665 - lr = 0.00865436 \n",
            "328/390 [========================>.....] - ETA: 21s - loss: 1.8538 - acc: 0.4960 - LRFinder: val_loss: 3.9747 - lr = 0.00867538 \n",
            "329/390 [========================>.....] - ETA: 21s - loss: 1.8536 - acc: 0.4962 - LRFinder: val_loss: 4.0871 - lr = 0.00869641 \n",
            "330/390 [========================>.....] - ETA: 21s - loss: 1.8532 - acc: 0.4964 - LRFinder: val_loss: 4.0473 - lr = 0.00871744 \n",
            "331/390 [========================>.....] - ETA: 20s - loss: 1.8531 - acc: 0.4962 - LRFinder: val_loss: 4.2799 - lr = 0.00873846 \n",
            "332/390 [========================>.....] - ETA: 20s - loss: 1.8529 - acc: 0.4963 - LRFinder: val_loss: 4.3936 - lr = 0.00875949 \n",
            "333/390 [========================>.....] - ETA: 19s - loss: 1.8531 - acc: 0.4962 - LRFinder: val_loss: 4.2923 - lr = 0.00878051 \n",
            "334/390 [========================>.....] - ETA: 19s - loss: 1.8528 - acc: 0.4963 - LRFinder: val_loss: 4.0343 - lr = 0.00880154 \n",
            "335/390 [========================>.....] - ETA: 19s - loss: 1.8526 - acc: 0.4965 - LRFinder: val_loss: 4.1904 - lr = 0.00882256 \n",
            "336/390 [========================>.....] - ETA: 18s - loss: 1.8529 - acc: 0.4962 - LRFinder: val_loss: 3.9347 - lr = 0.00884359 \n",
            "337/390 [========================>.....] - ETA: 18s - loss: 1.8527 - acc: 0.4961 - LRFinder: val_loss: 4.2465 - lr = 0.00886462 \n",
            "338/390 [=========================>....] - ETA: 18s - loss: 1.8527 - acc: 0.4961 - LRFinder: val_loss: 4.2190 - lr = 0.00888564 \n",
            "339/390 [=========================>....] - ETA: 17s - loss: 1.8526 - acc: 0.4962 - LRFinder: val_loss: 4.0457 - lr = 0.00890667 \n",
            "340/390 [=========================>....] - ETA: 17s - loss: 1.8523 - acc: 0.4963 - LRFinder: val_loss: 3.7695 - lr = 0.00892769 \n",
            "341/390 [=========================>....] - ETA: 17s - loss: 1.8525 - acc: 0.4962 - LRFinder: val_loss: 3.9431 - lr = 0.00894872 \n",
            "342/390 [=========================>....] - ETA: 16s - loss: 1.8518 - acc: 0.4965 - LRFinder: val_loss: 3.8449 - lr = 0.00896974 \n",
            "343/390 [=========================>....] - ETA: 16s - loss: 1.8516 - acc: 0.4966 - LRFinder: val_loss: 3.8715 - lr = 0.00899077 \n",
            "344/390 [=========================>....] - ETA: 16s - loss: 1.8511 - acc: 0.4967 - LRFinder: val_loss: 3.7916 - lr = 0.00901179 \n",
            "345/390 [=========================>....] - ETA: 15s - loss: 1.8513 - acc: 0.4965 - LRFinder: val_loss: 3.9167 - lr = 0.00903282 \n",
            "346/390 [=========================>....] - ETA: 15s - loss: 1.8511 - acc: 0.4967 - LRFinder: val_loss: 3.9310 - lr = 0.00905385 \n",
            "347/390 [=========================>....] - ETA: 15s - loss: 1.8513 - acc: 0.4967 - LRFinder: val_loss: 3.7544 - lr = 0.00907487 \n",
            "348/390 [=========================>....] - ETA: 14s - loss: 1.8510 - acc: 0.4968 - LRFinder: val_loss: 3.6038 - lr = 0.00909590 \n",
            "349/390 [=========================>....] - ETA: 14s - loss: 1.8505 - acc: 0.4969 - LRFinder: val_loss: 3.8169 - lr = 0.00911692 \n",
            "350/390 [=========================>....] - ETA: 13s - loss: 1.8500 - acc: 0.4972 - LRFinder: val_loss: 3.6547 - lr = 0.00913795 \n",
            "351/390 [==========================>...] - ETA: 13s - loss: 1.8496 - acc: 0.4973 - LRFinder: val_loss: 3.8678 - lr = 0.00915897 \n",
            "352/390 [==========================>...] - ETA: 13s - loss: 1.8495 - acc: 0.4973 - LRFinder: val_loss: 3.7021 - lr = 0.00918000 \n",
            "353/390 [==========================>...] - ETA: 12s - loss: 1.8492 - acc: 0.4974 - LRFinder: val_loss: 3.6933 - lr = 0.00920103 \n",
            "354/390 [==========================>...] - ETA: 12s - loss: 1.8489 - acc: 0.4974 - LRFinder: val_loss: 3.6384 - lr = 0.00922205 \n",
            "355/390 [==========================>...] - ETA: 12s - loss: 1.8488 - acc: 0.4974 - LRFinder: val_loss: 3.4216 - lr = 0.00924308 \n",
            "356/390 [==========================>...] - ETA: 11s - loss: 1.8482 - acc: 0.4977 - LRFinder: val_loss: 3.6281 - lr = 0.00926410 \n",
            "357/390 [==========================>...] - ETA: 11s - loss: 1.8481 - acc: 0.4979 - LRFinder: val_loss: 3.6845 - lr = 0.00928513 \n",
            "358/390 [==========================>...] - ETA: 11s - loss: 1.8480 - acc: 0.4980 - LRFinder: val_loss: 3.6293 - lr = 0.00930615 \n",
            "359/390 [==========================>...] - ETA: 10s - loss: 1.8481 - acc: 0.4981 - LRFinder: val_loss: 3.8528 - lr = 0.00932718 \n",
            "360/390 [==========================>...] - ETA: 10s - loss: 1.8477 - acc: 0.4982 - LRFinder: val_loss: 3.7357 - lr = 0.00934821 \n",
            "361/390 [==========================>...] - ETA: 10s - loss: 1.8474 - acc: 0.4983 - LRFinder: val_loss: 3.6922 - lr = 0.00936923 \n",
            "362/390 [==========================>...] - ETA: 9s - loss: 1.8469 - acc: 0.4985  - LRFinder: val_loss: 3.8155 - lr = 0.00939026 \n",
            "363/390 [==========================>...] - ETA: 9s - loss: 1.8470 - acc: 0.4985 - LRFinder: val_loss: 3.8199 - lr = 0.00941128 \n",
            "364/390 [===========================>..] - ETA: 9s - loss: 1.8467 - acc: 0.4985 - LRFinder: val_loss: 3.6663 - lr = 0.00943231 \n",
            "365/390 [===========================>..] - ETA: 8s - loss: 1.8464 - acc: 0.4986 - LRFinder: val_loss: 3.6906 - lr = 0.00945333 \n",
            "366/390 [===========================>..] - ETA: 8s - loss: 1.8464 - acc: 0.4986 - LRFinder: val_loss: 3.7468 - lr = 0.00947436 \n",
            "367/390 [===========================>..] - ETA: 8s - loss: 1.8463 - acc: 0.4986 - LRFinder: val_loss: 3.8326 - lr = 0.00949538 \n",
            "368/390 [===========================>..] - ETA: 7s - loss: 1.8463 - acc: 0.4985 - LRFinder: val_loss: 3.9143 - lr = 0.00951641 \n",
            "369/390 [===========================>..] - ETA: 7s - loss: 1.8462 - acc: 0.4987 - LRFinder: val_loss: 4.1989 - lr = 0.00953744 \n",
            "370/390 [===========================>..] - ETA: 6s - loss: 1.8458 - acc: 0.4989 - LRFinder: val_loss: 4.4100 - lr = 0.00955846 \n",
            "371/390 [===========================>..] - ETA: 6s - loss: 1.8460 - acc: 0.4988 - LRFinder: val_loss: 4.2385 - lr = 0.00957949 \n",
            "372/390 [===========================>..] - ETA: 6s - loss: 1.8454 - acc: 0.4989 - LRFinder: val_loss: 4.2211 - lr = 0.00960051 \n",
            "373/390 [===========================>..] - ETA: 5s - loss: 1.8451 - acc: 0.4989 - LRFinder: val_loss: 3.9792 - lr = 0.00962154 \n",
            "374/390 [===========================>..] - ETA: 5s - loss: 1.8447 - acc: 0.4989 - LRFinder: val_loss: 4.2154 - lr = 0.00964256 \n",
            "375/390 [===========================>..] - ETA: 5s - loss: 1.8442 - acc: 0.4990 - LRFinder: val_loss: 4.2026 - lr = 0.00966359 \n",
            "376/390 [===========================>..] - ETA: 4s - loss: 1.8441 - acc: 0.4991 - LRFinder: val_loss: 4.3880 - lr = 0.00968462 \n",
            "377/390 [============================>.] - ETA: 4s - loss: 1.8438 - acc: 0.4993 - LRFinder: val_loss: 4.3578 - lr = 0.00970564 \n",
            "378/390 [============================>.] - ETA: 4s - loss: 1.8439 - acc: 0.4993 - LRFinder: val_loss: 4.2862 - lr = 0.00972667 \n",
            "379/390 [============================>.] - ETA: 3s - loss: 1.8433 - acc: 0.4995 - LRFinder: val_loss: 4.2017 - lr = 0.00974769 \n",
            "380/390 [============================>.] - ETA: 3s - loss: 1.8434 - acc: 0.4995 - LRFinder: val_loss: 4.1590 - lr = 0.00976872 \n",
            "381/390 [============================>.] - ETA: 3s - loss: 1.8431 - acc: 0.4995 - LRFinder: val_loss: 3.9055 - lr = 0.00978974 \n",
            "382/390 [============================>.] - ETA: 2s - loss: 1.8432 - acc: 0.4996 - LRFinder: val_loss: 3.8330 - lr = 0.00981077 \n",
            "383/390 [============================>.] - ETA: 2s - loss: 1.8429 - acc: 0.4997 - LRFinder: val_loss: 3.8271 - lr = 0.00983179 \n",
            "384/390 [============================>.] - ETA: 2s - loss: 1.8426 - acc: 0.4998 - LRFinder: val_loss: 3.6724 - lr = 0.00985282 \n",
            "385/390 [============================>.] - ETA: 1s - loss: 1.8420 - acc: 0.4999 - LRFinder: val_loss: 3.6605 - lr = 0.00987385 \n",
            "386/390 [============================>.] - ETA: 1s - loss: 1.8420 - acc: 0.5001 - LRFinder: val_loss: 3.7345 - lr = 0.00989487 \n",
            "387/390 [============================>.] - ETA: 1s - loss: 1.8421 - acc: 0.5000 - LRFinder: val_loss: 3.8143 - lr = 0.00991590 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 1.8419 - acc: 0.5001 - LRFinder: val_loss: 3.6800 - lr = 0.00993692 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.8416 - acc: 0.5003 - LRFinder: val_loss: 3.5110 - lr = 0.00995795 \n",
            " - LRFinder: val_loss: 3.5790 - lr = 0.00997897 \n",
            "390/390 [==============================] - 139s 357ms/step - loss: 1.8413 - acc: 0.5002 - val_loss: 3.5282 - val_acc: 0.1593\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {weights/momentum/momentum-0.95/}\n",
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 15:21 - loss: 1.8409 - acc: 0.4922 - LRFinder: val_loss: 3.5827 - lr = 0.00180000 \n",
            "  2/390 [..............................] - ETA: 10:48 - loss: 1.7405 - acc: 0.5430 - LRFinder: val_loss: 3.5812 - lr = 0.00182103 \n",
            "  3/390 [..............................] - ETA: 8:01 - loss: 1.7604 - acc: 0.5286  - LRFinder: val_loss: 3.5916 - lr = 0.00184205 \n",
            "  4/390 [..............................] - ETA: 6:33 - loss: 1.7232 - acc: 0.5469 - LRFinder: val_loss: 3.5581 - lr = 0.00186308 \n",
            "  5/390 [..............................] - ETA: 5:40 - loss: 1.7518 - acc: 0.5344 - LRFinder: val_loss: 3.6833 - lr = 0.00188410 \n",
            "  6/390 [..............................] - ETA: 5:05 - loss: 1.7349 - acc: 0.5365 - LRFinder: val_loss: 3.7472 - lr = 0.00190513 \n",
            "  7/390 [..............................] - ETA: 4:40 - loss: 1.7139 - acc: 0.5458 - LRFinder: val_loss: 3.5770 - lr = 0.00192615 \n",
            "  8/390 [..............................] - ETA: 4:20 - loss: 1.7200 - acc: 0.5449 - LRFinder: val_loss: 3.7091 - lr = 0.00194718 \n",
            "  9/390 [..............................] - ETA: 4:05 - loss: 1.7190 - acc: 0.5469 - LRFinder: val_loss: 3.7611 - lr = 0.00196821 \n",
            " 10/390 [..............................] - ETA: 3:53 - loss: 1.7190 - acc: 0.5453 - LRFinder: val_loss: 3.7843 - lr = 0.00198923 \n",
            " 11/390 [..............................] - ETA: 3:43 - loss: 1.7211 - acc: 0.5412 - LRFinder: val_loss: 3.6850 - lr = 0.00201026 \n",
            " 12/390 [..............................] - ETA: 3:35 - loss: 1.7146 - acc: 0.5410 - LRFinder: val_loss: 3.6672 - lr = 0.00203128 \n",
            " 13/390 [>.............................] - ETA: 3:28 - loss: 1.7208 - acc: 0.5421 - LRFinder: val_loss: 3.5704 - lr = 0.00205231 \n",
            " 14/390 [>.............................] - ETA: 3:22 - loss: 1.7232 - acc: 0.5396 - LRFinder: val_loss: 3.5154 - lr = 0.00207333 \n",
            " 15/390 [>.............................] - ETA: 3:16 - loss: 1.7348 - acc: 0.5370 - LRFinder: val_loss: 3.4127 - lr = 0.00209436 \n",
            " 16/390 [>.............................] - ETA: 3:11 - loss: 1.7261 - acc: 0.5400 - LRFinder: val_loss: 3.4395 - lr = 0.00211538 \n",
            " 17/390 [>.............................] - ETA: 3:07 - loss: 1.7233 - acc: 0.5404 - LRFinder: val_loss: 3.4641 - lr = 0.00213641 \n",
            " 18/390 [>.............................] - ETA: 3:03 - loss: 1.7253 - acc: 0.5425 - LRFinder: val_loss: 3.4547 - lr = 0.00215744 \n",
            " 19/390 [>.............................] - ETA: 3:00 - loss: 1.7354 - acc: 0.5399 - LRFinder: val_loss: 3.4418 - lr = 0.00217846 \n",
            " 20/390 [>.............................] - ETA: 2:57 - loss: 1.7319 - acc: 0.5391 - LRFinder: val_loss: 3.5104 - lr = 0.00219949 \n",
            " 21/390 [>.............................] - ETA: 2:54 - loss: 1.7381 - acc: 0.5394 - LRFinder: val_loss: 3.5542 - lr = 0.00222051 \n",
            " 22/390 [>.............................] - ETA: 2:51 - loss: 1.7402 - acc: 0.5384 - LRFinder: val_loss: 3.4347 - lr = 0.00224154 \n",
            " 23/390 [>.............................] - ETA: 2:49 - loss: 1.7324 - acc: 0.5391 - LRFinder: val_loss: 3.5250 - lr = 0.00226256 \n",
            " 24/390 [>.............................] - ETA: 2:46 - loss: 1.7376 - acc: 0.5384 - LRFinder: val_loss: 3.4710 - lr = 0.00228359 \n",
            " 25/390 [>.............................] - ETA: 2:44 - loss: 1.7442 - acc: 0.5356 - LRFinder: val_loss: 3.4771 - lr = 0.00230462 \n",
            " 26/390 [=>............................] - ETA: 2:42 - loss: 1.7477 - acc: 0.5361 - LRFinder: val_loss: 3.4681 - lr = 0.00232564 \n",
            " 27/390 [=>............................] - ETA: 2:40 - loss: 1.7485 - acc: 0.5347 - LRFinder: val_loss: 3.5491 - lr = 0.00234667 \n",
            " 28/390 [=>............................] - ETA: 2:39 - loss: 1.7464 - acc: 0.5357 - LRFinder: val_loss: 3.4340 - lr = 0.00236769 \n",
            " 29/390 [=>............................] - ETA: 2:37 - loss: 1.7430 - acc: 0.5361 - LRFinder: val_loss: 3.6620 - lr = 0.00238872 \n",
            " 30/390 [=>............................] - ETA: 2:35 - loss: 1.7388 - acc: 0.5372 - LRFinder: val_loss: 3.4767 - lr = 0.00240974 \n",
            " 31/390 [=>............................] - ETA: 2:34 - loss: 1.7458 - acc: 0.5348 - LRFinder: val_loss: 3.6119 - lr = 0.00243077 \n",
            " 32/390 [=>............................] - ETA: 2:33 - loss: 1.7415 - acc: 0.5378 - LRFinder: val_loss: 3.5809 - lr = 0.00245179 \n",
            " 33/390 [=>............................] - ETA: 2:31 - loss: 1.7399 - acc: 0.5384 - LRFinder: val_loss: 3.5218 - lr = 0.00247282 \n",
            " 34/390 [=>............................] - ETA: 2:30 - loss: 1.7358 - acc: 0.5388 - LRFinder: val_loss: 3.5488 - lr = 0.00249385 \n",
            " 35/390 [=>............................] - ETA: 2:29 - loss: 1.7361 - acc: 0.5382 - LRFinder: val_loss: 3.5088 - lr = 0.00251487 \n",
            " 36/390 [=>............................] - ETA: 2:27 - loss: 1.7379 - acc: 0.5369 - LRFinder: val_loss: 3.6057 - lr = 0.00253590 \n",
            " 37/390 [=>............................] - ETA: 2:26 - loss: 1.7368 - acc: 0.5382 - LRFinder: val_loss: 3.5362 - lr = 0.00255692 \n",
            " 38/390 [=>............................] - ETA: 2:25 - loss: 1.7366 - acc: 0.5399 - LRFinder: val_loss: 3.5493 - lr = 0.00257795 \n",
            " 39/390 [==>...........................] - ETA: 2:24 - loss: 1.7342 - acc: 0.5389 - LRFinder: val_loss: 3.4871 - lr = 0.00259897 \n",
            " 40/390 [==>...........................] - ETA: 2:23 - loss: 1.7288 - acc: 0.5406 - LRFinder: val_loss: 3.4630 - lr = 0.00262000 \n",
            " 41/390 [==>...........................] - ETA: 2:22 - loss: 1.7301 - acc: 0.5408 - LRFinder: val_loss: 3.5041 - lr = 0.00264103 \n",
            " 42/390 [==>...........................] - ETA: 2:21 - loss: 1.7328 - acc: 0.5404 - LRFinder: val_loss: 3.4954 - lr = 0.00266205 \n",
            " 43/390 [==>...........................] - ETA: 2:20 - loss: 1.7315 - acc: 0.5411 - LRFinder: val_loss: 3.4227 - lr = 0.00268308 \n",
            " 44/390 [==>...........................] - ETA: 2:19 - loss: 1.7368 - acc: 0.5400 - LRFinder: val_loss: 3.5434 - lr = 0.00270410 \n",
            " 45/390 [==>...........................] - ETA: 2:19 - loss: 1.7376 - acc: 0.5399 - LRFinder: val_loss: 3.5777 - lr = 0.00272513 \n",
            " 46/390 [==>...........................] - ETA: 2:18 - loss: 1.7398 - acc: 0.5399 - LRFinder: val_loss: 3.4231 - lr = 0.00274615 \n",
            " 47/390 [==>...........................] - ETA: 2:17 - loss: 1.7378 - acc: 0.5407 - LRFinder: val_loss: 3.5097 - lr = 0.00276718 \n",
            " 48/390 [==>...........................] - ETA: 2:16 - loss: 1.7395 - acc: 0.5391 - LRFinder: val_loss: 3.4602 - lr = 0.00278821 \n",
            " 49/390 [==>...........................] - ETA: 2:15 - loss: 1.7396 - acc: 0.5378 - LRFinder: val_loss: 3.6024 - lr = 0.00280923 \n",
            " 50/390 [==>...........................] - ETA: 2:14 - loss: 1.7390 - acc: 0.5375 - LRFinder: val_loss: 3.6730 - lr = 0.00283026 \n",
            " 51/390 [==>...........................] - ETA: 2:14 - loss: 1.7398 - acc: 0.5386 - LRFinder: val_loss: 3.4376 - lr = 0.00285128 \n",
            " 52/390 [===>..........................] - ETA: 2:13 - loss: 1.7373 - acc: 0.5397 - LRFinder: val_loss: 3.6345 - lr = 0.00287231 \n",
            " 53/390 [===>..........................] - ETA: 2:12 - loss: 1.7364 - acc: 0.5392 - LRFinder: val_loss: 3.5417 - lr = 0.00289333 \n",
            " 54/390 [===>..........................] - ETA: 2:11 - loss: 1.7371 - acc: 0.5389 - LRFinder: val_loss: 3.6472 - lr = 0.00291436 \n",
            " 55/390 [===>..........................] - ETA: 2:11 - loss: 1.7379 - acc: 0.5384 - LRFinder: val_loss: 3.6762 - lr = 0.00293538 \n",
            " 56/390 [===>..........................] - ETA: 2:10 - loss: 1.7389 - acc: 0.5379 - LRFinder: val_loss: 3.8216 - lr = 0.00295641 \n",
            " 57/390 [===>..........................] - ETA: 2:09 - loss: 1.7403 - acc: 0.5369 - LRFinder: val_loss: 3.7124 - lr = 0.00297744 \n",
            " 58/390 [===>..........................] - ETA: 2:09 - loss: 1.7397 - acc: 0.5365 - LRFinder: val_loss: 3.8370 - lr = 0.00299846 \n",
            " 59/390 [===>..........................] - ETA: 2:08 - loss: 1.7407 - acc: 0.5361 - LRFinder: val_loss: 3.9659 - lr = 0.00301949 \n",
            " 60/390 [===>..........................] - ETA: 2:07 - loss: 1.7385 - acc: 0.5374 - LRFinder: val_loss: 3.8312 - lr = 0.00304051 \n",
            " 61/390 [===>..........................] - ETA: 2:07 - loss: 1.7379 - acc: 0.5370 - LRFinder: val_loss: 4.2278 - lr = 0.00306154 \n",
            " 62/390 [===>..........................] - ETA: 2:06 - loss: 1.7360 - acc: 0.5378 - LRFinder: val_loss: 4.1944 - lr = 0.00308256 \n",
            " 63/390 [===>..........................] - ETA: 2:06 - loss: 1.7351 - acc: 0.5381 - LRFinder: val_loss: 4.1684 - lr = 0.00310359 \n",
            " 64/390 [===>..........................] - ETA: 2:05 - loss: 1.7359 - acc: 0.5371 - LRFinder: val_loss: 4.2016 - lr = 0.00312462 \n",
            " 65/390 [====>.........................] - ETA: 2:04 - loss: 1.7356 - acc: 0.5370 - LRFinder: val_loss: 4.2461 - lr = 0.00314564 \n",
            " 66/390 [====>.........................] - ETA: 2:04 - loss: 1.7341 - acc: 0.5375 - LRFinder: val_loss: 4.3603 - lr = 0.00316667 \n",
            " 67/390 [====>.........................] - ETA: 2:03 - loss: 1.7363 - acc: 0.5368 - LRFinder: val_loss: 4.2253 - lr = 0.00318769 \n",
            " 68/390 [====>.........................] - ETA: 2:03 - loss: 1.7348 - acc: 0.5362 - LRFinder: val_loss: 4.3248 - lr = 0.00320872 \n",
            " 69/390 [====>.........................] - ETA: 2:02 - loss: 1.7313 - acc: 0.5379 - LRFinder: val_loss: 4.4609 - lr = 0.00322974 \n",
            " 70/390 [====>.........................] - ETA: 2:01 - loss: 1.7325 - acc: 0.5382 - LRFinder: val_loss: 4.3271 - lr = 0.00325077 \n",
            " 71/390 [====>.........................] - ETA: 2:01 - loss: 1.7318 - acc: 0.5382 - LRFinder: val_loss: 4.5576 - lr = 0.00327179 \n",
            " 72/390 [====>.........................] - ETA: 2:00 - loss: 1.7304 - acc: 0.5392 - LRFinder: val_loss: 4.2182 - lr = 0.00329282 \n",
            " 73/390 [====>.........................] - ETA: 2:00 - loss: 1.7301 - acc: 0.5394 - LRFinder: val_loss: 4.1942 - lr = 0.00331385 \n",
            " 74/390 [====>.........................] - ETA: 1:59 - loss: 1.7286 - acc: 0.5395 - LRFinder: val_loss: 4.1465 - lr = 0.00333487 \n",
            " 75/390 [====>.........................] - ETA: 1:59 - loss: 1.7290 - acc: 0.5393 - LRFinder: val_loss: 4.2116 - lr = 0.00335590 \n",
            " 76/390 [====>.........................] - ETA: 1:58 - loss: 1.7308 - acc: 0.5388 - LRFinder: val_loss: 4.2904 - lr = 0.00337692 \n",
            " 77/390 [====>.........................] - ETA: 1:58 - loss: 1.7302 - acc: 0.5393 - LRFinder: val_loss: 4.1941 - lr = 0.00339795 \n",
            " 78/390 [=====>........................] - ETA: 1:57 - loss: 1.7303 - acc: 0.5389 - LRFinder: val_loss: 4.1730 - lr = 0.00341897 \n",
            " 79/390 [=====>........................] - ETA: 1:57 - loss: 1.7292 - acc: 0.5392 - LRFinder: val_loss: 4.2215 - lr = 0.00344000 \n",
            " 80/390 [=====>........................] - ETA: 1:56 - loss: 1.7296 - acc: 0.5389 - LRFinder: val_loss: 4.1648 - lr = 0.00346103 \n",
            " 81/390 [=====>........................] - ETA: 1:56 - loss: 1.7299 - acc: 0.5393 - LRFinder: val_loss: 4.1312 - lr = 0.00348205 \n",
            " 82/390 [=====>........................] - ETA: 1:55 - loss: 1.7292 - acc: 0.5393 - LRFinder: val_loss: 4.0589 - lr = 0.00350308 \n",
            " 83/390 [=====>........................] - ETA: 1:55 - loss: 1.7285 - acc: 0.5398 - LRFinder: val_loss: 4.1142 - lr = 0.00352410 \n",
            " 84/390 [=====>........................] - ETA: 1:54 - loss: 1.7281 - acc: 0.5397 - LRFinder: val_loss: 4.1371 - lr = 0.00354513 \n",
            " 85/390 [=====>........................] - ETA: 1:54 - loss: 1.7279 - acc: 0.5401 - LRFinder: val_loss: 4.0955 - lr = 0.00356615 \n",
            " 86/390 [=====>........................] - ETA: 1:53 - loss: 1.7292 - acc: 0.5402 - LRFinder: val_loss: 3.9517 - lr = 0.00358718 \n",
            " 87/390 [=====>........................] - ETA: 1:53 - loss: 1.7292 - acc: 0.5401 - LRFinder: val_loss: 3.9946 - lr = 0.00360821 \n",
            " 88/390 [=====>........................] - ETA: 1:52 - loss: 1.7287 - acc: 0.5408 - LRFinder: val_loss: 3.9441 - lr = 0.00362923 \n",
            " 89/390 [=====>........................] - ETA: 1:52 - loss: 1.7293 - acc: 0.5405 - LRFinder: val_loss: 4.1090 - lr = 0.00365026 \n",
            " 90/390 [=====>........................] - ETA: 1:51 - loss: 1.7301 - acc: 0.5401 - LRFinder: val_loss: 4.1937 - lr = 0.00367128 \n",
            " 91/390 [======>.......................] - ETA: 1:51 - loss: 1.7285 - acc: 0.5410 - LRFinder: val_loss: 4.1288 - lr = 0.00369231 \n",
            " 92/390 [======>.......................] - ETA: 1:50 - loss: 1.7276 - acc: 0.5416 - LRFinder: val_loss: 4.1208 - lr = 0.00371333 \n",
            " 93/390 [======>.......................] - ETA: 1:50 - loss: 1.7281 - acc: 0.5419 - LRFinder: val_loss: 4.3819 - lr = 0.00373436 \n",
            " 94/390 [======>.......................] - ETA: 1:49 - loss: 1.7283 - acc: 0.5417 - LRFinder: val_loss: 4.3835 - lr = 0.00375538 \n",
            " 95/390 [======>.......................] - ETA: 1:49 - loss: 1.7272 - acc: 0.5416 - LRFinder: val_loss: 4.4843 - lr = 0.00377641 \n",
            " 96/390 [======>.......................] - ETA: 1:48 - loss: 1.7275 - acc: 0.5419 - LRFinder: val_loss: 4.3832 - lr = 0.00379744 \n",
            " 97/390 [======>.......................] - ETA: 1:48 - loss: 1.7254 - acc: 0.5427 - LRFinder: val_loss: 4.3376 - lr = 0.00381846 \n",
            " 98/390 [======>.......................] - ETA: 1:48 - loss: 1.7247 - acc: 0.5430 - LRFinder: val_loss: 4.2714 - lr = 0.00383949 \n",
            " 99/390 [======>.......................] - ETA: 1:47 - loss: 1.7246 - acc: 0.5434 - LRFinder: val_loss: 4.2484 - lr = 0.00386051 \n",
            "100/390 [======>.......................] - ETA: 1:47 - loss: 1.7243 - acc: 0.5430 - LRFinder: val_loss: 4.3921 - lr = 0.00388154 \n",
            "101/390 [======>.......................] - ETA: 1:46 - loss: 1.7242 - acc: 0.5429 - LRFinder: val_loss: 4.2633 - lr = 0.00390256 \n",
            "102/390 [======>.......................] - ETA: 1:46 - loss: 1.7233 - acc: 0.5430 - LRFinder: val_loss: 4.2183 - lr = 0.00392359 \n",
            "103/390 [======>.......................] - ETA: 1:45 - loss: 1.7241 - acc: 0.5427 - LRFinder: val_loss: 4.3640 - lr = 0.00394462 \n",
            "104/390 [=======>......................] - ETA: 1:45 - loss: 1.7240 - acc: 0.5432 - LRFinder: val_loss: 4.2638 - lr = 0.00396564 \n",
            "105/390 [=======>......................] - ETA: 1:44 - loss: 1.7245 - acc: 0.5428 - LRFinder: val_loss: 4.1588 - lr = 0.00398667 \n",
            "106/390 [=======>......................] - ETA: 1:44 - loss: 1.7237 - acc: 0.5435 - LRFinder: val_loss: 4.3114 - lr = 0.00400769 \n",
            "107/390 [=======>......................] - ETA: 1:44 - loss: 1.7240 - acc: 0.5431 - LRFinder: val_loss: 4.7314 - lr = 0.00402872 \n",
            "108/390 [=======>......................] - ETA: 1:43 - loss: 1.7247 - acc: 0.5425 - LRFinder: val_loss: 4.9628 - lr = 0.00404974 \n",
            "109/390 [=======>......................] - ETA: 1:43 - loss: 1.7251 - acc: 0.5429 - LRFinder: val_loss: 4.8999 - lr = 0.00407077 \n",
            "110/390 [=======>......................] - ETA: 1:42 - loss: 1.7265 - acc: 0.5420 - LRFinder: val_loss: 5.0057 - lr = 0.00409179 \n",
            "111/390 [=======>......................] - ETA: 1:42 - loss: 1.7266 - acc: 0.5421 - LRFinder: val_loss: 5.3362 - lr = 0.00411282 \n",
            "112/390 [=======>......................] - ETA: 1:42 - loss: 1.7272 - acc: 0.5422 - LRFinder: val_loss: 5.2304 - lr = 0.00413385 \n",
            "113/390 [=======>......................] - ETA: 1:41 - loss: 1.7270 - acc: 0.5421 - LRFinder: val_loss: 5.2748 - lr = 0.00415487 \n",
            "114/390 [=======>......................] - ETA: 1:41 - loss: 1.7280 - acc: 0.5419 - LRFinder: val_loss: 5.3325 - lr = 0.00417590 \n",
            "115/390 [=======>......................] - ETA: 1:40 - loss: 1.7289 - acc: 0.5415 - LRFinder: val_loss: 5.2507 - lr = 0.00419692 \n",
            "116/390 [=======>......................] - ETA: 1:40 - loss: 1.7288 - acc: 0.5410 - LRFinder: val_loss: 4.8590 - lr = 0.00421795 \n",
            "117/390 [========>.....................] - ETA: 1:39 - loss: 1.7281 - acc: 0.5413 - LRFinder: val_loss: 4.8077 - lr = 0.00423897 \n",
            "118/390 [========>.....................] - ETA: 1:39 - loss: 1.7293 - acc: 0.5409 - LRFinder: val_loss: 4.6966 - lr = 0.00426000 \n",
            "119/390 [========>.....................] - ETA: 1:39 - loss: 1.7307 - acc: 0.5407 - LRFinder: val_loss: 4.3873 - lr = 0.00428103 \n",
            "120/390 [========>.....................] - ETA: 1:38 - loss: 1.7298 - acc: 0.5410 - LRFinder: val_loss: 4.5655 - lr = 0.00430205 \n",
            "121/390 [========>.....................] - ETA: 1:38 - loss: 1.7296 - acc: 0.5411 - LRFinder: val_loss: 4.2811 - lr = 0.00432308 \n",
            "122/390 [========>.....................] - ETA: 1:37 - loss: 1.7288 - acc: 0.5412 - LRFinder: val_loss: 4.1831 - lr = 0.00434410 \n",
            "123/390 [========>.....................] - ETA: 1:37 - loss: 1.7305 - acc: 0.5409 - LRFinder: val_loss: 4.3766 - lr = 0.00436513 \n",
            "124/390 [========>.....................] - ETA: 1:37 - loss: 1.7319 - acc: 0.5404 - LRFinder: val_loss: 4.4773 - lr = 0.00438615 \n",
            "125/390 [========>.....................] - ETA: 1:36 - loss: 1.7339 - acc: 0.5399 - LRFinder: val_loss: 4.5155 - lr = 0.00440718 \n",
            "126/390 [========>.....................] - ETA: 1:36 - loss: 1.7338 - acc: 0.5399 - LRFinder: val_loss: 4.7838 - lr = 0.00442821 \n",
            "127/390 [========>.....................] - ETA: 1:35 - loss: 1.7329 - acc: 0.5403 - LRFinder: val_loss: 4.7334 - lr = 0.00444923 \n",
            "128/390 [========>.....................] - ETA: 1:35 - loss: 1.7318 - acc: 0.5410 - LRFinder: val_loss: 4.6817 - lr = 0.00447026 \n",
            "129/390 [========>.....................] - ETA: 1:34 - loss: 1.7310 - acc: 0.5410 - LRFinder: val_loss: 4.8013 - lr = 0.00449128 \n",
            "130/390 [=========>....................] - ETA: 1:34 - loss: 1.7302 - acc: 0.5413 - LRFinder: val_loss: 4.6174 - lr = 0.00451231 \n",
            "131/390 [=========>....................] - ETA: 1:34 - loss: 1.7301 - acc: 0.5414 - LRFinder: val_loss: 4.7373 - lr = 0.00453333 \n",
            "132/390 [=========>....................] - ETA: 1:33 - loss: 1.7315 - acc: 0.5409 - LRFinder: val_loss: 4.8029 - lr = 0.00455436 \n",
            "133/390 [=========>....................] - ETA: 1:33 - loss: 1.7305 - acc: 0.5412 - LRFinder: val_loss: 4.9432 - lr = 0.00457538 \n",
            "134/390 [=========>....................] - ETA: 1:32 - loss: 1.7309 - acc: 0.5410 - LRFinder: val_loss: 5.1503 - lr = 0.00459641 \n",
            "135/390 [=========>....................] - ETA: 1:32 - loss: 1.7315 - acc: 0.5409 - LRFinder: val_loss: 5.1123 - lr = 0.00461744 \n",
            "136/390 [=========>....................] - ETA: 1:32 - loss: 1.7328 - acc: 0.5406 - LRFinder: val_loss: 5.1568 - lr = 0.00463846 \n",
            "137/390 [=========>....................] - ETA: 1:31 - loss: 1.7330 - acc: 0.5403 - LRFinder: val_loss: 5.5713 - lr = 0.00465949 \n",
            "138/390 [=========>....................] - ETA: 1:31 - loss: 1.7323 - acc: 0.5408 - LRFinder: val_loss: 5.5373 - lr = 0.00468051 \n",
            "139/390 [=========>....................] - ETA: 1:30 - loss: 1.7313 - acc: 0.5410 - LRFinder: val_loss: 5.5514 - lr = 0.00470154 \n",
            "140/390 [=========>....................] - ETA: 1:30 - loss: 1.7311 - acc: 0.5411 - LRFinder: val_loss: 5.6101 - lr = 0.00472256 \n",
            "141/390 [=========>....................] - ETA: 1:30 - loss: 1.7309 - acc: 0.5410 - LRFinder: val_loss: 5.2337 - lr = 0.00474359 \n",
            "142/390 [=========>....................] - ETA: 1:29 - loss: 1.7309 - acc: 0.5410 - LRFinder: val_loss: 5.1597 - lr = 0.00476462 \n",
            "143/390 [==========>...................] - ETA: 1:29 - loss: 1.7310 - acc: 0.5410 - LRFinder: val_loss: 4.7266 - lr = 0.00478564 \n",
            "144/390 [==========>...................] - ETA: 1:28 - loss: 1.7304 - acc: 0.5414 - LRFinder: val_loss: 5.0566 - lr = 0.00480667 \n",
            "145/390 [==========>...................] - ETA: 1:28 - loss: 1.7313 - acc: 0.5411 - LRFinder: val_loss: 4.9556 - lr = 0.00482769 \n",
            "146/390 [==========>...................] - ETA: 1:28 - loss: 1.7317 - acc: 0.5411 - LRFinder: val_loss: 4.8380 - lr = 0.00484872 \n",
            "147/390 [==========>...................] - ETA: 1:27 - loss: 1.7327 - acc: 0.5401 - LRFinder: val_loss: 5.0739 - lr = 0.00486974 \n",
            "148/390 [==========>...................] - ETA: 1:27 - loss: 1.7333 - acc: 0.5399 - LRFinder: val_loss: 5.0941 - lr = 0.00489077 \n",
            "149/390 [==========>...................] - ETA: 1:26 - loss: 1.7330 - acc: 0.5400 - LRFinder: val_loss: 4.8150 - lr = 0.00491179 \n",
            "150/390 [==========>...................] - ETA: 1:26 - loss: 1.7338 - acc: 0.5396 - LRFinder: val_loss: 4.2166 - lr = 0.00493282 \n",
            "151/390 [==========>...................] - ETA: 1:26 - loss: 1.7341 - acc: 0.5395 - LRFinder: val_loss: 4.2425 - lr = 0.00495385 \n",
            "152/390 [==========>...................] - ETA: 1:25 - loss: 1.7345 - acc: 0.5391 - LRFinder: val_loss: 3.8543 - lr = 0.00497487 \n",
            "153/390 [==========>...................] - ETA: 1:25 - loss: 1.7355 - acc: 0.5390 - LRFinder: val_loss: 3.8821 - lr = 0.00499590 \n",
            "154/390 [==========>...................] - ETA: 1:25 - loss: 1.7358 - acc: 0.5388 - LRFinder: val_loss: 3.4569 - lr = 0.00501692 \n",
            "155/390 [==========>...................] - ETA: 1:24 - loss: 1.7361 - acc: 0.5391 - LRFinder: val_loss: 3.5204 - lr = 0.00503795 \n",
            "156/390 [===========>..................] - ETA: 1:24 - loss: 1.7366 - acc: 0.5390 - LRFinder: val_loss: 3.3107 - lr = 0.00505897 \n",
            "157/390 [===========>..................] - ETA: 1:23 - loss: 1.7370 - acc: 0.5387 - LRFinder: val_loss: 3.3427 - lr = 0.00508000 \n",
            "158/390 [===========>..................] - ETA: 1:23 - loss: 1.7368 - acc: 0.5387 - LRFinder: val_loss: 3.4119 - lr = 0.00510103 \n",
            "159/390 [===========>..................] - ETA: 1:23 - loss: 1.7359 - acc: 0.5390 - LRFinder: val_loss: 3.4325 - lr = 0.00512205 \n",
            "160/390 [===========>..................] - ETA: 1:22 - loss: 1.7368 - acc: 0.5384 - LRFinder: val_loss: 3.3126 - lr = 0.00514308 \n",
            "161/390 [===========>..................] - ETA: 1:22 - loss: 1.7376 - acc: 0.5382 - LRFinder: val_loss: 3.3999 - lr = 0.00516410 \n",
            "162/390 [===========>..................] - ETA: 1:21 - loss: 1.7370 - acc: 0.5383 - LRFinder: val_loss: 3.3217 - lr = 0.00518513 \n",
            "163/390 [===========>..................] - ETA: 1:21 - loss: 1.7369 - acc: 0.5384 - LRFinder: val_loss: 3.2465 - lr = 0.00520615 \n",
            "164/390 [===========>..................] - ETA: 1:21 - loss: 1.7363 - acc: 0.5388 - LRFinder: val_loss: 3.4548 - lr = 0.00522718 \n",
            "165/390 [===========>..................] - ETA: 1:20 - loss: 1.7369 - acc: 0.5385 - LRFinder: val_loss: 3.2666 - lr = 0.00524821 \n",
            "166/390 [===========>..................] - ETA: 1:20 - loss: 1.7376 - acc: 0.5384 - LRFinder: val_loss: 3.2181 - lr = 0.00526923 \n",
            "167/390 [===========>..................] - ETA: 1:20 - loss: 1.7367 - acc: 0.5390 - LRFinder: val_loss: 3.1786 - lr = 0.00529026 \n",
            "168/390 [===========>..................] - ETA: 1:19 - loss: 1.7361 - acc: 0.5394 - LRFinder: val_loss: 3.3111 - lr = 0.00531128 \n",
            "169/390 [============>.................] - ETA: 1:19 - loss: 1.7355 - acc: 0.5397 - LRFinder: val_loss: 3.2224 - lr = 0.00533231 \n",
            "170/390 [============>.................] - ETA: 1:18 - loss: 1.7347 - acc: 0.5395 - LRFinder: val_loss: 3.3026 - lr = 0.00535333 \n",
            "171/390 [============>.................] - ETA: 1:18 - loss: 1.7346 - acc: 0.5397 - LRFinder: val_loss: 3.2533 - lr = 0.00537436 \n",
            "172/390 [============>.................] - ETA: 1:18 - loss: 1.7354 - acc: 0.5392 - LRFinder: val_loss: 3.3602 - lr = 0.00539538 \n",
            "173/390 [============>.................] - ETA: 1:17 - loss: 1.7347 - acc: 0.5396 - LRFinder: val_loss: 3.2356 - lr = 0.00541641 \n",
            "174/390 [============>.................] - ETA: 1:17 - loss: 1.7346 - acc: 0.5396 - LRFinder: val_loss: 3.2864 - lr = 0.00543744 \n",
            "175/390 [============>.................] - ETA: 1:17 - loss: 1.7343 - acc: 0.5397 - LRFinder: val_loss: 3.2378 - lr = 0.00545846 \n",
            "176/390 [============>.................] - ETA: 1:16 - loss: 1.7340 - acc: 0.5396 - LRFinder: val_loss: 3.3002 - lr = 0.00547949 \n",
            "177/390 [============>.................] - ETA: 1:16 - loss: 1.7333 - acc: 0.5399 - LRFinder: val_loss: 3.3812 - lr = 0.00550051 \n",
            "178/390 [============>.................] - ETA: 1:15 - loss: 1.7344 - acc: 0.5394 - LRFinder: val_loss: 3.4773 - lr = 0.00552154 \n",
            "179/390 [============>.................] - ETA: 1:15 - loss: 1.7346 - acc: 0.5389 - LRFinder: val_loss: 3.4442 - lr = 0.00554256 \n",
            "180/390 [============>.................] - ETA: 1:15 - loss: 1.7347 - acc: 0.5390 - LRFinder: val_loss: 3.5322 - lr = 0.00556359 \n",
            "181/390 [============>.................] - ETA: 1:14 - loss: 1.7351 - acc: 0.5386 - LRFinder: val_loss: 3.6265 - lr = 0.00558462 \n",
            "182/390 [=============>................] - ETA: 1:14 - loss: 1.7353 - acc: 0.5384 - LRFinder: val_loss: 3.6645 - lr = 0.00560564 \n",
            "183/390 [=============>................] - ETA: 1:14 - loss: 1.7351 - acc: 0.5384 - LRFinder: val_loss: 3.8658 - lr = 0.00562667 \n",
            "184/390 [=============>................] - ETA: 1:13 - loss: 1.7347 - acc: 0.5386 - LRFinder: val_loss: 3.8583 - lr = 0.00564769 \n",
            "185/390 [=============>................] - ETA: 1:13 - loss: 1.7346 - acc: 0.5388 - LRFinder: val_loss: 3.8958 - lr = 0.00566872 \n",
            "186/390 [=============>................] - ETA: 1:12 - loss: 1.7350 - acc: 0.5390 - LRFinder: val_loss: 3.9153 - lr = 0.00568974 \n",
            "187/390 [=============>................] - ETA: 1:12 - loss: 1.7350 - acc: 0.5389 - LRFinder: val_loss: 4.0841 - lr = 0.00571077 \n",
            "188/390 [=============>................] - ETA: 1:12 - loss: 1.7354 - acc: 0.5389 - LRFinder: val_loss: 4.1114 - lr = 0.00573179 \n",
            "189/390 [=============>................] - ETA: 1:11 - loss: 1.7347 - acc: 0.5393 - LRFinder: val_loss: 4.1009 - lr = 0.00575282 \n",
            "190/390 [=============>................] - ETA: 1:11 - loss: 1.7350 - acc: 0.5396 - LRFinder: val_loss: 4.2220 - lr = 0.00577385 \n",
            "191/390 [=============>................] - ETA: 1:11 - loss: 1.7353 - acc: 0.5393 - LRFinder: val_loss: 4.0974 - lr = 0.00579487 \n",
            "192/390 [=============>................] - ETA: 1:10 - loss: 1.7345 - acc: 0.5395 - LRFinder: val_loss: 4.0826 - lr = 0.00581590 \n",
            "193/390 [=============>................] - ETA: 1:10 - loss: 1.7343 - acc: 0.5398 - LRFinder: val_loss: 4.2982 - lr = 0.00583692 \n",
            "194/390 [=============>................] - ETA: 1:09 - loss: 1.7342 - acc: 0.5397 - LRFinder: val_loss: 4.2305 - lr = 0.00585795 \n",
            "195/390 [==============>...............] - ETA: 1:09 - loss: 1.7342 - acc: 0.5397 - LRFinder: val_loss: 4.3184 - lr = 0.00587897 \n",
            "196/390 [==============>...............] - ETA: 1:09 - loss: 1.7338 - acc: 0.5399 - LRFinder: val_loss: 4.3193 - lr = 0.00590000 \n",
            "197/390 [==============>...............] - ETA: 1:08 - loss: 1.7331 - acc: 0.5403 - LRFinder: val_loss: 4.1687 - lr = 0.00592103 \n",
            "198/390 [==============>...............] - ETA: 1:08 - loss: 1.7332 - acc: 0.5404 - LRFinder: val_loss: 4.0114 - lr = 0.00594205 \n",
            "199/390 [==============>...............] - ETA: 1:08 - loss: 1.7337 - acc: 0.5402 - LRFinder: val_loss: 4.1731 - lr = 0.00596308 \n",
            "200/390 [==============>...............] - ETA: 1:07 - loss: 1.7343 - acc: 0.5400 - LRFinder: val_loss: 4.1935 - lr = 0.00598410 \n",
            "201/390 [==============>...............] - ETA: 1:07 - loss: 1.7336 - acc: 0.5400 - LRFinder: val_loss: 4.2408 - lr = 0.00600513 \n",
            "202/390 [==============>...............] - ETA: 1:06 - loss: 1.7339 - acc: 0.5399 - LRFinder: val_loss: 4.0387 - lr = 0.00602615 \n",
            "203/390 [==============>...............] - ETA: 1:06 - loss: 1.7333 - acc: 0.5399 - LRFinder: val_loss: 4.1460 - lr = 0.00604718 \n",
            "204/390 [==============>...............] - ETA: 1:06 - loss: 1.7333 - acc: 0.5399 - LRFinder: val_loss: 4.0556 - lr = 0.00606821 \n",
            "205/390 [==============>...............] - ETA: 1:05 - loss: 1.7330 - acc: 0.5401 - LRFinder: val_loss: 4.1294 - lr = 0.00608923 \n",
            "206/390 [==============>...............] - ETA: 1:05 - loss: 1.7329 - acc: 0.5400 - LRFinder: val_loss: 4.2350 - lr = 0.00611026 \n",
            "207/390 [==============>...............] - ETA: 1:05 - loss: 1.7324 - acc: 0.5402 - LRFinder: val_loss: 4.1926 - lr = 0.00613128 \n",
            "208/390 [===============>..............] - ETA: 1:04 - loss: 1.7323 - acc: 0.5401 - LRFinder: val_loss: 4.2062 - lr = 0.00615231 \n",
            "209/390 [===============>..............] - ETA: 1:04 - loss: 1.7317 - acc: 0.5405 - LRFinder: val_loss: 4.4359 - lr = 0.00617333 \n",
            "210/390 [===============>..............] - ETA: 1:04 - loss: 1.7325 - acc: 0.5403 - LRFinder: val_loss: 4.5003 - lr = 0.00619436 \n",
            "211/390 [===============>..............] - ETA: 1:03 - loss: 1.7324 - acc: 0.5404 - LRFinder: val_loss: 4.3171 - lr = 0.00621538 \n",
            "212/390 [===============>..............] - ETA: 1:03 - loss: 1.7321 - acc: 0.5403 - LRFinder: val_loss: 4.3060 - lr = 0.00623641 \n",
            "213/390 [===============>..............] - ETA: 1:02 - loss: 1.7319 - acc: 0.5402 - LRFinder: val_loss: 4.2648 - lr = 0.00625744 \n",
            "214/390 [===============>..............] - ETA: 1:02 - loss: 1.7315 - acc: 0.5404 - LRFinder: val_loss: 4.2780 - lr = 0.00627846 \n",
            "215/390 [===============>..............] - ETA: 1:02 - loss: 1.7314 - acc: 0.5404 - LRFinder: val_loss: 4.2318 - lr = 0.00629949 \n",
            "216/390 [===============>..............] - ETA: 1:01 - loss: 1.7310 - acc: 0.5406 - LRFinder: val_loss: 4.3360 - lr = 0.00632051 \n",
            "217/390 [===============>..............] - ETA: 1:01 - loss: 1.7305 - acc: 0.5408 - LRFinder: val_loss: 4.2625 - lr = 0.00634154 \n",
            "218/390 [===============>..............] - ETA: 1:01 - loss: 1.7302 - acc: 0.5409 - LRFinder: val_loss: 4.3129 - lr = 0.00636256 \n",
            "219/390 [===============>..............] - ETA: 1:00 - loss: 1.7295 - acc: 0.5411 - LRFinder: val_loss: 4.4263 - lr = 0.00638359 \n",
            "220/390 [===============>..............] - ETA: 1:00 - loss: 1.7298 - acc: 0.5411 - LRFinder: val_loss: 4.5830 - lr = 0.00640462 \n",
            "221/390 [================>.............] - ETA: 59s - loss: 1.7292 - acc: 0.5412  - LRFinder: val_loss: 4.6695 - lr = 0.00642564 \n",
            "222/390 [================>.............] - ETA: 59s - loss: 1.7288 - acc: 0.5412 - LRFinder: val_loss: 4.5579 - lr = 0.00644667 \n",
            "223/390 [================>.............] - ETA: 59s - loss: 1.7274 - acc: 0.5419 - LRFinder: val_loss: 4.4187 - lr = 0.00646769 \n",
            "224/390 [================>.............] - ETA: 58s - loss: 1.7268 - acc: 0.5421 - LRFinder: val_loss: 4.3074 - lr = 0.00648872 \n",
            "225/390 [================>.............] - ETA: 58s - loss: 1.7260 - acc: 0.5422 - LRFinder: val_loss: 4.1876 - lr = 0.00650974 \n",
            "226/390 [================>.............] - ETA: 58s - loss: 1.7254 - acc: 0.5424 - LRFinder: val_loss: 4.2471 - lr = 0.00653077 \n",
            "227/390 [================>.............] - ETA: 57s - loss: 1.7261 - acc: 0.5423 - LRFinder: val_loss: 4.2209 - lr = 0.00655179 \n",
            "228/390 [================>.............] - ETA: 57s - loss: 1.7259 - acc: 0.5425 - LRFinder: val_loss: 4.1256 - lr = 0.00657282 \n",
            "229/390 [================>.............] - ETA: 57s - loss: 1.7261 - acc: 0.5425 - LRFinder: val_loss: 3.9995 - lr = 0.00659385 \n",
            "230/390 [================>.............] - ETA: 56s - loss: 1.7263 - acc: 0.5426 - LRFinder: val_loss: 3.8096 - lr = 0.00661487 \n",
            "231/390 [================>.............] - ETA: 56s - loss: 1.7261 - acc: 0.5426 - LRFinder: val_loss: 3.8542 - lr = 0.00663590 \n",
            "232/390 [================>.............] - ETA: 55s - loss: 1.7257 - acc: 0.5429 - LRFinder: val_loss: 3.8991 - lr = 0.00665692 \n",
            "233/390 [================>.............] - ETA: 55s - loss: 1.7253 - acc: 0.5428 - LRFinder: val_loss: 3.9356 - lr = 0.00667795 \n",
            "234/390 [=================>............] - ETA: 55s - loss: 1.7251 - acc: 0.5428 - LRFinder: val_loss: 4.0969 - lr = 0.00669897 \n",
            "235/390 [=================>............] - ETA: 54s - loss: 1.7247 - acc: 0.5431 - LRFinder: val_loss: 4.1862 - lr = 0.00672000 \n",
            "236/390 [=================>............] - ETA: 54s - loss: 1.7249 - acc: 0.5430 - LRFinder: val_loss: 4.0003 - lr = 0.00674103 \n",
            "237/390 [=================>............] - ETA: 54s - loss: 1.7250 - acc: 0.5429 - LRFinder: val_loss: 4.0101 - lr = 0.00676205 \n",
            "238/390 [=================>............] - ETA: 53s - loss: 1.7254 - acc: 0.5428 - LRFinder: val_loss: 3.9553 - lr = 0.00678308 \n",
            "239/390 [=================>............] - ETA: 53s - loss: 1.7252 - acc: 0.5429 - LRFinder: val_loss: 3.8420 - lr = 0.00680410 \n",
            "240/390 [=================>............] - ETA: 53s - loss: 1.7252 - acc: 0.5429 - LRFinder: val_loss: 3.9226 - lr = 0.00682513 \n",
            "241/390 [=================>............] - ETA: 52s - loss: 1.7243 - acc: 0.5432 - LRFinder: val_loss: 3.7837 - lr = 0.00684615 \n",
            "242/390 [=================>............] - ETA: 52s - loss: 1.7249 - acc: 0.5429 - LRFinder: val_loss: 3.8265 - lr = 0.00686718 \n",
            "243/390 [=================>............] - ETA: 52s - loss: 1.7247 - acc: 0.5431 - LRFinder: val_loss: 3.8916 - lr = 0.00688821 \n",
            "244/390 [=================>............] - ETA: 51s - loss: 1.7243 - acc: 0.5434 - LRFinder: val_loss: 3.7812 - lr = 0.00690923 \n",
            "245/390 [=================>............] - ETA: 51s - loss: 1.7241 - acc: 0.5437 - LRFinder: val_loss: 3.8496 - lr = 0.00693026 \n",
            "246/390 [=================>............] - ETA: 50s - loss: 1.7234 - acc: 0.5440 - LRFinder: val_loss: 4.0606 - lr = 0.00695128 \n",
            "247/390 [==================>...........] - ETA: 50s - loss: 1.7237 - acc: 0.5440 - LRFinder: val_loss: 4.0291 - lr = 0.00697231 \n",
            "248/390 [==================>...........] - ETA: 50s - loss: 1.7233 - acc: 0.5441 - LRFinder: val_loss: 3.9453 - lr = 0.00699333 \n",
            "249/390 [==================>...........] - ETA: 49s - loss: 1.7231 - acc: 0.5441 - LRFinder: val_loss: 4.1451 - lr = 0.00701436 \n",
            "250/390 [==================>...........] - ETA: 49s - loss: 1.7226 - acc: 0.5443 - LRFinder: val_loss: 4.1587 - lr = 0.00703538 \n",
            "251/390 [==================>...........] - ETA: 49s - loss: 1.7224 - acc: 0.5445 - LRFinder: val_loss: 4.2410 - lr = 0.00705641 \n",
            "252/390 [==================>...........] - ETA: 48s - loss: 1.7220 - acc: 0.5445 - LRFinder: val_loss: 4.1239 - lr = 0.00707744 \n",
            "253/390 [==================>...........] - ETA: 48s - loss: 1.7220 - acc: 0.5444 - LRFinder: val_loss: 4.3977 - lr = 0.00709846 \n",
            "254/390 [==================>...........] - ETA: 48s - loss: 1.7217 - acc: 0.5446 - LRFinder: val_loss: 4.2771 - lr = 0.00711949 \n",
            "255/390 [==================>...........] - ETA: 47s - loss: 1.7217 - acc: 0.5446 - LRFinder: val_loss: 4.3304 - lr = 0.00714051 \n",
            "256/390 [==================>...........] - ETA: 47s - loss: 1.7220 - acc: 0.5445 - LRFinder: val_loss: 4.3580 - lr = 0.00716154 \n",
            "257/390 [==================>...........] - ETA: 46s - loss: 1.7217 - acc: 0.5445 - LRFinder: val_loss: 4.2843 - lr = 0.00718256 \n",
            "258/390 [==================>...........] - ETA: 46s - loss: 1.7215 - acc: 0.5446 - LRFinder: val_loss: 4.3771 - lr = 0.00720359 \n",
            "259/390 [==================>...........] - ETA: 46s - loss: 1.7219 - acc: 0.5446 - LRFinder: val_loss: 4.2904 - lr = 0.00722462 \n",
            "260/390 [===================>..........] - ETA: 45s - loss: 1.7223 - acc: 0.5446 - LRFinder: val_loss: 4.3637 - lr = 0.00724564 \n",
            "261/390 [===================>..........] - ETA: 45s - loss: 1.7227 - acc: 0.5443 - LRFinder: val_loss: 4.5710 - lr = 0.00726667 \n",
            "262/390 [===================>..........] - ETA: 45s - loss: 1.7220 - acc: 0.5447 - LRFinder: val_loss: 4.4522 - lr = 0.00728769 \n",
            "263/390 [===================>..........] - ETA: 44s - loss: 1.7218 - acc: 0.5446 - LRFinder: val_loss: 4.4944 - lr = 0.00730872 \n",
            "264/390 [===================>..........] - ETA: 44s - loss: 1.7220 - acc: 0.5445 - LRFinder: val_loss: 4.4971 - lr = 0.00732974 \n",
            "265/390 [===================>..........] - ETA: 44s - loss: 1.7220 - acc: 0.5446 - LRFinder: val_loss: 4.8735 - lr = 0.00735077 \n",
            "266/390 [===================>..........] - ETA: 43s - loss: 1.7225 - acc: 0.5444 - LRFinder: val_loss: 4.7852 - lr = 0.00737179 \n",
            "267/390 [===================>..........] - ETA: 43s - loss: 1.7223 - acc: 0.5445 - LRFinder: val_loss: 4.7238 - lr = 0.00739282 \n",
            "268/390 [===================>..........] - ETA: 43s - loss: 1.7221 - acc: 0.5447 - LRFinder: val_loss: 4.7890 - lr = 0.00741385 \n",
            "269/390 [===================>..........] - ETA: 42s - loss: 1.7223 - acc: 0.5448 - LRFinder: val_loss: 4.6157 - lr = 0.00743487 \n",
            "270/390 [===================>..........] - ETA: 42s - loss: 1.7231 - acc: 0.5447 - LRFinder: val_loss: 4.9477 - lr = 0.00745590 \n",
            "271/390 [===================>..........] - ETA: 41s - loss: 1.7232 - acc: 0.5445 - LRFinder: val_loss: 4.7634 - lr = 0.00747692 \n",
            "272/390 [===================>..........] - ETA: 41s - loss: 1.7233 - acc: 0.5445 - LRFinder: val_loss: 4.6820 - lr = 0.00749795 \n",
            "273/390 [====================>.........] - ETA: 41s - loss: 1.7229 - acc: 0.5446 - LRFinder: val_loss: 4.7142 - lr = 0.00751897 \n",
            "274/390 [====================>.........] - ETA: 40s - loss: 1.7224 - acc: 0.5447 - LRFinder: val_loss: 4.3730 - lr = 0.00754000 \n",
            "275/390 [====================>.........] - ETA: 40s - loss: 1.7224 - acc: 0.5449 - LRFinder: val_loss: 4.6492 - lr = 0.00756103 \n",
            "276/390 [====================>.........] - ETA: 40s - loss: 1.7222 - acc: 0.5448 - LRFinder: val_loss: 4.6195 - lr = 0.00758205 \n",
            "277/390 [====================>.........] - ETA: 39s - loss: 1.7218 - acc: 0.5451 - LRFinder: val_loss: 4.6016 - lr = 0.00760308 \n",
            "278/390 [====================>.........] - ETA: 39s - loss: 1.7218 - acc: 0.5452 - LRFinder: val_loss: 4.7084 - lr = 0.00762410 \n",
            "279/390 [====================>.........] - ETA: 39s - loss: 1.7218 - acc: 0.5453 - LRFinder: val_loss: 4.6572 - lr = 0.00764513 \n",
            "280/390 [====================>.........] - ETA: 38s - loss: 1.7214 - acc: 0.5454 - LRFinder: val_loss: 4.6837 - lr = 0.00766615 \n",
            "281/390 [====================>.........] - ETA: 38s - loss: 1.7218 - acc: 0.5453 - LRFinder: val_loss: 4.8049 - lr = 0.00768718 \n",
            "282/390 [====================>.........] - ETA: 38s - loss: 1.7215 - acc: 0.5455 - LRFinder: val_loss: 4.6042 - lr = 0.00770821 \n",
            "283/390 [====================>.........] - ETA: 37s - loss: 1.7208 - acc: 0.5460 - LRFinder: val_loss: 4.7437 - lr = 0.00772923 \n",
            "284/390 [====================>.........] - ETA: 37s - loss: 1.7208 - acc: 0.5460 - LRFinder: val_loss: 4.8027 - lr = 0.00775026 \n",
            "285/390 [====================>.........] - ETA: 36s - loss: 1.7201 - acc: 0.5461 - LRFinder: val_loss: 4.7973 - lr = 0.00777128 \n",
            "286/390 [=====================>........] - ETA: 36s - loss: 1.7199 - acc: 0.5462 - LRFinder: val_loss: 4.6954 - lr = 0.00779231 \n",
            "287/390 [=====================>........] - ETA: 36s - loss: 1.7198 - acc: 0.5463 - LRFinder: val_loss: 4.9005 - lr = 0.00781333 \n",
            "288/390 [=====================>........] - ETA: 35s - loss: 1.7201 - acc: 0.5461 - LRFinder: val_loss: 4.7473 - lr = 0.00783436 \n",
            "289/390 [=====================>........] - ETA: 35s - loss: 1.7201 - acc: 0.5462 - LRFinder: val_loss: 4.8563 - lr = 0.00785538 \n",
            "290/390 [=====================>........] - ETA: 35s - loss: 1.7200 - acc: 0.5465 - LRFinder: val_loss: 4.6806 - lr = 0.00787641 \n",
            "291/390 [=====================>........] - ETA: 34s - loss: 1.7199 - acc: 0.5465 - LRFinder: val_loss: 4.6450 - lr = 0.00789744 \n",
            "292/390 [=====================>........] - ETA: 34s - loss: 1.7202 - acc: 0.5462 - LRFinder: val_loss: 4.6582 - lr = 0.00791846 \n",
            "293/390 [=====================>........] - ETA: 34s - loss: 1.7207 - acc: 0.5459 - LRFinder: val_loss: 4.8169 - lr = 0.00793949 \n",
            "294/390 [=====================>........] - ETA: 33s - loss: 1.7208 - acc: 0.5459 - LRFinder: val_loss: 4.4194 - lr = 0.00796051 \n",
            "295/390 [=====================>........] - ETA: 33s - loss: 1.7207 - acc: 0.5459 - LRFinder: val_loss: 4.2598 - lr = 0.00798154 \n",
            "296/390 [=====================>........] - ETA: 33s - loss: 1.7211 - acc: 0.5457 - LRFinder: val_loss: 4.3942 - lr = 0.00800256 \n",
            "297/390 [=====================>........] - ETA: 32s - loss: 1.7210 - acc: 0.5457 - LRFinder: val_loss: 4.1424 - lr = 0.00802359 \n",
            "298/390 [=====================>........] - ETA: 32s - loss: 1.7206 - acc: 0.5457 - LRFinder: val_loss: 4.4391 - lr = 0.00804462 \n",
            "299/390 [======================>.......] - ETA: 32s - loss: 1.7200 - acc: 0.5459 - LRFinder: val_loss: 4.0728 - lr = 0.00806564 \n",
            "300/390 [======================>.......] - ETA: 31s - loss: 1.7197 - acc: 0.5459 - LRFinder: val_loss: 4.1520 - lr = 0.00808667 \n",
            "301/390 [======================>.......] - ETA: 31s - loss: 1.7196 - acc: 0.5458 - LRFinder: val_loss: 3.8281 - lr = 0.00810769 \n",
            "302/390 [======================>.......] - ETA: 30s - loss: 1.7194 - acc: 0.5458 - LRFinder: val_loss: 3.7867 - lr = 0.00812872 \n",
            "303/390 [======================>.......] - ETA: 30s - loss: 1.7192 - acc: 0.5460 - LRFinder: val_loss: 3.8469 - lr = 0.00814974 \n",
            "304/390 [======================>.......] - ETA: 30s - loss: 1.7185 - acc: 0.5463 - LRFinder: val_loss: 3.8648 - lr = 0.00817077 \n",
            "305/390 [======================>.......] - ETA: 29s - loss: 1.7182 - acc: 0.5463 - LRFinder: val_loss: 3.7261 - lr = 0.00819179 \n",
            "306/390 [======================>.......] - ETA: 29s - loss: 1.7179 - acc: 0.5463 - LRFinder: val_loss: 3.6846 - lr = 0.00821282 \n",
            "307/390 [======================>.......] - ETA: 29s - loss: 1.7174 - acc: 0.5465 - LRFinder: val_loss: 3.9074 - lr = 0.00823385 \n",
            "308/390 [======================>.......] - ETA: 28s - loss: 1.7165 - acc: 0.5468 - LRFinder: val_loss: 3.6824 - lr = 0.00825487 \n",
            "309/390 [======================>.......] - ETA: 28s - loss: 1.7166 - acc: 0.5466 - LRFinder: val_loss: 3.7323 - lr = 0.00827590 \n",
            "310/390 [======================>.......] - ETA: 28s - loss: 1.7167 - acc: 0.5467 - LRFinder: val_loss: 3.8267 - lr = 0.00829692 \n",
            "311/390 [======================>.......] - ETA: 27s - loss: 1.7168 - acc: 0.5468 - LRFinder: val_loss: 3.7118 - lr = 0.00831795 \n",
            "312/390 [=======================>......] - ETA: 27s - loss: 1.7166 - acc: 0.5471 - LRFinder: val_loss: 3.5650 - lr = 0.00833897 \n",
            "313/390 [=======================>......] - ETA: 27s - loss: 1.7171 - acc: 0.5468 - LRFinder: val_loss: 3.6716 - lr = 0.00836000 \n",
            "314/390 [=======================>......] - ETA: 26s - loss: 1.7169 - acc: 0.5468 - LRFinder: val_loss: 3.6035 - lr = 0.00838103 \n",
            "315/390 [=======================>......] - ETA: 26s - loss: 1.7172 - acc: 0.5467 - LRFinder: val_loss: 3.7316 - lr = 0.00840205 \n",
            "316/390 [=======================>......] - ETA: 25s - loss: 1.7164 - acc: 0.5471 - LRFinder: val_loss: 3.7572 - lr = 0.00842308 \n",
            "317/390 [=======================>......] - ETA: 25s - loss: 1.7162 - acc: 0.5473 - LRFinder: val_loss: 3.7233 - lr = 0.00844410 \n",
            "318/390 [=======================>......] - ETA: 25s - loss: 1.7162 - acc: 0.5472 - LRFinder: val_loss: 3.5894 - lr = 0.00846513 \n",
            "319/390 [=======================>......] - ETA: 24s - loss: 1.7156 - acc: 0.5475 - LRFinder: val_loss: 3.6756 - lr = 0.00848615 \n",
            "320/390 [=======================>......] - ETA: 24s - loss: 1.7151 - acc: 0.5478 - LRFinder: val_loss: 3.6441 - lr = 0.00850718 \n",
            "321/390 [=======================>......] - ETA: 24s - loss: 1.7148 - acc: 0.5479 - LRFinder: val_loss: 3.5563 - lr = 0.00852821 \n",
            "322/390 [=======================>......] - ETA: 23s - loss: 1.7145 - acc: 0.5480 - LRFinder: val_loss: 3.6996 - lr = 0.00854923 \n",
            "323/390 [=======================>......] - ETA: 23s - loss: 1.7144 - acc: 0.5481 - LRFinder: val_loss: 3.5871 - lr = 0.00857026 \n",
            "324/390 [=======================>......] - ETA: 23s - loss: 1.7139 - acc: 0.5483 - LRFinder: val_loss: 3.7278 - lr = 0.00859128 \n",
            "325/390 [========================>.....] - ETA: 22s - loss: 1.7137 - acc: 0.5483 - LRFinder: val_loss: 3.6640 - lr = 0.00861231 \n",
            "326/390 [========================>.....] - ETA: 22s - loss: 1.7134 - acc: 0.5485 - LRFinder: val_loss: 3.7368 - lr = 0.00863333 \n",
            "327/390 [========================>.....] - ETA: 22s - loss: 1.7136 - acc: 0.5484 - LRFinder: val_loss: 3.9565 - lr = 0.00865436 \n",
            "328/390 [========================>.....] - ETA: 21s - loss: 1.7138 - acc: 0.5481 - LRFinder: val_loss: 3.8201 - lr = 0.00867538 \n",
            "329/390 [========================>.....] - ETA: 21s - loss: 1.7138 - acc: 0.5481 - LRFinder: val_loss: 3.8498 - lr = 0.00869641 \n",
            "330/390 [========================>.....] - ETA: 21s - loss: 1.7135 - acc: 0.5482 - LRFinder: val_loss: 3.9054 - lr = 0.00871744 \n",
            "331/390 [========================>.....] - ETA: 20s - loss: 1.7135 - acc: 0.5481 - LRFinder: val_loss: 4.0331 - lr = 0.00873846 \n",
            "332/390 [========================>.....] - ETA: 20s - loss: 1.7136 - acc: 0.5480 - LRFinder: val_loss: 4.0531 - lr = 0.00875949 \n",
            "333/390 [========================>.....] - ETA: 19s - loss: 1.7128 - acc: 0.5483 - LRFinder: val_loss: 4.0617 - lr = 0.00878051 \n",
            "334/390 [========================>.....] - ETA: 19s - loss: 1.7133 - acc: 0.5482 - LRFinder: val_loss: 3.9347 - lr = 0.00880154 \n",
            "335/390 [========================>.....] - ETA: 19s - loss: 1.7128 - acc: 0.5483 - LRFinder: val_loss: 4.0040 - lr = 0.00882256 \n",
            "336/390 [========================>.....] - ETA: 18s - loss: 1.7129 - acc: 0.5483 - LRFinder: val_loss: 3.8465 - lr = 0.00884359 \n",
            "337/390 [========================>.....] - ETA: 18s - loss: 1.7124 - acc: 0.5486 - LRFinder: val_loss: 3.9779 - lr = 0.00886462 \n",
            "338/390 [=========================>....] - ETA: 18s - loss: 1.7122 - acc: 0.5486 - LRFinder: val_loss: 3.7474 - lr = 0.00888564 \n",
            "339/390 [=========================>....] - ETA: 17s - loss: 1.7118 - acc: 0.5489 - LRFinder: val_loss: 3.9321 - lr = 0.00890667 \n",
            "340/390 [=========================>....] - ETA: 17s - loss: 1.7116 - acc: 0.5491 - LRFinder: val_loss: 3.9879 - lr = 0.00892769 \n",
            "341/390 [=========================>....] - ETA: 17s - loss: 1.7113 - acc: 0.5491 - LRFinder: val_loss: 4.0958 - lr = 0.00894872 \n",
            "342/390 [=========================>....] - ETA: 16s - loss: 1.7111 - acc: 0.5493 - LRFinder: val_loss: 4.1730 - lr = 0.00896974 \n",
            "343/390 [=========================>....] - ETA: 16s - loss: 1.7108 - acc: 0.5494 - LRFinder: val_loss: 4.4227 - lr = 0.00899077 \n",
            "344/390 [=========================>....] - ETA: 16s - loss: 1.7105 - acc: 0.5496 - LRFinder: val_loss: 4.5274 - lr = 0.00901179 \n",
            "345/390 [=========================>....] - ETA: 15s - loss: 1.7105 - acc: 0.5493 - LRFinder: val_loss: 4.6800 - lr = 0.00903282 \n",
            "346/390 [=========================>....] - ETA: 15s - loss: 1.7105 - acc: 0.5493 - LRFinder: val_loss: 4.5423 - lr = 0.00905385 \n",
            "347/390 [=========================>....] - ETA: 15s - loss: 1.7103 - acc: 0.5494 - LRFinder: val_loss: 4.6783 - lr = 0.00907487 \n",
            "348/390 [=========================>....] - ETA: 14s - loss: 1.7101 - acc: 0.5493 - LRFinder: val_loss: 4.4656 - lr = 0.00909590 \n",
            "349/390 [=========================>....] - ETA: 14s - loss: 1.7101 - acc: 0.5493 - LRFinder: val_loss: 4.3221 - lr = 0.00911692 \n",
            "350/390 [=========================>....] - ETA: 14s - loss: 1.7097 - acc: 0.5495 - LRFinder: val_loss: 4.3518 - lr = 0.00913795 \n",
            "351/390 [==========================>...] - ETA: 13s - loss: 1.7097 - acc: 0.5494 - LRFinder: val_loss: 4.4592 - lr = 0.00915897 \n",
            "352/390 [==========================>...] - ETA: 13s - loss: 1.7095 - acc: 0.5495 - LRFinder: val_loss: 4.4221 - lr = 0.00918000 \n",
            "353/390 [==========================>...] - ETA: 12s - loss: 1.7094 - acc: 0.5497 - LRFinder: val_loss: 4.5559 - lr = 0.00920103 \n",
            "354/390 [==========================>...] - ETA: 12s - loss: 1.7088 - acc: 0.5499 - LRFinder: val_loss: 4.7217 - lr = 0.00922205 \n",
            "355/390 [==========================>...] - ETA: 12s - loss: 1.7087 - acc: 0.5500 - LRFinder: val_loss: 4.4605 - lr = 0.00924308 \n",
            "356/390 [==========================>...] - ETA: 11s - loss: 1.7086 - acc: 0.5500 - LRFinder: val_loss: 4.5526 - lr = 0.00926410 \n",
            "357/390 [==========================>...] - ETA: 11s - loss: 1.7091 - acc: 0.5500 - LRFinder: val_loss: 4.5827 - lr = 0.00928513 \n",
            "358/390 [==========================>...] - ETA: 11s - loss: 1.7091 - acc: 0.5501 - LRFinder: val_loss: 4.4587 - lr = 0.00930615 \n",
            "359/390 [==========================>...] - ETA: 10s - loss: 1.7089 - acc: 0.5502 - LRFinder: val_loss: 4.2399 - lr = 0.00932718 \n",
            "360/390 [==========================>...] - ETA: 10s - loss: 1.7088 - acc: 0.5502 - LRFinder: val_loss: 4.2150 - lr = 0.00934821 \n",
            "361/390 [==========================>...] - ETA: 10s - loss: 1.7093 - acc: 0.5500 - LRFinder: val_loss: 4.6373 - lr = 0.00936923 \n",
            "362/390 [==========================>...] - ETA: 9s - loss: 1.7096 - acc: 0.5499  - LRFinder: val_loss: 4.2898 - lr = 0.00939026 \n",
            "363/390 [==========================>...] - ETA: 9s - loss: 1.7093 - acc: 0.5501 - LRFinder: val_loss: 5.1244 - lr = 0.00941128 \n",
            "364/390 [===========================>..] - ETA: 9s - loss: 1.7088 - acc: 0.5503 - LRFinder: val_loss: 5.4963 - lr = 0.00943231 \n",
            "365/390 [===========================>..] - ETA: 8s - loss: 1.7082 - acc: 0.5505 - LRFinder: val_loss: 4.8035 - lr = 0.00945333 \n",
            "366/390 [===========================>..] - ETA: 8s - loss: 1.7084 - acc: 0.5504 - LRFinder: val_loss: 4.7111 - lr = 0.00947436 \n",
            "367/390 [===========================>..] - ETA: 8s - loss: 1.7081 - acc: 0.5504 - LRFinder: val_loss: 4.2908 - lr = 0.00949538 \n",
            "368/390 [===========================>..] - ETA: 7s - loss: 1.7074 - acc: 0.5507 - LRFinder: val_loss: 4.2476 - lr = 0.00951641 \n",
            "369/390 [===========================>..] - ETA: 7s - loss: 1.7068 - acc: 0.5508 - LRFinder: val_loss: 4.3185 - lr = 0.00953744 \n",
            "370/390 [===========================>..] - ETA: 6s - loss: 1.7064 - acc: 0.5508 - LRFinder: val_loss: 4.4281 - lr = 0.00955846 \n",
            "371/390 [===========================>..] - ETA: 6s - loss: 1.7058 - acc: 0.5511 - LRFinder: val_loss: 4.3383 - lr = 0.00957949 \n",
            "372/390 [===========================>..] - ETA: 6s - loss: 1.7056 - acc: 0.5511 - LRFinder: val_loss: 4.5934 - lr = 0.00960051 \n",
            "373/390 [===========================>..] - ETA: 5s - loss: 1.7061 - acc: 0.5510 - LRFinder: val_loss: 5.1565 - lr = 0.00962154 \n",
            "374/390 [===========================>..] - ETA: 5s - loss: 1.7060 - acc: 0.5509 - LRFinder: val_loss: 5.3267 - lr = 0.00964256 \n",
            "375/390 [===========================>..] - ETA: 5s - loss: 1.7060 - acc: 0.5509 - LRFinder: val_loss: 5.3668 - lr = 0.00966359 \n",
            "376/390 [===========================>..] - ETA: 4s - loss: 1.7060 - acc: 0.5509 - LRFinder: val_loss: 5.1676 - lr = 0.00968462 \n",
            "377/390 [============================>.] - ETA: 4s - loss: 1.7058 - acc: 0.5509 - LRFinder: val_loss: 4.8140 - lr = 0.00970564 \n",
            "378/390 [============================>.] - ETA: 4s - loss: 1.7057 - acc: 0.5509 - LRFinder: val_loss: 4.6519 - lr = 0.00972667 \n",
            "379/390 [============================>.] - ETA: 3s - loss: 1.7051 - acc: 0.5510 - LRFinder: val_loss: 4.8057 - lr = 0.00974769 \n",
            "380/390 [============================>.] - ETA: 3s - loss: 1.7056 - acc: 0.5508 - LRFinder: val_loss: 4.9761 - lr = 0.00976872 \n",
            "381/390 [============================>.] - ETA: 3s - loss: 1.7053 - acc: 0.5510 - LRFinder: val_loss: 5.0493 - lr = 0.00978974 \n",
            "382/390 [============================>.] - ETA: 2s - loss: 1.7053 - acc: 0.5509 - LRFinder: val_loss: 5.0385 - lr = 0.00981077 \n",
            "383/390 [============================>.] - ETA: 2s - loss: 1.7048 - acc: 0.5512 - LRFinder: val_loss: 4.8047 - lr = 0.00983179 \n",
            "384/390 [============================>.] - ETA: 2s - loss: 1.7050 - acc: 0.5510 - LRFinder: val_loss: 4.6849 - lr = 0.00985282 \n",
            "385/390 [============================>.] - ETA: 1s - loss: 1.7044 - acc: 0.5513 - LRFinder: val_loss: 4.5653 - lr = 0.00987385 \n",
            "386/390 [============================>.] - ETA: 1s - loss: 1.7043 - acc: 0.5512 - LRFinder: val_loss: 4.9021 - lr = 0.00989487 \n",
            "387/390 [============================>.] - ETA: 1s - loss: 1.7040 - acc: 0.5513 - LRFinder: val_loss: 4.6454 - lr = 0.00991590 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 1.7038 - acc: 0.5515 - LRFinder: val_loss: 4.9606 - lr = 0.00993692 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.7038 - acc: 0.5515 - LRFinder: val_loss: 4.9296 - lr = 0.00995795 \n",
            " - LRFinder: val_loss: 4.6108 - lr = 0.00997897 \n",
            "390/390 [==============================] - 140s 358ms/step - loss: 1.7039 - acc: 0.5515 - val_loss: 4.8151 - val_acc: 0.2043\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {weights/momentum/momentum-0.99/}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAESCAYAAAAR2wXeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIHiGEEZfIBAIniH+B75/jl67KmfWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2gUKJtFkDxv",
        "colab_type": "text"
      },
      "source": [
        "#BEST MOMENTUM IS 0.9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yv5SXdqMa5f",
        "colab_type": "code",
        "outputId": "432dbb1a-0c43-458f-eda2-71bda84108f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# INITIAL WEIGHT DECAY FACTORS\n",
        "# WEIGHT_DECAY_FACTORS = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
        "\n",
        "# FINEGRAINED WEIGHT DECAY FACTORS\n",
        "WEIGHT_DECAY_FACTORS = [1e-7, 3e-7, 3e-6]\n",
        "\n",
        "for weight_decay in WEIGHT_DECAY_FACTORS:\n",
        "     lr_finder = LRFinder(n_train, BATCH_SIZE, minimum_lr=0.0018, maximum_lr=1e-2,\n",
        "                          validation_data=(X_test, Y_test),\n",
        "                          validation_sample_rate=5,\n",
        "                          lr_scale='linear', save_dir='weights/weight_decay/weight_decay-%s/' % str(weight_decay),\n",
        "                          verbose=True)\n",
        "\n",
        "#     # set the weight_decay here !\n",
        "#     # lr doesnt matter as it will be over written by the callback\n",
        "     optimizer = SGD(lr=0.0038, momentum=0.9, nesterov=True)\n",
        "     model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "#\n",
        "#         # Fit the model on the batches generated by datagen.flow().\n",
        "     model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE, shuffle=True),\n",
        "                             steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
        "                             validation_data=(X_test, Y_test),\n",
        "                             epochs=nb_epoch, verbose=1,\n",
        "                             callbacks=[lr_finder])\n",
        "\n",
        "# from plot we see, the model isnt impacted by the weight_decay very much at all\n",
        "# so we can use any of them.\n",
        "\n",
        "for weight_decay in WEIGHT_DECAY_FACTORS:\n",
        "    directory = 'weights/weight_decay/weight_decay-%s/' % str(weight_decay)\n",
        "\n",
        "    losses, lrs = LRFinder.restore_schedule_from_dir(directory, 10, 5)\n",
        "    plt.plot(lrs, losses, label='weight_decay=%0.7f' % weight_decay)\n",
        "\n",
        "plt.title(\"Weight Decay\")\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 15:38 - loss: 1.6685 - acc: 0.5312 - LRFinder: val_loss: 4.7843 - lr = 0.00180000 \n",
            "  2/390 [..............................] - ETA: 11:15 - loss: 1.6558 - acc: 0.5742 - LRFinder: val_loss: 4.6739 - lr = 0.00182103 \n",
            "  3/390 [..............................] - ETA: 8:15 - loss: 1.6557 - acc: 0.5729  - LRFinder: val_loss: 4.6517 - lr = 0.00184205 \n",
            "  4/390 [..............................] - ETA: 6:44 - loss: 1.6351 - acc: 0.5820 - LRFinder: val_loss: 4.8209 - lr = 0.00186308 \n",
            "  5/390 [..............................] - ETA: 5:48 - loss: 1.6109 - acc: 0.5984 - LRFinder: val_loss: 4.8203 - lr = 0.00188410 \n",
            "  6/390 [..............................] - ETA: 5:11 - loss: 1.6049 - acc: 0.5951 - LRFinder: val_loss: 4.7804 - lr = 0.00190513 \n",
            "  7/390 [..............................] - ETA: 4:45 - loss: 1.6204 - acc: 0.5871 - LRFinder: val_loss: 4.5071 - lr = 0.00192615 \n",
            "  8/390 [..............................] - ETA: 4:25 - loss: 1.5915 - acc: 0.5996 - LRFinder: val_loss: 4.5381 - lr = 0.00194718 \n",
            "  9/390 [..............................] - ETA: 4:09 - loss: 1.5870 - acc: 0.6007 - LRFinder: val_loss: 4.4130 - lr = 0.00196821 \n",
            " 10/390 [..............................] - ETA: 3:56 - loss: 1.5884 - acc: 0.6016 - LRFinder: val_loss: 4.5273 - lr = 0.00198923 \n",
            " 11/390 [..............................] - ETA: 3:46 - loss: 1.5917 - acc: 0.5980 - LRFinder: val_loss: 4.4819 - lr = 0.00201026 \n",
            " 12/390 [..............................] - ETA: 3:38 - loss: 1.5835 - acc: 0.6022 - LRFinder: val_loss: 4.3819 - lr = 0.00203128 \n",
            " 13/390 [>.............................] - ETA: 3:30 - loss: 1.5728 - acc: 0.6064 - LRFinder: val_loss: 4.5187 - lr = 0.00205231 \n",
            " 14/390 [>.............................] - ETA: 3:24 - loss: 1.5767 - acc: 0.6038 - LRFinder: val_loss: 4.4849 - lr = 0.00207333 \n",
            " 15/390 [>.............................] - ETA: 3:18 - loss: 1.5838 - acc: 0.6042 - LRFinder: val_loss: 4.3027 - lr = 0.00209436 \n",
            " 16/390 [>.............................] - ETA: 3:13 - loss: 1.5762 - acc: 0.6069 - LRFinder: val_loss: 4.3301 - lr = 0.00211538 \n",
            " 17/390 [>.............................] - ETA: 3:09 - loss: 1.5711 - acc: 0.6094 - LRFinder: val_loss: 4.3900 - lr = 0.00213641 \n",
            " 18/390 [>.............................] - ETA: 3:05 - loss: 1.5673 - acc: 0.6137 - LRFinder: val_loss: 4.4165 - lr = 0.00215744 \n",
            " 19/390 [>.............................] - ETA: 3:02 - loss: 1.5607 - acc: 0.6123 - LRFinder: val_loss: 4.2785 - lr = 0.00217846 \n",
            " 20/390 [>.............................] - ETA: 2:58 - loss: 1.5641 - acc: 0.6133 - LRFinder: val_loss: 4.3423 - lr = 0.00219949 \n",
            " 21/390 [>.............................] - ETA: 2:56 - loss: 1.5622 - acc: 0.6131 - LRFinder: val_loss: 4.2524 - lr = 0.00222051 \n",
            " 22/390 [>.............................] - ETA: 2:53 - loss: 1.5619 - acc: 0.6133 - LRFinder: val_loss: 4.2856 - lr = 0.00224154 \n",
            " 23/390 [>.............................] - ETA: 2:50 - loss: 1.5573 - acc: 0.6145 - LRFinder: val_loss: 4.3755 - lr = 0.00226256 \n",
            " 24/390 [>.............................] - ETA: 2:48 - loss: 1.5618 - acc: 0.6126 - LRFinder: val_loss: 4.1806 - lr = 0.00228359 \n",
            " 25/390 [>.............................] - ETA: 2:46 - loss: 1.5666 - acc: 0.6125 - LRFinder: val_loss: 4.2352 - lr = 0.00230462 \n",
            " 26/390 [=>............................] - ETA: 2:44 - loss: 1.5639 - acc: 0.6127 - LRFinder: val_loss: 4.1150 - lr = 0.00232564 \n",
            " 27/390 [=>............................] - ETA: 2:42 - loss: 1.5626 - acc: 0.6126 - LRFinder: val_loss: 4.2416 - lr = 0.00234667 \n",
            " 28/390 [=>............................] - ETA: 2:40 - loss: 1.5601 - acc: 0.6119 - LRFinder: val_loss: 4.1229 - lr = 0.00236769 \n",
            " 29/390 [=>............................] - ETA: 2:38 - loss: 1.5645 - acc: 0.6107 - LRFinder: val_loss: 4.0687 - lr = 0.00238872 \n",
            " 30/390 [=>............................] - ETA: 2:37 - loss: 1.5669 - acc: 0.6089 - LRFinder: val_loss: 4.2708 - lr = 0.00240974 \n",
            " 31/390 [=>............................] - ETA: 2:35 - loss: 1.5691 - acc: 0.6069 - LRFinder: val_loss: 4.1321 - lr = 0.00243077 \n",
            " 32/390 [=>............................] - ETA: 2:34 - loss: 1.5704 - acc: 0.6050 - LRFinder: val_loss: 4.3457 - lr = 0.00245179 \n",
            " 33/390 [=>............................] - ETA: 2:32 - loss: 1.5680 - acc: 0.6063 - LRFinder: val_loss: 4.1226 - lr = 0.00247282 \n",
            " 34/390 [=>............................] - ETA: 2:31 - loss: 1.5671 - acc: 0.6068 - LRFinder: val_loss: 4.0712 - lr = 0.00249385 \n",
            " 35/390 [=>............................] - ETA: 2:30 - loss: 1.5664 - acc: 0.6071 - LRFinder: val_loss: 4.1479 - lr = 0.00251487 \n",
            " 36/390 [=>............................] - ETA: 2:28 - loss: 1.5673 - acc: 0.6063 - LRFinder: val_loss: 4.0765 - lr = 0.00253590 \n",
            " 37/390 [=>............................] - ETA: 2:27 - loss: 1.5644 - acc: 0.6075 - LRFinder: val_loss: 3.9682 - lr = 0.00255692 \n",
            " 38/390 [=>............................] - ETA: 2:26 - loss: 1.5632 - acc: 0.6065 - LRFinder: val_loss: 4.1496 - lr = 0.00257795 \n",
            " 39/390 [==>...........................] - ETA: 2:25 - loss: 1.5608 - acc: 0.6080 - LRFinder: val_loss: 3.9755 - lr = 0.00259897 \n",
            " 40/390 [==>...........................] - ETA: 2:24 - loss: 1.5623 - acc: 0.6062 - LRFinder: val_loss: 3.9438 - lr = 0.00262000 \n",
            " 41/390 [==>...........................] - ETA: 2:23 - loss: 1.5586 - acc: 0.6092 - LRFinder: val_loss: 4.0154 - lr = 0.00264103 \n",
            " 42/390 [==>...........................] - ETA: 2:22 - loss: 1.5585 - acc: 0.6090 - LRFinder: val_loss: 4.0207 - lr = 0.00266205 \n",
            " 43/390 [==>...........................] - ETA: 2:21 - loss: 1.5579 - acc: 0.6086 - LRFinder: val_loss: 4.0165 - lr = 0.00268308 \n",
            " 44/390 [==>...........................] - ETA: 2:20 - loss: 1.5606 - acc: 0.6074 - LRFinder: val_loss: 3.9338 - lr = 0.00270410 \n",
            " 45/390 [==>...........................] - ETA: 2:19 - loss: 1.5641 - acc: 0.6059 - LRFinder: val_loss: 3.8980 - lr = 0.00272513 \n",
            " 46/390 [==>...........................] - ETA: 2:18 - loss: 1.5622 - acc: 0.6055 - LRFinder: val_loss: 4.1225 - lr = 0.00274615 \n",
            " 47/390 [==>...........................] - ETA: 2:18 - loss: 1.5582 - acc: 0.6064 - LRFinder: val_loss: 4.1610 - lr = 0.00276718 \n",
            " 48/390 [==>...........................] - ETA: 2:17 - loss: 1.5590 - acc: 0.6063 - LRFinder: val_loss: 3.9494 - lr = 0.00278821 \n",
            " 49/390 [==>...........................] - ETA: 2:16 - loss: 1.5572 - acc: 0.6075 - LRFinder: val_loss: 3.9024 - lr = 0.00280923 \n",
            " 50/390 [==>...........................] - ETA: 2:15 - loss: 1.5580 - acc: 0.6080 - LRFinder: val_loss: 3.9015 - lr = 0.00283026 \n",
            " 51/390 [==>...........................] - ETA: 2:14 - loss: 1.5554 - acc: 0.6088 - LRFinder: val_loss: 3.8285 - lr = 0.00285128 \n",
            " 52/390 [===>..........................] - ETA: 2:14 - loss: 1.5530 - acc: 0.6098 - LRFinder: val_loss: 3.7862 - lr = 0.00287231 \n",
            " 53/390 [===>..........................] - ETA: 2:13 - loss: 1.5517 - acc: 0.6104 - LRFinder: val_loss: 3.8205 - lr = 0.00289333 \n",
            " 54/390 [===>..........................] - ETA: 2:12 - loss: 1.5507 - acc: 0.6105 - LRFinder: val_loss: 3.9911 - lr = 0.00291436 \n",
            " 55/390 [===>..........................] - ETA: 2:11 - loss: 1.5504 - acc: 0.6111 - LRFinder: val_loss: 3.7875 - lr = 0.00293538 \n",
            " 56/390 [===>..........................] - ETA: 2:11 - loss: 1.5508 - acc: 0.6105 - LRFinder: val_loss: 3.8053 - lr = 0.00295641 \n",
            " 57/390 [===>..........................] - ETA: 2:10 - loss: 1.5489 - acc: 0.6107 - LRFinder: val_loss: 3.8181 - lr = 0.00297744 \n",
            " 58/390 [===>..........................] - ETA: 2:09 - loss: 1.5498 - acc: 0.6109 - LRFinder: val_loss: 3.9095 - lr = 0.00299846 \n",
            " 59/390 [===>..........................] - ETA: 2:09 - loss: 1.5506 - acc: 0.6100 - LRFinder: val_loss: 3.8773 - lr = 0.00301949 \n",
            " 60/390 [===>..........................] - ETA: 2:08 - loss: 1.5496 - acc: 0.6103 - LRFinder: val_loss: 3.8700 - lr = 0.00304051 \n",
            " 61/390 [===>..........................] - ETA: 2:07 - loss: 1.5494 - acc: 0.6099 - LRFinder: val_loss: 3.8385 - lr = 0.00306154 \n",
            " 62/390 [===>..........................] - ETA: 2:07 - loss: 1.5487 - acc: 0.6099 - LRFinder: val_loss: 3.7606 - lr = 0.00308256 \n",
            " 63/390 [===>..........................] - ETA: 2:06 - loss: 1.5507 - acc: 0.6100 - LRFinder: val_loss: 3.7902 - lr = 0.00310359 \n",
            " 64/390 [===>..........................] - ETA: 2:05 - loss: 1.5523 - acc: 0.6089 - LRFinder: val_loss: 3.8773 - lr = 0.00312462 \n",
            " 65/390 [====>.........................] - ETA: 2:05 - loss: 1.5525 - acc: 0.6085 - LRFinder: val_loss: 3.8788 - lr = 0.00314564 \n",
            " 66/390 [====>.........................] - ETA: 2:04 - loss: 1.5514 - acc: 0.6101 - LRFinder: val_loss: 3.7856 - lr = 0.00316667 \n",
            " 67/390 [====>.........................] - ETA: 2:04 - loss: 1.5508 - acc: 0.6101 - LRFinder: val_loss: 3.8412 - lr = 0.00318769 \n",
            " 68/390 [====>.........................] - ETA: 2:03 - loss: 1.5525 - acc: 0.6096 - LRFinder: val_loss: 3.7851 - lr = 0.00320872 \n",
            " 69/390 [====>.........................] - ETA: 2:02 - loss: 1.5504 - acc: 0.6102 - LRFinder: val_loss: 3.8722 - lr = 0.00322974 \n",
            " 70/390 [====>.........................] - ETA: 2:02 - loss: 1.5509 - acc: 0.6097 - LRFinder: val_loss: 3.8581 - lr = 0.00325077 \n",
            " 71/390 [====>.........................] - ETA: 2:01 - loss: 1.5496 - acc: 0.6097 - LRFinder: val_loss: 3.7535 - lr = 0.00327179 \n",
            " 72/390 [====>.........................] - ETA: 2:01 - loss: 1.5516 - acc: 0.6089 - LRFinder: val_loss: 3.9024 - lr = 0.00329282 \n",
            " 73/390 [====>.........................] - ETA: 2:00 - loss: 1.5500 - acc: 0.6088 - LRFinder: val_loss: 3.7537 - lr = 0.00331385 \n",
            " 74/390 [====>.........................] - ETA: 2:00 - loss: 1.5483 - acc: 0.6095 - LRFinder: val_loss: 3.8110 - lr = 0.00333487 \n",
            " 75/390 [====>.........................] - ETA: 1:59 - loss: 1.5493 - acc: 0.6095 - LRFinder: val_loss: 3.7662 - lr = 0.00335590 \n",
            " 76/390 [====>.........................] - ETA: 1:59 - loss: 1.5488 - acc: 0.6100 - LRFinder: val_loss: 3.8173 - lr = 0.00337692 \n",
            " 77/390 [====>.........................] - ETA: 1:58 - loss: 1.5497 - acc: 0.6100 - LRFinder: val_loss: 3.9646 - lr = 0.00339795 \n",
            " 78/390 [=====>........................] - ETA: 1:58 - loss: 1.5497 - acc: 0.6101 - LRFinder: val_loss: 3.9864 - lr = 0.00341897 \n",
            " 79/390 [=====>........................] - ETA: 1:57 - loss: 1.5494 - acc: 0.6099 - LRFinder: val_loss: 3.8958 - lr = 0.00344000 \n",
            " 80/390 [=====>........................] - ETA: 1:57 - loss: 1.5501 - acc: 0.6096 - LRFinder: val_loss: 3.9583 - lr = 0.00346103 \n",
            " 81/390 [=====>........................] - ETA: 1:56 - loss: 1.5504 - acc: 0.6098 - LRFinder: val_loss: 4.0914 - lr = 0.00348205 \n",
            " 82/390 [=====>........................] - ETA: 1:55 - loss: 1.5488 - acc: 0.6106 - LRFinder: val_loss: 4.0149 - lr = 0.00350308 \n",
            " 83/390 [=====>........................] - ETA: 1:55 - loss: 1.5498 - acc: 0.6104 - LRFinder: val_loss: 4.0824 - lr = 0.00352410 \n",
            " 84/390 [=====>........................] - ETA: 1:54 - loss: 1.5492 - acc: 0.6107 - LRFinder: val_loss: 4.2442 - lr = 0.00354513 \n",
            " 85/390 [=====>........................] - ETA: 1:54 - loss: 1.5491 - acc: 0.6111 - LRFinder: val_loss: 3.8709 - lr = 0.00356615 \n",
            " 86/390 [=====>........................] - ETA: 1:53 - loss: 1.5501 - acc: 0.6107 - LRFinder: val_loss: 4.1345 - lr = 0.00358718 \n",
            " 87/390 [=====>........................] - ETA: 1:53 - loss: 1.5489 - acc: 0.6110 - LRFinder: val_loss: 4.0581 - lr = 0.00360821 \n",
            " 88/390 [=====>........................] - ETA: 1:52 - loss: 1.5472 - acc: 0.6116 - LRFinder: val_loss: 4.0350 - lr = 0.00362923 \n",
            " 89/390 [=====>........................] - ETA: 1:52 - loss: 1.5477 - acc: 0.6117 - LRFinder: val_loss: 3.9745 - lr = 0.00365026 \n",
            " 90/390 [=====>........................] - ETA: 1:51 - loss: 1.5481 - acc: 0.6112 - LRFinder: val_loss: 3.8915 - lr = 0.00367128 \n",
            " 91/390 [======>.......................] - ETA: 1:51 - loss: 1.5485 - acc: 0.6107 - LRFinder: val_loss: 4.1234 - lr = 0.00369231 \n",
            " 92/390 [======>.......................] - ETA: 1:51 - loss: 1.5465 - acc: 0.6115 - LRFinder: val_loss: 3.9917 - lr = 0.00371333 \n",
            " 93/390 [======>.......................] - ETA: 1:50 - loss: 1.5478 - acc: 0.6111 - LRFinder: val_loss: 3.8952 - lr = 0.00373436 \n",
            " 94/390 [======>.......................] - ETA: 1:50 - loss: 1.5473 - acc: 0.6112 - LRFinder: val_loss: 4.1006 - lr = 0.00375538 \n",
            " 95/390 [======>.......................] - ETA: 1:49 - loss: 1.5457 - acc: 0.6119 - LRFinder: val_loss: 4.0292 - lr = 0.00377641 \n",
            " 96/390 [======>.......................] - ETA: 1:49 - loss: 1.5469 - acc: 0.6116 - LRFinder: val_loss: 4.0270 - lr = 0.00379744 \n",
            " 97/390 [======>.......................] - ETA: 1:48 - loss: 1.5468 - acc: 0.6117 - LRFinder: val_loss: 3.8325 - lr = 0.00381846 \n",
            " 98/390 [======>.......................] - ETA: 1:48 - loss: 1.5460 - acc: 0.6119 - LRFinder: val_loss: 3.8997 - lr = 0.00383949 \n",
            " 99/390 [======>.......................] - ETA: 1:47 - loss: 1.5455 - acc: 0.6117 - LRFinder: val_loss: 4.0226 - lr = 0.00386051 \n",
            "100/390 [======>.......................] - ETA: 1:47 - loss: 1.5457 - acc: 0.6114 - LRFinder: val_loss: 3.9229 - lr = 0.00388154 \n",
            "101/390 [======>.......................] - ETA: 1:46 - loss: 1.5443 - acc: 0.6117 - LRFinder: val_loss: 3.9865 - lr = 0.00390256 \n",
            "102/390 [======>.......................] - ETA: 1:46 - loss: 1.5426 - acc: 0.6124 - LRFinder: val_loss: 4.1862 - lr = 0.00392359 \n",
            "103/390 [======>.......................] - ETA: 1:45 - loss: 1.5416 - acc: 0.6125 - LRFinder: val_loss: 4.0181 - lr = 0.00394462 \n",
            "104/390 [=======>......................] - ETA: 1:45 - loss: 1.5403 - acc: 0.6131 - LRFinder: val_loss: 3.9598 - lr = 0.00396564 \n",
            "105/390 [=======>......................] - ETA: 1:45 - loss: 1.5395 - acc: 0.6131 - LRFinder: val_loss: 4.0956 - lr = 0.00398667 \n",
            "106/390 [=======>......................] - ETA: 1:44 - loss: 1.5391 - acc: 0.6133 - LRFinder: val_loss: 3.8809 - lr = 0.00400769 \n",
            "107/390 [=======>......................] - ETA: 1:44 - loss: 1.5386 - acc: 0.6132 - LRFinder: val_loss: 4.1205 - lr = 0.00402872 \n",
            "108/390 [=======>......................] - ETA: 1:43 - loss: 1.5385 - acc: 0.6124 - LRFinder: val_loss: 3.9064 - lr = 0.00404974 \n",
            "109/390 [=======>......................] - ETA: 1:43 - loss: 1.5387 - acc: 0.6126 - LRFinder: val_loss: 3.9200 - lr = 0.00407077 \n",
            "110/390 [=======>......................] - ETA: 1:42 - loss: 1.5388 - acc: 0.6125 - LRFinder: val_loss: 3.9030 - lr = 0.00409179 \n",
            "111/390 [=======>......................] - ETA: 1:42 - loss: 1.5394 - acc: 0.6123 - LRFinder: val_loss: 3.9746 - lr = 0.00411282 \n",
            "112/390 [=======>......................] - ETA: 1:42 - loss: 1.5404 - acc: 0.6124 - LRFinder: val_loss: 3.9444 - lr = 0.00413385 \n",
            "113/390 [=======>......................] - ETA: 1:41 - loss: 1.5400 - acc: 0.6131 - LRFinder: val_loss: 3.9633 - lr = 0.00415487 \n",
            "114/390 [=======>......................] - ETA: 1:41 - loss: 1.5384 - acc: 0.6136 - LRFinder: val_loss: 4.1384 - lr = 0.00417590 \n",
            "115/390 [=======>......................] - ETA: 1:40 - loss: 1.5388 - acc: 0.6135 - LRFinder: val_loss: 3.9487 - lr = 0.00419692 \n",
            "116/390 [=======>......................] - ETA: 1:40 - loss: 1.5390 - acc: 0.6129 - LRFinder: val_loss: 4.0634 - lr = 0.00421795 \n",
            "117/390 [========>.....................] - ETA: 1:39 - loss: 1.5393 - acc: 0.6127 - LRFinder: val_loss: 4.0566 - lr = 0.00423897 \n",
            "118/390 [========>.....................] - ETA: 1:39 - loss: 1.5390 - acc: 0.6130 - LRFinder: val_loss: 3.8566 - lr = 0.00426000 \n",
            "119/390 [========>.....................] - ETA: 1:39 - loss: 1.5387 - acc: 0.6130 - LRFinder: val_loss: 3.8537 - lr = 0.00428103 \n",
            "120/390 [========>.....................] - ETA: 1:38 - loss: 1.5395 - acc: 0.6125 - LRFinder: val_loss: 3.9188 - lr = 0.00430205 \n",
            "121/390 [========>.....................] - ETA: 1:38 - loss: 1.5390 - acc: 0.6129 - LRFinder: val_loss: 4.0173 - lr = 0.00432308 \n",
            "122/390 [========>.....................] - ETA: 1:37 - loss: 1.5386 - acc: 0.6133 - LRFinder: val_loss: 3.8879 - lr = 0.00434410 \n",
            "123/390 [========>.....................] - ETA: 1:37 - loss: 1.5381 - acc: 0.6138 - LRFinder: val_loss: 3.9463 - lr = 0.00436513 \n",
            "124/390 [========>.....................] - ETA: 1:37 - loss: 1.5398 - acc: 0.6130 - LRFinder: val_loss: 4.0072 - lr = 0.00438615 \n",
            "125/390 [========>.....................] - ETA: 1:36 - loss: 1.5398 - acc: 0.6128 - LRFinder: val_loss: 3.8636 - lr = 0.00440718 \n",
            "126/390 [========>.....................] - ETA: 1:36 - loss: 1.5404 - acc: 0.6122 - LRFinder: val_loss: 3.9869 - lr = 0.00442821 \n",
            "127/390 [========>.....................] - ETA: 1:35 - loss: 1.5405 - acc: 0.6124 - LRFinder: val_loss: 3.9947 - lr = 0.00444923 \n",
            "128/390 [========>.....................] - ETA: 1:35 - loss: 1.5406 - acc: 0.6121 - LRFinder: val_loss: 3.8764 - lr = 0.00447026 \n",
            "129/390 [========>.....................] - ETA: 1:34 - loss: 1.5400 - acc: 0.6123 - LRFinder: val_loss: 3.9183 - lr = 0.00449128 \n",
            "130/390 [=========>....................] - ETA: 1:34 - loss: 1.5383 - acc: 0.6129 - LRFinder: val_loss: 3.9873 - lr = 0.00451231 \n",
            "131/390 [=========>....................] - ETA: 1:34 - loss: 1.5377 - acc: 0.6128 - LRFinder: val_loss: 3.9540 - lr = 0.00453333 \n",
            "132/390 [=========>....................] - ETA: 1:33 - loss: 1.5363 - acc: 0.6134 - LRFinder: val_loss: 3.9870 - lr = 0.00455436 \n",
            "133/390 [=========>....................] - ETA: 1:33 - loss: 1.5355 - acc: 0.6135 - LRFinder: val_loss: 3.8807 - lr = 0.00457538 \n",
            "134/390 [=========>....................] - ETA: 1:32 - loss: 1.5343 - acc: 0.6137 - LRFinder: val_loss: 3.9382 - lr = 0.00459641 \n",
            "135/390 [=========>....................] - ETA: 1:32 - loss: 1.5334 - acc: 0.6142 - LRFinder: val_loss: 4.0215 - lr = 0.00461744 \n",
            "136/390 [=========>....................] - ETA: 1:32 - loss: 1.5325 - acc: 0.6144 - LRFinder: val_loss: 3.9455 - lr = 0.00463846 \n",
            "137/390 [=========>....................] - ETA: 1:31 - loss: 1.5324 - acc: 0.6143 - LRFinder: val_loss: 4.0308 - lr = 0.00465949 \n",
            "138/390 [=========>....................] - ETA: 1:31 - loss: 1.5326 - acc: 0.6141 - LRFinder: val_loss: 4.0121 - lr = 0.00468051 \n",
            "139/390 [=========>....................] - ETA: 1:30 - loss: 1.5323 - acc: 0.6144 - LRFinder: val_loss: 3.9971 - lr = 0.00470154 \n",
            "140/390 [=========>....................] - ETA: 1:30 - loss: 1.5320 - acc: 0.6147 - LRFinder: val_loss: 3.8659 - lr = 0.00472256 \n",
            "141/390 [=========>....................] - ETA: 1:30 - loss: 1.5327 - acc: 0.6141 - LRFinder: val_loss: 4.0050 - lr = 0.00474359 \n",
            "142/390 [=========>....................] - ETA: 1:29 - loss: 1.5318 - acc: 0.6147 - LRFinder: val_loss: 4.0685 - lr = 0.00476462 \n",
            "143/390 [==========>...................] - ETA: 1:29 - loss: 1.5313 - acc: 0.6148 - LRFinder: val_loss: 4.0930 - lr = 0.00478564 \n",
            "144/390 [==========>...................] - ETA: 1:28 - loss: 1.5309 - acc: 0.6147 - LRFinder: val_loss: 3.9324 - lr = 0.00480667 \n",
            "145/390 [==========>...................] - ETA: 1:28 - loss: 1.5296 - acc: 0.6152 - LRFinder: val_loss: 4.3076 - lr = 0.00482769 \n",
            "146/390 [==========>...................] - ETA: 1:28 - loss: 1.5283 - acc: 0.6154 - LRFinder: val_loss: 4.0473 - lr = 0.00484872 \n",
            "147/390 [==========>...................] - ETA: 1:27 - loss: 1.5292 - acc: 0.6153 - LRFinder: val_loss: 3.9428 - lr = 0.00486974 \n",
            "148/390 [==========>...................] - ETA: 1:27 - loss: 1.5291 - acc: 0.6153 - LRFinder: val_loss: 4.1260 - lr = 0.00489077 \n",
            "149/390 [==========>...................] - ETA: 1:27 - loss: 1.5294 - acc: 0.6156 - LRFinder: val_loss: 4.0252 - lr = 0.00491179 \n",
            "150/390 [==========>...................] - ETA: 1:26 - loss: 1.5298 - acc: 0.6153 - LRFinder: val_loss: 3.9934 - lr = 0.00493282 \n",
            "151/390 [==========>...................] - ETA: 1:26 - loss: 1.5293 - acc: 0.6154 - LRFinder: val_loss: 4.0626 - lr = 0.00495385 \n",
            "152/390 [==========>...................] - ETA: 1:25 - loss: 1.5288 - acc: 0.6156 - LRFinder: val_loss: 4.1150 - lr = 0.00497487 \n",
            "153/390 [==========>...................] - ETA: 1:25 - loss: 1.5276 - acc: 0.6161 - LRFinder: val_loss: 4.2760 - lr = 0.00499590 \n",
            "154/390 [==========>...................] - ETA: 1:25 - loss: 1.5273 - acc: 0.6161 - LRFinder: val_loss: 3.9978 - lr = 0.00501692 \n",
            "155/390 [==========>...................] - ETA: 1:24 - loss: 1.5282 - acc: 0.6155 - LRFinder: val_loss: 4.1901 - lr = 0.00503795 \n",
            "156/390 [===========>..................] - ETA: 1:24 - loss: 1.5274 - acc: 0.6156 - LRFinder: val_loss: 3.8270 - lr = 0.00505897 \n",
            "157/390 [===========>..................] - ETA: 1:23 - loss: 1.5274 - acc: 0.6156 - LRFinder: val_loss: 4.0354 - lr = 0.00508000 \n",
            "158/390 [===========>..................] - ETA: 1:23 - loss: 1.5268 - acc: 0.6159 - LRFinder: val_loss: 4.1450 - lr = 0.00510103 \n",
            "159/390 [===========>..................] - ETA: 1:23 - loss: 1.5266 - acc: 0.6160 - LRFinder: val_loss: 4.0995 - lr = 0.00512205 \n",
            "160/390 [===========>..................] - ETA: 1:22 - loss: 1.5259 - acc: 0.6162 - LRFinder: val_loss: 4.0447 - lr = 0.00514308 \n",
            "161/390 [===========>..................] - ETA: 1:22 - loss: 1.5253 - acc: 0.6164 - LRFinder: val_loss: 4.1282 - lr = 0.00516410 \n",
            "162/390 [===========>..................] - ETA: 1:21 - loss: 1.5252 - acc: 0.6164 - LRFinder: val_loss: 4.3102 - lr = 0.00518513 \n",
            "163/390 [===========>..................] - ETA: 1:21 - loss: 1.5252 - acc: 0.6163 - LRFinder: val_loss: 4.0481 - lr = 0.00520615 \n",
            "164/390 [===========>..................] - ETA: 1:21 - loss: 1.5236 - acc: 0.6169 - LRFinder: val_loss: 4.0834 - lr = 0.00522718 \n",
            "165/390 [===========>..................] - ETA: 1:20 - loss: 1.5234 - acc: 0.6168 - LRFinder: val_loss: 4.1325 - lr = 0.00524821 \n",
            "166/390 [===========>..................] - ETA: 1:20 - loss: 1.5226 - acc: 0.6170 - LRFinder: val_loss: 4.0361 - lr = 0.00526923 \n",
            "167/390 [===========>..................] - ETA: 1:20 - loss: 1.5214 - acc: 0.6175 - LRFinder: val_loss: 4.1401 - lr = 0.00529026 \n",
            "168/390 [===========>..................] - ETA: 1:19 - loss: 1.5209 - acc: 0.6174 - LRFinder: val_loss: 4.0114 - lr = 0.00531128 \n",
            "169/390 [============>.................] - ETA: 1:19 - loss: 1.5202 - acc: 0.6176 - LRFinder: val_loss: 4.0189 - lr = 0.00533231 \n",
            "170/390 [============>.................] - ETA: 1:18 - loss: 1.5202 - acc: 0.6176 - LRFinder: val_loss: 4.0740 - lr = 0.00535333 \n",
            "171/390 [============>.................] - ETA: 1:18 - loss: 1.5197 - acc: 0.6182 - LRFinder: val_loss: 3.9164 - lr = 0.00537436 \n",
            "172/390 [============>.................] - ETA: 1:18 - loss: 1.5196 - acc: 0.6185 - LRFinder: val_loss: 4.0827 - lr = 0.00539538 \n",
            "173/390 [============>.................] - ETA: 1:17 - loss: 1.5190 - acc: 0.6187 - LRFinder: val_loss: 4.0545 - lr = 0.00541641 \n",
            "174/390 [============>.................] - ETA: 1:17 - loss: 1.5186 - acc: 0.6189 - LRFinder: val_loss: 4.0970 - lr = 0.00543744 \n",
            "175/390 [============>.................] - ETA: 1:17 - loss: 1.5182 - acc: 0.6192 - LRFinder: val_loss: 3.9927 - lr = 0.00545846 \n",
            "176/390 [============>.................] - ETA: 1:16 - loss: 1.5184 - acc: 0.6191 - LRFinder: val_loss: 3.9834 - lr = 0.00547949 \n",
            "177/390 [============>.................] - ETA: 1:16 - loss: 1.5182 - acc: 0.6191 - LRFinder: val_loss: 4.1916 - lr = 0.00550051 \n",
            "178/390 [============>.................] - ETA: 1:15 - loss: 1.5177 - acc: 0.6193 - LRFinder: val_loss: 3.9892 - lr = 0.00552154 \n",
            "179/390 [============>.................] - ETA: 1:15 - loss: 1.5182 - acc: 0.6192 - LRFinder: val_loss: 4.1282 - lr = 0.00554256 \n",
            "180/390 [============>.................] - ETA: 1:15 - loss: 1.5179 - acc: 0.6193 - LRFinder: val_loss: 4.2468 - lr = 0.00556359 \n",
            "181/390 [============>.................] - ETA: 1:14 - loss: 1.5173 - acc: 0.6196 - LRFinder: val_loss: 3.9977 - lr = 0.00558462 \n",
            "182/390 [=============>................] - ETA: 1:14 - loss: 1.5172 - acc: 0.6195 - LRFinder: val_loss: 4.0670 - lr = 0.00560564 \n",
            "183/390 [=============>................] - ETA: 1:14 - loss: 1.5170 - acc: 0.6194 - LRFinder: val_loss: 3.9547 - lr = 0.00562667 \n",
            "184/390 [=============>................] - ETA: 1:13 - loss: 1.5159 - acc: 0.6196 - LRFinder: val_loss: 3.9393 - lr = 0.00564769 \n",
            "185/390 [=============>................] - ETA: 1:13 - loss: 1.5157 - acc: 0.6197 - LRFinder: val_loss: 4.0037 - lr = 0.00566872 \n",
            "186/390 [=============>................] - ETA: 1:12 - loss: 1.5154 - acc: 0.6200 - LRFinder: val_loss: 4.1596 - lr = 0.00568974 \n",
            "187/390 [=============>................] - ETA: 1:12 - loss: 1.5150 - acc: 0.6202 - LRFinder: val_loss: 3.9716 - lr = 0.00571077 \n",
            "188/390 [=============>................] - ETA: 1:12 - loss: 1.5154 - acc: 0.6202 - LRFinder: val_loss: 3.8575 - lr = 0.00573179 \n",
            "189/390 [=============>................] - ETA: 1:11 - loss: 1.5152 - acc: 0.6200 - LRFinder: val_loss: 3.9928 - lr = 0.00575282 \n",
            "190/390 [=============>................] - ETA: 1:11 - loss: 1.5149 - acc: 0.6201 - LRFinder: val_loss: 3.8968 - lr = 0.00577385 \n",
            "191/390 [=============>................] - ETA: 1:11 - loss: 1.5148 - acc: 0.6198 - LRFinder: val_loss: 4.0571 - lr = 0.00579487 \n",
            "192/390 [=============>................] - ETA: 1:10 - loss: 1.5148 - acc: 0.6198 - LRFinder: val_loss: 3.8936 - lr = 0.00581590 \n",
            "193/390 [=============>................] - ETA: 1:10 - loss: 1.5146 - acc: 0.6200 - LRFinder: val_loss: 3.9948 - lr = 0.00583692 \n",
            "194/390 [=============>................] - ETA: 1:09 - loss: 1.5143 - acc: 0.6200 - LRFinder: val_loss: 4.0412 - lr = 0.00585795 \n",
            "195/390 [==============>...............] - ETA: 1:09 - loss: 1.5138 - acc: 0.6203 - LRFinder: val_loss: 4.0312 - lr = 0.00587897 \n",
            "196/390 [==============>...............] - ETA: 1:09 - loss: 1.5131 - acc: 0.6205 - LRFinder: val_loss: 3.9180 - lr = 0.00590000 \n",
            "197/390 [==============>...............] - ETA: 1:08 - loss: 1.5124 - acc: 0.6208 - LRFinder: val_loss: 4.0018 - lr = 0.00592103 \n",
            "198/390 [==============>...............] - ETA: 1:08 - loss: 1.5126 - acc: 0.6205 - LRFinder: val_loss: 4.1938 - lr = 0.00594205 \n",
            "199/390 [==============>...............] - ETA: 1:08 - loss: 1.5117 - acc: 0.6208 - LRFinder: val_loss: 4.1373 - lr = 0.00596308 \n",
            "200/390 [==============>...............] - ETA: 1:07 - loss: 1.5122 - acc: 0.6208 - LRFinder: val_loss: 3.8746 - lr = 0.00598410 \n",
            "201/390 [==============>...............] - ETA: 1:07 - loss: 1.5118 - acc: 0.6212 - LRFinder: val_loss: 4.1167 - lr = 0.00600513 \n",
            "202/390 [==============>...............] - ETA: 1:06 - loss: 1.5118 - acc: 0.6212 - LRFinder: val_loss: 4.0578 - lr = 0.00602615 \n",
            "203/390 [==============>...............] - ETA: 1:06 - loss: 1.5113 - acc: 0.6213 - LRFinder: val_loss: 4.0413 - lr = 0.00604718 \n",
            "204/390 [==============>...............] - ETA: 1:06 - loss: 1.5109 - acc: 0.6214 - LRFinder: val_loss: 3.9502 - lr = 0.00606821 \n",
            "205/390 [==============>...............] - ETA: 1:05 - loss: 1.5105 - acc: 0.6214 - LRFinder: val_loss: 4.1008 - lr = 0.00608923 \n",
            "206/390 [==============>...............] - ETA: 1:05 - loss: 1.5105 - acc: 0.6212 - LRFinder: val_loss: 3.9131 - lr = 0.00611026 \n",
            "207/390 [==============>...............] - ETA: 1:05 - loss: 1.5107 - acc: 0.6212 - LRFinder: val_loss: 3.8746 - lr = 0.00613128 \n",
            "208/390 [===============>..............] - ETA: 1:04 - loss: 1.5104 - acc: 0.6210 - LRFinder: val_loss: 3.7908 - lr = 0.00615231 \n",
            "209/390 [===============>..............] - ETA: 1:04 - loss: 1.5099 - acc: 0.6211 - LRFinder: val_loss: 3.8911 - lr = 0.00617333 \n",
            "210/390 [===============>..............] - ETA: 1:04 - loss: 1.5099 - acc: 0.6211 - LRFinder: val_loss: 3.8209 - lr = 0.00619436 \n",
            "211/390 [===============>..............] - ETA: 1:03 - loss: 1.5093 - acc: 0.6214 - LRFinder: val_loss: 3.9875 - lr = 0.00621538 \n",
            "212/390 [===============>..............] - ETA: 1:03 - loss: 1.5100 - acc: 0.6210 - LRFinder: val_loss: 3.9802 - lr = 0.00623641 \n",
            "213/390 [===============>..............] - ETA: 1:02 - loss: 1.5097 - acc: 0.6213 - LRFinder: val_loss: 3.9717 - lr = 0.00625744 \n",
            "214/390 [===============>..............] - ETA: 1:02 - loss: 1.5092 - acc: 0.6215 - LRFinder: val_loss: 3.8705 - lr = 0.00627846 \n",
            "215/390 [===============>..............] - ETA: 1:02 - loss: 1.5092 - acc: 0.6216 - LRFinder: val_loss: 3.9651 - lr = 0.00629949 \n",
            "216/390 [===============>..............] - ETA: 1:01 - loss: 1.5090 - acc: 0.6217 - LRFinder: val_loss: 3.8800 - lr = 0.00632051 \n",
            "217/390 [===============>..............] - ETA: 1:01 - loss: 1.5095 - acc: 0.6216 - LRFinder: val_loss: 3.8986 - lr = 0.00634154 \n",
            "218/390 [===============>..............] - ETA: 1:01 - loss: 1.5095 - acc: 0.6214 - LRFinder: val_loss: 3.9959 - lr = 0.00636256 \n",
            "219/390 [===============>..............] - ETA: 1:00 - loss: 1.5095 - acc: 0.6212 - LRFinder: val_loss: 3.9350 - lr = 0.00638359 \n",
            "220/390 [===============>..............] - ETA: 1:00 - loss: 1.5087 - acc: 0.6217 - LRFinder: val_loss: 4.0611 - lr = 0.00640462 \n",
            "221/390 [================>.............] - ETA: 1:00 - loss: 1.5085 - acc: 0.6219 - LRFinder: val_loss: 3.9685 - lr = 0.00642564 \n",
            "222/390 [================>.............] - ETA: 59s - loss: 1.5082 - acc: 0.6220  - LRFinder: val_loss: 3.7898 - lr = 0.00644667 \n",
            "223/390 [================>.............] - ETA: 59s - loss: 1.5082 - acc: 0.6222 - LRFinder: val_loss: 3.8601 - lr = 0.00646769 \n",
            "224/390 [================>.............] - ETA: 58s - loss: 1.5084 - acc: 0.6220 - LRFinder: val_loss: 3.8618 - lr = 0.00648872 \n",
            "225/390 [================>.............] - ETA: 58s - loss: 1.5083 - acc: 0.6220 - LRFinder: val_loss: 3.9287 - lr = 0.00650974 \n",
            "226/390 [================>.............] - ETA: 58s - loss: 1.5083 - acc: 0.6219 - LRFinder: val_loss: 3.9807 - lr = 0.00653077 \n",
            "227/390 [================>.............] - ETA: 57s - loss: 1.5085 - acc: 0.6219 - LRFinder: val_loss: 3.9155 - lr = 0.00655179 \n",
            "228/390 [================>.............] - ETA: 57s - loss: 1.5078 - acc: 0.6222 - LRFinder: val_loss: 4.0277 - lr = 0.00657282 \n",
            "229/390 [================>.............] - ETA: 57s - loss: 1.5077 - acc: 0.6220 - LRFinder: val_loss: 4.1936 - lr = 0.00659385 \n",
            "230/390 [================>.............] - ETA: 56s - loss: 1.5076 - acc: 0.6220 - LRFinder: val_loss: 4.2230 - lr = 0.00661487 \n",
            "231/390 [================>.............] - ETA: 56s - loss: 1.5075 - acc: 0.6219 - LRFinder: val_loss: 4.0892 - lr = 0.00663590 \n",
            "232/390 [================>.............] - ETA: 56s - loss: 1.5069 - acc: 0.6220 - LRFinder: val_loss: 4.0920 - lr = 0.00665692 \n",
            "233/390 [================>.............] - ETA: 55s - loss: 1.5063 - acc: 0.6222 - LRFinder: val_loss: 4.0420 - lr = 0.00667795 \n",
            "234/390 [=================>............] - ETA: 55s - loss: 1.5064 - acc: 0.6222 - LRFinder: val_loss: 4.2144 - lr = 0.00669897 \n",
            "235/390 [=================>............] - ETA: 54s - loss: 1.5062 - acc: 0.6223 - LRFinder: val_loss: 4.0916 - lr = 0.00672000 \n",
            "236/390 [=================>............] - ETA: 54s - loss: 1.5061 - acc: 0.6224 - LRFinder: val_loss: 3.8902 - lr = 0.00674103 \n",
            "237/390 [=================>............] - ETA: 54s - loss: 1.5054 - acc: 0.6227 - LRFinder: val_loss: 3.8841 - lr = 0.00676205 \n",
            "238/390 [=================>............] - ETA: 53s - loss: 1.5051 - acc: 0.6228 - LRFinder: val_loss: 3.9923 - lr = 0.00678308 \n",
            "239/390 [=================>............] - ETA: 53s - loss: 1.5055 - acc: 0.6225 - LRFinder: val_loss: 3.8476 - lr = 0.00680410 \n",
            "240/390 [=================>............] - ETA: 53s - loss: 1.5042 - acc: 0.6230 - LRFinder: val_loss: 3.8184 - lr = 0.00682513 \n",
            "241/390 [=================>............] - ETA: 52s - loss: 1.5039 - acc: 0.6232 - LRFinder: val_loss: 3.7845 - lr = 0.00684615 \n",
            "242/390 [=================>............] - ETA: 52s - loss: 1.5034 - acc: 0.6235 - LRFinder: val_loss: 3.8897 - lr = 0.00686718 \n",
            "243/390 [=================>............] - ETA: 52s - loss: 1.5029 - acc: 0.6236 - LRFinder: val_loss: 3.8553 - lr = 0.00688821 \n",
            "244/390 [=================>............] - ETA: 51s - loss: 1.5024 - acc: 0.6236 - LRFinder: val_loss: 3.8571 - lr = 0.00690923 \n",
            "245/390 [=================>............] - ETA: 51s - loss: 1.5020 - acc: 0.6235 - LRFinder: val_loss: 3.7684 - lr = 0.00693026 \n",
            "246/390 [=================>............] - ETA: 50s - loss: 1.5019 - acc: 0.6234 - LRFinder: val_loss: 3.8252 - lr = 0.00695128 \n",
            "247/390 [==================>...........] - ETA: 50s - loss: 1.5015 - acc: 0.6235 - LRFinder: val_loss: 3.7357 - lr = 0.00697231 \n",
            "248/390 [==================>...........] - ETA: 50s - loss: 1.5013 - acc: 0.6236 - LRFinder: val_loss: 3.8156 - lr = 0.00699333 \n",
            "249/390 [==================>...........] - ETA: 49s - loss: 1.5020 - acc: 0.6235 - LRFinder: val_loss: 3.8706 - lr = 0.00701436 \n",
            "250/390 [==================>...........] - ETA: 49s - loss: 1.5014 - acc: 0.6237 - LRFinder: val_loss: 3.9080 - lr = 0.00703538 \n",
            "251/390 [==================>...........] - ETA: 49s - loss: 1.5015 - acc: 0.6237 - LRFinder: val_loss: 3.9127 - lr = 0.00705641 \n",
            "252/390 [==================>...........] - ETA: 48s - loss: 1.5023 - acc: 0.6235 - LRFinder: val_loss: 4.0286 - lr = 0.00707744 \n",
            "253/390 [==================>...........] - ETA: 48s - loss: 1.5027 - acc: 0.6234 - LRFinder: val_loss: 4.0027 - lr = 0.00709846 \n",
            "254/390 [==================>...........] - ETA: 48s - loss: 1.5024 - acc: 0.6233 - LRFinder: val_loss: 3.9508 - lr = 0.00711949 \n",
            "255/390 [==================>...........] - ETA: 47s - loss: 1.5024 - acc: 0.6234 - LRFinder: val_loss: 3.8839 - lr = 0.00714051 \n",
            "256/390 [==================>...........] - ETA: 47s - loss: 1.5021 - acc: 0.6238 - LRFinder: val_loss: 3.9050 - lr = 0.00716154 \n",
            "257/390 [==================>...........] - ETA: 46s - loss: 1.5019 - acc: 0.6237 - LRFinder: val_loss: 3.8554 - lr = 0.00718256 \n",
            "258/390 [==================>...........] - ETA: 46s - loss: 1.5015 - acc: 0.6238 - LRFinder: val_loss: 3.9389 - lr = 0.00720359 \n",
            "259/390 [==================>...........] - ETA: 46s - loss: 1.5011 - acc: 0.6239 - LRFinder: val_loss: 3.8452 - lr = 0.00722462 \n",
            "260/390 [===================>..........] - ETA: 45s - loss: 1.5011 - acc: 0.6239 - LRFinder: val_loss: 3.8972 - lr = 0.00724564 \n",
            "261/390 [===================>..........] - ETA: 45s - loss: 1.5009 - acc: 0.6239 - LRFinder: val_loss: 3.8106 - lr = 0.00726667 \n",
            "262/390 [===================>..........] - ETA: 45s - loss: 1.5009 - acc: 0.6237 - LRFinder: val_loss: 3.9928 - lr = 0.00728769 \n",
            "263/390 [===================>..........] - ETA: 44s - loss: 1.5005 - acc: 0.6240 - LRFinder: val_loss: 4.0023 - lr = 0.00730872 \n",
            "264/390 [===================>..........] - ETA: 44s - loss: 1.5003 - acc: 0.6239 - LRFinder: val_loss: 4.1487 - lr = 0.00732974 \n",
            "265/390 [===================>..........] - ETA: 44s - loss: 1.5004 - acc: 0.6239 - LRFinder: val_loss: 3.9881 - lr = 0.00735077 \n",
            "266/390 [===================>..........] - ETA: 43s - loss: 1.5003 - acc: 0.6239 - LRFinder: val_loss: 4.1872 - lr = 0.00737179 \n",
            "267/390 [===================>..........] - ETA: 43s - loss: 1.5003 - acc: 0.6238 - LRFinder: val_loss: 4.1754 - lr = 0.00739282 \n",
            "268/390 [===================>..........] - ETA: 43s - loss: 1.5000 - acc: 0.6240 - LRFinder: val_loss: 3.9918 - lr = 0.00741385 \n",
            "269/390 [===================>..........] - ETA: 42s - loss: 1.5000 - acc: 0.6240 - LRFinder: val_loss: 4.0258 - lr = 0.00743487 \n",
            "270/390 [===================>..........] - ETA: 42s - loss: 1.5000 - acc: 0.6240 - LRFinder: val_loss: 4.0393 - lr = 0.00745590 \n",
            "271/390 [===================>..........] - ETA: 41s - loss: 1.4994 - acc: 0.6244 - LRFinder: val_loss: 3.9814 - lr = 0.00747692 \n",
            "272/390 [===================>..........] - ETA: 41s - loss: 1.4992 - acc: 0.6245 - LRFinder: val_loss: 4.0866 - lr = 0.00749795 \n",
            "273/390 [====================>.........] - ETA: 41s - loss: 1.4993 - acc: 0.6245 - LRFinder: val_loss: 4.0932 - lr = 0.00751897 \n",
            "274/390 [====================>.........] - ETA: 40s - loss: 1.4992 - acc: 0.6245 - LRFinder: val_loss: 4.0665 - lr = 0.00754000 \n",
            "275/390 [====================>.........] - ETA: 40s - loss: 1.4992 - acc: 0.6246 - LRFinder: val_loss: 4.1888 - lr = 0.00756103 \n",
            "276/390 [====================>.........] - ETA: 40s - loss: 1.4994 - acc: 0.6246 - LRFinder: val_loss: 4.0347 - lr = 0.00758205 \n",
            "277/390 [====================>.........] - ETA: 39s - loss: 1.4996 - acc: 0.6248 - LRFinder: val_loss: 3.8545 - lr = 0.00760308 \n",
            "278/390 [====================>.........] - ETA: 39s - loss: 1.4993 - acc: 0.6249 - LRFinder: val_loss: 4.1079 - lr = 0.00762410 \n",
            "279/390 [====================>.........] - ETA: 39s - loss: 1.4998 - acc: 0.6246 - LRFinder: val_loss: 3.9996 - lr = 0.00764513 \n",
            "280/390 [====================>.........] - ETA: 38s - loss: 1.4997 - acc: 0.6247 - LRFinder: val_loss: 3.8343 - lr = 0.00766615 \n",
            "281/390 [====================>.........] - ETA: 38s - loss: 1.4992 - acc: 0.6249 - LRFinder: val_loss: 4.0138 - lr = 0.00768718 \n",
            "282/390 [====================>.........] - ETA: 38s - loss: 1.4992 - acc: 0.6248 - LRFinder: val_loss: 4.0134 - lr = 0.00770821 \n",
            "283/390 [====================>.........] - ETA: 37s - loss: 1.4993 - acc: 0.6246 - LRFinder: val_loss: 4.0652 - lr = 0.00772923 \n",
            "284/390 [====================>.........] - ETA: 37s - loss: 1.4986 - acc: 0.6249 - LRFinder: val_loss: 4.0063 - lr = 0.00775026 \n",
            "285/390 [====================>.........] - ETA: 36s - loss: 1.4986 - acc: 0.6249 - LRFinder: val_loss: 4.0031 - lr = 0.00777128 \n",
            "286/390 [=====================>........] - ETA: 36s - loss: 1.4985 - acc: 0.6249 - LRFinder: val_loss: 3.9266 - lr = 0.00779231 \n",
            "287/390 [=====================>........] - ETA: 36s - loss: 1.4984 - acc: 0.6251 - LRFinder: val_loss: 4.0194 - lr = 0.00781333 \n",
            "288/390 [=====================>........] - ETA: 35s - loss: 1.4980 - acc: 0.6252 - LRFinder: val_loss: 3.9664 - lr = 0.00783436 \n",
            "289/390 [=====================>........] - ETA: 35s - loss: 1.4979 - acc: 0.6251 - LRFinder: val_loss: 3.8059 - lr = 0.00785538 \n",
            "290/390 [=====================>........] - ETA: 35s - loss: 1.4980 - acc: 0.6250 - LRFinder: val_loss: 3.9868 - lr = 0.00787641 \n",
            "291/390 [=====================>........] - ETA: 34s - loss: 1.4979 - acc: 0.6250 - LRFinder: val_loss: 4.0202 - lr = 0.00789744 \n",
            "292/390 [=====================>........] - ETA: 34s - loss: 1.4979 - acc: 0.6250 - LRFinder: val_loss: 3.9752 - lr = 0.00791846 \n",
            "293/390 [=====================>........] - ETA: 34s - loss: 1.4985 - acc: 0.6245 - LRFinder: val_loss: 4.0613 - lr = 0.00793949 \n",
            "294/390 [=====================>........] - ETA: 33s - loss: 1.4982 - acc: 0.6248 - LRFinder: val_loss: 4.0725 - lr = 0.00796051 \n",
            "295/390 [=====================>........] - ETA: 33s - loss: 1.4981 - acc: 0.6247 - LRFinder: val_loss: 3.9039 - lr = 0.00798154 \n",
            "296/390 [=====================>........] - ETA: 33s - loss: 1.4981 - acc: 0.6247 - LRFinder: val_loss: 3.9388 - lr = 0.00800256 \n",
            "297/390 [=====================>........] - ETA: 32s - loss: 1.4982 - acc: 0.6246 - LRFinder: val_loss: 3.8768 - lr = 0.00802359 \n",
            "298/390 [=====================>........] - ETA: 32s - loss: 1.4985 - acc: 0.6245 - LRFinder: val_loss: 3.6809 - lr = 0.00804462 \n",
            "299/390 [======================>.......] - ETA: 32s - loss: 1.4978 - acc: 0.6247 - LRFinder: val_loss: 3.9365 - lr = 0.00806564 \n",
            "300/390 [======================>.......] - ETA: 31s - loss: 1.4977 - acc: 0.6247 - LRFinder: val_loss: 3.9132 - lr = 0.00808667 \n",
            "301/390 [======================>.......] - ETA: 31s - loss: 1.4977 - acc: 0.6246 - LRFinder: val_loss: 3.8872 - lr = 0.00810769 \n",
            "302/390 [======================>.......] - ETA: 30s - loss: 1.4981 - acc: 0.6245 - LRFinder: val_loss: 3.8033 - lr = 0.00812872 \n",
            "303/390 [======================>.......] - ETA: 30s - loss: 1.4982 - acc: 0.6245 - LRFinder: val_loss: 3.7647 - lr = 0.00814974 \n",
            "304/390 [======================>.......] - ETA: 30s - loss: 1.4979 - acc: 0.6246 - LRFinder: val_loss: 3.8074 - lr = 0.00817077 \n",
            "305/390 [======================>.......] - ETA: 29s - loss: 1.4979 - acc: 0.6247 - LRFinder: val_loss: 3.9736 - lr = 0.00819179 \n",
            "306/390 [======================>.......] - ETA: 29s - loss: 1.4975 - acc: 0.6250 - LRFinder: val_loss: 4.0006 - lr = 0.00821282 \n",
            "307/390 [======================>.......] - ETA: 29s - loss: 1.4974 - acc: 0.6249 - LRFinder: val_loss: 3.8381 - lr = 0.00823385 \n",
            "308/390 [======================>.......] - ETA: 28s - loss: 1.4977 - acc: 0.6246 - LRFinder: val_loss: 3.9662 - lr = 0.00825487 \n",
            "309/390 [======================>.......] - ETA: 28s - loss: 1.4973 - acc: 0.6247 - LRFinder: val_loss: 4.1668 - lr = 0.00827590 \n",
            "310/390 [======================>.......] - ETA: 28s - loss: 1.4972 - acc: 0.6246 - LRFinder: val_loss: 4.0514 - lr = 0.00829692 \n",
            "311/390 [======================>.......] - ETA: 27s - loss: 1.4968 - acc: 0.6248 - LRFinder: val_loss: 3.8867 - lr = 0.00831795 \n",
            "312/390 [=======================>......] - ETA: 27s - loss: 1.4964 - acc: 0.6248 - LRFinder: val_loss: 4.1032 - lr = 0.00833897 \n",
            "313/390 [=======================>......] - ETA: 27s - loss: 1.4955 - acc: 0.6252 - LRFinder: val_loss: 3.9618 - lr = 0.00836000 \n",
            "314/390 [=======================>......] - ETA: 26s - loss: 1.4953 - acc: 0.6252 - LRFinder: val_loss: 3.8529 - lr = 0.00838103 \n",
            "315/390 [=======================>......] - ETA: 26s - loss: 1.4949 - acc: 0.6253 - LRFinder: val_loss: 3.9865 - lr = 0.00840205 \n",
            "316/390 [=======================>......] - ETA: 25s - loss: 1.4944 - acc: 0.6255 - LRFinder: val_loss: 4.0363 - lr = 0.00842308 \n",
            "317/390 [=======================>......] - ETA: 25s - loss: 1.4947 - acc: 0.6253 - LRFinder: val_loss: 4.0719 - lr = 0.00844410 \n",
            "318/390 [=======================>......] - ETA: 25s - loss: 1.4943 - acc: 0.6254 - LRFinder: val_loss: 4.0134 - lr = 0.00846513 \n",
            "319/390 [=======================>......] - ETA: 24s - loss: 1.4938 - acc: 0.6257 - LRFinder: val_loss: 3.8790 - lr = 0.00848615 \n",
            "320/390 [=======================>......] - ETA: 24s - loss: 1.4937 - acc: 0.6257 - LRFinder: val_loss: 3.9758 - lr = 0.00850718 \n",
            "321/390 [=======================>......] - ETA: 24s - loss: 1.4933 - acc: 0.6259 - LRFinder: val_loss: 3.9833 - lr = 0.00852821 \n",
            "322/390 [=======================>......] - ETA: 23s - loss: 1.4930 - acc: 0.6260 - LRFinder: val_loss: 4.0389 - lr = 0.00854923 \n",
            "323/390 [=======================>......] - ETA: 23s - loss: 1.4929 - acc: 0.6260 - LRFinder: val_loss: 3.9353 - lr = 0.00857026 \n",
            "324/390 [=======================>......] - ETA: 23s - loss: 1.4927 - acc: 0.6263 - LRFinder: val_loss: 3.9584 - lr = 0.00859128 \n",
            "325/390 [========================>.....] - ETA: 22s - loss: 1.4927 - acc: 0.6263 - LRFinder: val_loss: 3.9515 - lr = 0.00861231 \n",
            "326/390 [========================>.....] - ETA: 22s - loss: 1.4925 - acc: 0.6266 - LRFinder: val_loss: 3.7525 - lr = 0.00863333 \n",
            "327/390 [========================>.....] - ETA: 22s - loss: 1.4922 - acc: 0.6267 - LRFinder: val_loss: 3.8492 - lr = 0.00865436 \n",
            "328/390 [========================>.....] - ETA: 21s - loss: 1.4919 - acc: 0.6270 - LRFinder: val_loss: 3.8486 - lr = 0.00867538 \n",
            "329/390 [========================>.....] - ETA: 21s - loss: 1.4919 - acc: 0.6268 - LRFinder: val_loss: 3.8136 - lr = 0.00869641 \n",
            "330/390 [========================>.....] - ETA: 21s - loss: 1.4920 - acc: 0.6268 - LRFinder: val_loss: 3.9139 - lr = 0.00871744 \n",
            "331/390 [========================>.....] - ETA: 20s - loss: 1.4920 - acc: 0.6266 - LRFinder: val_loss: 4.1733 - lr = 0.00873846 \n",
            "332/390 [========================>.....] - ETA: 20s - loss: 1.4920 - acc: 0.6266 - LRFinder: val_loss: 3.9584 - lr = 0.00875949 \n",
            "333/390 [========================>.....] - ETA: 19s - loss: 1.4918 - acc: 0.6267 - LRFinder: val_loss: 3.9026 - lr = 0.00878051 \n",
            "334/390 [========================>.....] - ETA: 19s - loss: 1.4916 - acc: 0.6266 - LRFinder: val_loss: 3.9108 - lr = 0.00880154 \n",
            "335/390 [========================>.....] - ETA: 19s - loss: 1.4913 - acc: 0.6266 - LRFinder: val_loss: 3.9454 - lr = 0.00882256 \n",
            "336/390 [========================>.....] - ETA: 18s - loss: 1.4912 - acc: 0.6267 - LRFinder: val_loss: 3.9674 - lr = 0.00884359 \n",
            "337/390 [========================>.....] - ETA: 18s - loss: 1.4906 - acc: 0.6269 - LRFinder: val_loss: 3.8698 - lr = 0.00886462 \n",
            "338/390 [=========================>....] - ETA: 18s - loss: 1.4908 - acc: 0.6269 - LRFinder: val_loss: 3.9968 - lr = 0.00888564 \n",
            "339/390 [=========================>....] - ETA: 17s - loss: 1.4914 - acc: 0.6267 - LRFinder: val_loss: 4.1695 - lr = 0.00890667 \n",
            "340/390 [=========================>....] - ETA: 17s - loss: 1.4915 - acc: 0.6267 - LRFinder: val_loss: 3.9416 - lr = 0.00892769 \n",
            "341/390 [=========================>....] - ETA: 17s - loss: 1.4918 - acc: 0.6266 - LRFinder: val_loss: 3.9652 - lr = 0.00894872 \n",
            "342/390 [=========================>....] - ETA: 16s - loss: 1.4916 - acc: 0.6265 - LRFinder: val_loss: 3.9179 - lr = 0.00896974 \n",
            "343/390 [=========================>....] - ETA: 16s - loss: 1.4914 - acc: 0.6265 - LRFinder: val_loss: 3.9631 - lr = 0.00899077 \n",
            "344/390 [=========================>....] - ETA: 16s - loss: 1.4915 - acc: 0.6264 - LRFinder: val_loss: 4.0438 - lr = 0.00901179 \n",
            "345/390 [=========================>....] - ETA: 15s - loss: 1.4916 - acc: 0.6264 - LRFinder: val_loss: 4.1192 - lr = 0.00903282 \n",
            "346/390 [=========================>....] - ETA: 15s - loss: 1.4914 - acc: 0.6265 - LRFinder: val_loss: 4.1017 - lr = 0.00905385 \n",
            "347/390 [=========================>....] - ETA: 15s - loss: 1.4912 - acc: 0.6265 - LRFinder: val_loss: 4.0345 - lr = 0.00907487 \n",
            "348/390 [=========================>....] - ETA: 14s - loss: 1.4912 - acc: 0.6265 - LRFinder: val_loss: 4.2798 - lr = 0.00909590 \n",
            "349/390 [=========================>....] - ETA: 14s - loss: 1.4913 - acc: 0.6265 - LRFinder: val_loss: 3.9143 - lr = 0.00911692 \n",
            "350/390 [=========================>....] - ETA: 14s - loss: 1.4907 - acc: 0.6266 - LRFinder: val_loss: 4.0476 - lr = 0.00913795 \n",
            "351/390 [==========================>...] - ETA: 13s - loss: 1.4908 - acc: 0.6266 - LRFinder: val_loss: 3.9945 - lr = 0.00915897 \n",
            "352/390 [==========================>...] - ETA: 13s - loss: 1.4908 - acc: 0.6266 - LRFinder: val_loss: 3.9586 - lr = 0.00918000 \n",
            "353/390 [==========================>...] - ETA: 12s - loss: 1.4912 - acc: 0.6265 - LRFinder: val_loss: 3.9583 - lr = 0.00920103 \n",
            "354/390 [==========================>...] - ETA: 12s - loss: 1.4912 - acc: 0.6263 - LRFinder: val_loss: 3.8072 - lr = 0.00922205 \n",
            "355/390 [==========================>...] - ETA: 12s - loss: 1.4914 - acc: 0.6261 - LRFinder: val_loss: 4.0621 - lr = 0.00924308 \n",
            "356/390 [==========================>...] - ETA: 11s - loss: 1.4910 - acc: 0.6263 - LRFinder: val_loss: 3.8913 - lr = 0.00926410 \n",
            "357/390 [==========================>...] - ETA: 11s - loss: 1.4911 - acc: 0.6262 - LRFinder: val_loss: 4.0938 - lr = 0.00928513 \n",
            "358/390 [==========================>...] - ETA: 11s - loss: 1.4913 - acc: 0.6261 - LRFinder: val_loss: 3.9566 - lr = 0.00930615 \n",
            "359/390 [==========================>...] - ETA: 10s - loss: 1.4912 - acc: 0.6262 - LRFinder: val_loss: 3.6442 - lr = 0.00932718 \n",
            "360/390 [==========================>...] - ETA: 10s - loss: 1.4909 - acc: 0.6264 - LRFinder: val_loss: 3.9829 - lr = 0.00934821 \n",
            "361/390 [==========================>...] - ETA: 10s - loss: 1.4909 - acc: 0.6264 - LRFinder: val_loss: 3.9499 - lr = 0.00936923 \n",
            "362/390 [==========================>...] - ETA: 9s - loss: 1.4909 - acc: 0.6263  - LRFinder: val_loss: 3.8683 - lr = 0.00939026 \n",
            "363/390 [==========================>...] - ETA: 9s - loss: 1.4915 - acc: 0.6259 - LRFinder: val_loss: 3.7383 - lr = 0.00941128 \n",
            "364/390 [===========================>..] - ETA: 9s - loss: 1.4912 - acc: 0.6260 - LRFinder: val_loss: 3.7136 - lr = 0.00943231 \n",
            "365/390 [===========================>..] - ETA: 8s - loss: 1.4914 - acc: 0.6260 - LRFinder: val_loss: 3.7450 - lr = 0.00945333 \n",
            "366/390 [===========================>..] - ETA: 8s - loss: 1.4914 - acc: 0.6259 - LRFinder: val_loss: 3.6970 - lr = 0.00947436 \n",
            "367/390 [===========================>..] - ETA: 8s - loss: 1.4911 - acc: 0.6260 - LRFinder: val_loss: 3.9015 - lr = 0.00949538 \n",
            "368/390 [===========================>..] - ETA: 7s - loss: 1.4906 - acc: 0.6262 - LRFinder: val_loss: 3.8207 - lr = 0.00951641 \n",
            "369/390 [===========================>..] - ETA: 7s - loss: 1.4907 - acc: 0.6261 - LRFinder: val_loss: 3.7697 - lr = 0.00953744 \n",
            "370/390 [===========================>..] - ETA: 7s - loss: 1.4901 - acc: 0.6264 - LRFinder: val_loss: 3.8325 - lr = 0.00955846 \n",
            "371/390 [===========================>..] - ETA: 6s - loss: 1.4900 - acc: 0.6266 - LRFinder: val_loss: 3.9433 - lr = 0.00957949 \n",
            "372/390 [===========================>..] - ETA: 6s - loss: 1.4901 - acc: 0.6266 - LRFinder: val_loss: 3.9968 - lr = 0.00960051 \n",
            "373/390 [===========================>..] - ETA: 5s - loss: 1.4895 - acc: 0.6267 - LRFinder: val_loss: 3.8513 - lr = 0.00962154 \n",
            "374/390 [===========================>..] - ETA: 5s - loss: 1.4892 - acc: 0.6269 - LRFinder: val_loss: 3.9668 - lr = 0.00964256 \n",
            "375/390 [===========================>..] - ETA: 5s - loss: 1.4893 - acc: 0.6270 - LRFinder: val_loss: 3.8211 - lr = 0.00966359 \n",
            "376/390 [===========================>..] - ETA: 4s - loss: 1.4890 - acc: 0.6271 - LRFinder: val_loss: 3.8838 - lr = 0.00968462 \n",
            "377/390 [============================>.] - ETA: 4s - loss: 1.4884 - acc: 0.6274 - LRFinder: val_loss: 3.9223 - lr = 0.00970564 \n",
            "378/390 [============================>.] - ETA: 4s - loss: 1.4881 - acc: 0.6274 - LRFinder: val_loss: 3.8694 - lr = 0.00972667 \n",
            "379/390 [============================>.] - ETA: 3s - loss: 1.4883 - acc: 0.6274 - LRFinder: val_loss: 3.9507 - lr = 0.00974769 \n",
            "380/390 [============================>.] - ETA: 3s - loss: 1.4882 - acc: 0.6275 - LRFinder: val_loss: 4.0045 - lr = 0.00976872 \n",
            "381/390 [============================>.] - ETA: 3s - loss: 1.4883 - acc: 0.6276 - LRFinder: val_loss: 3.9304 - lr = 0.00978974 \n",
            "382/390 [============================>.] - ETA: 2s - loss: 1.4884 - acc: 0.6275 - LRFinder: val_loss: 4.0117 - lr = 0.00981077 \n",
            "383/390 [============================>.] - ETA: 2s - loss: 1.4886 - acc: 0.6275 - LRFinder: val_loss: 3.9245 - lr = 0.00983179 \n",
            "384/390 [============================>.] - ETA: 2s - loss: 1.4885 - acc: 0.6276 - LRFinder: val_loss: 3.9196 - lr = 0.00985282 \n",
            "385/390 [============================>.] - ETA: 1s - loss: 1.4883 - acc: 0.6276 - LRFinder: val_loss: 4.0523 - lr = 0.00987385 \n",
            "386/390 [============================>.] - ETA: 1s - loss: 1.4878 - acc: 0.6278 - LRFinder: val_loss: 3.9732 - lr = 0.00989487 \n",
            "387/390 [============================>.] - ETA: 1s - loss: 1.4881 - acc: 0.6277 - LRFinder: val_loss: 4.1340 - lr = 0.00991590 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 1.4882 - acc: 0.6277 - LRFinder: val_loss: 4.2488 - lr = 0.00993692 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.4890 - acc: 0.6276 - LRFinder: val_loss: 4.2583 - lr = 0.00995795 \n",
            " - LRFinder: val_loss: 4.0620 - lr = 0.00997897 \n",
            "390/390 [==============================] - 140s 358ms/step - loss: 1.4890 - acc: 0.6275 - val_loss: 4.1126 - val_acc: 0.1741\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {weights/weight_decay/weight_decay-1e-07/}\n",
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 15:41 - loss: 1.4659 - acc: 0.6250 - LRFinder: val_loss: 3.9785 - lr = 0.00180000 \n",
            "  2/390 [..............................] - ETA: 11:19 - loss: 1.4023 - acc: 0.6445 - LRFinder: val_loss: 4.2382 - lr = 0.00182103 \n",
            "  3/390 [..............................] - ETA: 8:19 - loss: 1.4359 - acc: 0.6328  - LRFinder: val_loss: 4.1582 - lr = 0.00184205 \n",
            "  4/390 [..............................] - ETA: 6:46 - loss: 1.4067 - acc: 0.6309 - LRFinder: val_loss: 4.0898 - lr = 0.00186308 \n",
            "  5/390 [..............................] - ETA: 5:50 - loss: 1.4100 - acc: 0.6375 - LRFinder: val_loss: 4.5551 - lr = 0.00188410 \n",
            "  6/390 [..............................] - ETA: 5:12 - loss: 1.4090 - acc: 0.6341 - LRFinder: val_loss: 4.1762 - lr = 0.00190513 \n",
            "  7/390 [..............................] - ETA: 4:46 - loss: 1.4093 - acc: 0.6395 - LRFinder: val_loss: 4.1177 - lr = 0.00192615 \n",
            "  8/390 [..............................] - ETA: 4:26 - loss: 1.4135 - acc: 0.6396 - LRFinder: val_loss: 4.1830 - lr = 0.00194718 \n",
            "  9/390 [..............................] - ETA: 4:10 - loss: 1.4196 - acc: 0.6415 - LRFinder: val_loss: 4.1602 - lr = 0.00196821 \n",
            " 10/390 [..............................] - ETA: 3:57 - loss: 1.4087 - acc: 0.6500 - LRFinder: val_loss: 4.1413 - lr = 0.00198923 \n",
            " 11/390 [..............................] - ETA: 3:47 - loss: 1.4132 - acc: 0.6477 - LRFinder: val_loss: 4.1454 - lr = 0.00201026 \n",
            " 12/390 [..............................] - ETA: 3:38 - loss: 1.4184 - acc: 0.6458 - LRFinder: val_loss: 4.1543 - lr = 0.00203128 \n",
            " 13/390 [>.............................] - ETA: 3:31 - loss: 1.4216 - acc: 0.6448 - LRFinder: val_loss: 4.1291 - lr = 0.00205231 \n",
            " 14/390 [>.............................] - ETA: 3:24 - loss: 1.4266 - acc: 0.6429 - LRFinder: val_loss: 4.0269 - lr = 0.00207333 \n",
            " 15/390 [>.............................] - ETA: 3:19 - loss: 1.4245 - acc: 0.6458 - LRFinder: val_loss: 4.1114 - lr = 0.00209436 \n",
            " 16/390 [>.............................] - ETA: 3:14 - loss: 1.4299 - acc: 0.6436 - LRFinder: val_loss: 4.0786 - lr = 0.00211538 \n",
            " 17/390 [>.............................] - ETA: 3:09 - loss: 1.4350 - acc: 0.6402 - LRFinder: val_loss: 4.2520 - lr = 0.00213641 \n",
            " 18/390 [>.............................] - ETA: 3:05 - loss: 1.4406 - acc: 0.6398 - LRFinder: val_loss: 4.1904 - lr = 0.00215744 \n",
            " 19/390 [>.............................] - ETA: 3:02 - loss: 1.4334 - acc: 0.6419 - LRFinder: val_loss: 4.1619 - lr = 0.00217846 \n",
            " 20/390 [>.............................] - ETA: 2:58 - loss: 1.4288 - acc: 0.6441 - LRFinder: val_loss: 4.0939 - lr = 0.00219949 \n",
            " 21/390 [>.............................] - ETA: 2:55 - loss: 1.4283 - acc: 0.6421 - LRFinder: val_loss: 3.9966 - lr = 0.00222051 \n",
            " 22/390 [>.............................] - ETA: 2:53 - loss: 1.4308 - acc: 0.6410 - LRFinder: val_loss: 4.1929 - lr = 0.00224154 \n",
            " 23/390 [>.............................] - ETA: 2:50 - loss: 1.4279 - acc: 0.6399 - LRFinder: val_loss: 4.2186 - lr = 0.00226256 \n",
            " 24/390 [>.............................] - ETA: 2:48 - loss: 1.4228 - acc: 0.6426 - LRFinder: val_loss: 4.1752 - lr = 0.00228359 \n",
            " 25/390 [>.............................] - ETA: 2:46 - loss: 1.4257 - acc: 0.6412 - LRFinder: val_loss: 4.0846 - lr = 0.00230462 \n",
            " 26/390 [=>............................] - ETA: 2:43 - loss: 1.4225 - acc: 0.6433 - LRFinder: val_loss: 4.2693 - lr = 0.00232564 \n",
            " 27/390 [=>............................] - ETA: 2:42 - loss: 1.4216 - acc: 0.6450 - LRFinder: val_loss: 4.0665 - lr = 0.00234667 \n",
            " 28/390 [=>............................] - ETA: 2:40 - loss: 1.4209 - acc: 0.6456 - LRFinder: val_loss: 4.0528 - lr = 0.00236769 \n",
            " 29/390 [=>............................] - ETA: 2:38 - loss: 1.4197 - acc: 0.6468 - LRFinder: val_loss: 4.1988 - lr = 0.00238872 \n",
            " 30/390 [=>............................] - ETA: 2:36 - loss: 1.4236 - acc: 0.6464 - LRFinder: val_loss: 3.9745 - lr = 0.00240974 \n",
            " 31/390 [=>............................] - ETA: 2:35 - loss: 1.4238 - acc: 0.6462 - LRFinder: val_loss: 4.0153 - lr = 0.00243077 \n",
            " 32/390 [=>............................] - ETA: 2:34 - loss: 1.4308 - acc: 0.6443 - LRFinder: val_loss: 3.9941 - lr = 0.00245179 \n",
            " 33/390 [=>............................] - ETA: 2:32 - loss: 1.4288 - acc: 0.6442 - LRFinder: val_loss: 4.0252 - lr = 0.00247282 \n",
            " 34/390 [=>............................] - ETA: 2:31 - loss: 1.4289 - acc: 0.6459 - LRFinder: val_loss: 4.1268 - lr = 0.00249385 \n",
            " 35/390 [=>............................] - ETA: 2:30 - loss: 1.4254 - acc: 0.6469 - LRFinder: val_loss: 4.1521 - lr = 0.00251487 \n",
            " 36/390 [=>............................] - ETA: 2:28 - loss: 1.4246 - acc: 0.6471 - LRFinder: val_loss: 4.0668 - lr = 0.00253590 \n",
            " 37/390 [=>............................] - ETA: 2:27 - loss: 1.4263 - acc: 0.6470 - LRFinder: val_loss: 4.0540 - lr = 0.00255692 \n",
            " 38/390 [=>............................] - ETA: 2:26 - loss: 1.4269 - acc: 0.6454 - LRFinder: val_loss: 4.0857 - lr = 0.00257795 \n",
            " 39/390 [==>...........................] - ETA: 2:25 - loss: 1.4280 - acc: 0.6456 - LRFinder: val_loss: 3.8264 - lr = 0.00259897 \n",
            " 40/390 [==>...........................] - ETA: 2:24 - loss: 1.4274 - acc: 0.6461 - LRFinder: val_loss: 3.7636 - lr = 0.00262000 \n",
            " 41/390 [==>...........................] - ETA: 2:23 - loss: 1.4281 - acc: 0.6463 - LRFinder: val_loss: 4.0169 - lr = 0.00264103 \n",
            " 42/390 [==>...........................] - ETA: 2:22 - loss: 1.4313 - acc: 0.6462 - LRFinder: val_loss: 3.9253 - lr = 0.00266205 \n",
            " 43/390 [==>...........................] - ETA: 2:21 - loss: 1.4284 - acc: 0.6466 - LRFinder: val_loss: 4.0386 - lr = 0.00268308 \n",
            " 44/390 [==>...........................] - ETA: 2:20 - loss: 1.4273 - acc: 0.6472 - LRFinder: val_loss: 3.9928 - lr = 0.00270410 \n",
            " 45/390 [==>...........................] - ETA: 2:19 - loss: 1.4263 - acc: 0.6467 - LRFinder: val_loss: 3.7510 - lr = 0.00272513 \n",
            " 46/390 [==>...........................] - ETA: 2:18 - loss: 1.4242 - acc: 0.6478 - LRFinder: val_loss: 3.8148 - lr = 0.00274615 \n",
            " 47/390 [==>...........................] - ETA: 2:17 - loss: 1.4227 - acc: 0.6474 - LRFinder: val_loss: 3.9087 - lr = 0.00276718 \n",
            " 48/390 [==>...........................] - ETA: 2:16 - loss: 1.4263 - acc: 0.6468 - LRFinder: val_loss: 3.8889 - lr = 0.00278821 \n",
            " 49/390 [==>...........................] - ETA: 2:16 - loss: 1.4293 - acc: 0.6467 - LRFinder: val_loss: 4.0337 - lr = 0.00280923 \n",
            " 50/390 [==>...........................] - ETA: 2:15 - loss: 1.4318 - acc: 0.6462 - LRFinder: val_loss: 3.8367 - lr = 0.00283026 \n",
            " 51/390 [==>...........................] - ETA: 2:14 - loss: 1.4361 - acc: 0.6452 - LRFinder: val_loss: 4.1147 - lr = 0.00285128 \n",
            " 52/390 [===>..........................] - ETA: 2:13 - loss: 1.4372 - acc: 0.6453 - LRFinder: val_loss: 3.9047 - lr = 0.00287231 \n",
            " 53/390 [===>..........................] - ETA: 2:13 - loss: 1.4379 - acc: 0.6443 - LRFinder: val_loss: 3.9655 - lr = 0.00289333 \n",
            " 54/390 [===>..........................] - ETA: 2:12 - loss: 1.4407 - acc: 0.6434 - LRFinder: val_loss: 4.1328 - lr = 0.00291436 \n",
            " 55/390 [===>..........................] - ETA: 2:11 - loss: 1.4399 - acc: 0.6445 - LRFinder: val_loss: 4.0102 - lr = 0.00293538 \n",
            " 56/390 [===>..........................] - ETA: 2:10 - loss: 1.4391 - acc: 0.6461 - LRFinder: val_loss: 4.0877 - lr = 0.00295641 \n",
            " 57/390 [===>..........................] - ETA: 2:10 - loss: 1.4398 - acc: 0.6456 - LRFinder: val_loss: 4.1217 - lr = 0.00297744 \n",
            " 58/390 [===>..........................] - ETA: 2:09 - loss: 1.4415 - acc: 0.6448 - LRFinder: val_loss: 4.2596 - lr = 0.00299846 \n",
            " 59/390 [===>..........................] - ETA: 2:08 - loss: 1.4399 - acc: 0.6459 - LRFinder: val_loss: 4.0697 - lr = 0.00301949 \n",
            " 60/390 [===>..........................] - ETA: 2:08 - loss: 1.4384 - acc: 0.6462 - LRFinder: val_loss: 4.2000 - lr = 0.00304051 \n",
            " 61/390 [===>..........................] - ETA: 2:07 - loss: 1.4376 - acc: 0.6469 - LRFinder: val_loss: 4.0818 - lr = 0.00306154 \n",
            " 62/390 [===>..........................] - ETA: 2:06 - loss: 1.4393 - acc: 0.6463 - LRFinder: val_loss: 4.0398 - lr = 0.00308256 \n",
            " 63/390 [===>..........................] - ETA: 2:06 - loss: 1.4384 - acc: 0.6463 - LRFinder: val_loss: 4.0517 - lr = 0.00310359 \n",
            " 64/390 [===>..........................] - ETA: 2:05 - loss: 1.4393 - acc: 0.6458 - LRFinder: val_loss: 4.1106 - lr = 0.00312462 \n",
            " 65/390 [====>.........................] - ETA: 2:05 - loss: 1.4394 - acc: 0.6456 - LRFinder: val_loss: 4.0436 - lr = 0.00314564 \n",
            " 66/390 [====>.........................] - ETA: 2:04 - loss: 1.4399 - acc: 0.6462 - LRFinder: val_loss: 4.2159 - lr = 0.00316667 \n",
            " 67/390 [====>.........................] - ETA: 2:03 - loss: 1.4400 - acc: 0.6456 - LRFinder: val_loss: 4.1034 - lr = 0.00318769 \n",
            " 68/390 [====>.........................] - ETA: 2:03 - loss: 1.4390 - acc: 0.6459 - LRFinder: val_loss: 4.1935 - lr = 0.00320872 \n",
            " 69/390 [====>.........................] - ETA: 2:02 - loss: 1.4391 - acc: 0.6458 - LRFinder: val_loss: 4.0542 - lr = 0.00322974 \n",
            " 70/390 [====>.........................] - ETA: 2:02 - loss: 1.4381 - acc: 0.6465 - LRFinder: val_loss: 4.1722 - lr = 0.00325077 \n",
            " 71/390 [====>.........................] - ETA: 2:01 - loss: 1.4366 - acc: 0.6476 - LRFinder: val_loss: 4.0178 - lr = 0.00327179 \n",
            " 72/390 [====>.........................] - ETA: 2:01 - loss: 1.4374 - acc: 0.6480 - LRFinder: val_loss: 4.2209 - lr = 0.00329282 \n",
            " 73/390 [====>.........................] - ETA: 2:00 - loss: 1.4363 - acc: 0.6487 - LRFinder: val_loss: 4.1043 - lr = 0.00331385 \n",
            " 74/390 [====>.........................] - ETA: 1:59 - loss: 1.4346 - acc: 0.6495 - LRFinder: val_loss: 4.0872 - lr = 0.00333487 \n",
            " 75/390 [====>.........................] - ETA: 1:59 - loss: 1.4358 - acc: 0.6492 - LRFinder: val_loss: 4.2666 - lr = 0.00335590 \n",
            " 76/390 [====>.........................] - ETA: 1:58 - loss: 1.4359 - acc: 0.6500 - LRFinder: val_loss: 3.9267 - lr = 0.00337692 \n",
            " 77/390 [====>.........................] - ETA: 1:58 - loss: 1.4357 - acc: 0.6497 - LRFinder: val_loss: 3.8764 - lr = 0.00339795 \n",
            " 78/390 [=====>........................] - ETA: 1:57 - loss: 1.4351 - acc: 0.6500 - LRFinder: val_loss: 4.0399 - lr = 0.00341897 \n",
            " 79/390 [=====>........................] - ETA: 1:57 - loss: 1.4334 - acc: 0.6501 - LRFinder: val_loss: 3.9738 - lr = 0.00344000 \n",
            " 80/390 [=====>........................] - ETA: 1:56 - loss: 1.4326 - acc: 0.6505 - LRFinder: val_loss: 4.0310 - lr = 0.00346103 \n",
            " 81/390 [=====>........................] - ETA: 1:56 - loss: 1.4337 - acc: 0.6499 - LRFinder: val_loss: 4.0771 - lr = 0.00348205 \n",
            " 82/390 [=====>........................] - ETA: 1:55 - loss: 1.4332 - acc: 0.6498 - LRFinder: val_loss: 3.9818 - lr = 0.00350308 \n",
            " 83/390 [=====>........................] - ETA: 1:55 - loss: 1.4321 - acc: 0.6499 - LRFinder: val_loss: 4.3037 - lr = 0.00352410 \n",
            " 84/390 [=====>........................] - ETA: 1:54 - loss: 1.4337 - acc: 0.6492 - LRFinder: val_loss: 4.2490 - lr = 0.00354513 \n",
            " 85/390 [=====>........................] - ETA: 1:54 - loss: 1.4322 - acc: 0.6494 - LRFinder: val_loss: 4.1336 - lr = 0.00356615 \n",
            " 86/390 [=====>........................] - ETA: 1:53 - loss: 1.4312 - acc: 0.6499 - LRFinder: val_loss: 4.0525 - lr = 0.00358718 \n",
            " 87/390 [=====>........................] - ETA: 1:53 - loss: 1.4306 - acc: 0.6496 - LRFinder: val_loss: 4.1106 - lr = 0.00360821 \n",
            " 88/390 [=====>........................] - ETA: 1:52 - loss: 1.4312 - acc: 0.6499 - LRFinder: val_loss: 4.1934 - lr = 0.00362923 \n",
            " 89/390 [=====>........................] - ETA: 1:52 - loss: 1.4315 - acc: 0.6499 - LRFinder: val_loss: 4.2153 - lr = 0.00365026 \n",
            " 90/390 [=====>........................] - ETA: 1:51 - loss: 1.4299 - acc: 0.6503 - LRFinder: val_loss: 4.1438 - lr = 0.00367128 \n",
            " 91/390 [======>.......................] - ETA: 1:51 - loss: 1.4308 - acc: 0.6499 - LRFinder: val_loss: 4.2728 - lr = 0.00369231 \n",
            " 92/390 [======>.......................] - ETA: 1:50 - loss: 1.4307 - acc: 0.6502 - LRFinder: val_loss: 4.0692 - lr = 0.00371333 \n",
            " 93/390 [======>.......................] - ETA: 1:50 - loss: 1.4311 - acc: 0.6504 - LRFinder: val_loss: 3.9652 - lr = 0.00373436 \n",
            " 94/390 [======>.......................] - ETA: 1:50 - loss: 1.4295 - acc: 0.6511 - LRFinder: val_loss: 3.9353 - lr = 0.00375538 \n",
            " 95/390 [======>.......................] - ETA: 1:49 - loss: 1.4299 - acc: 0.6507 - LRFinder: val_loss: 4.0779 - lr = 0.00377641 \n",
            " 96/390 [======>.......................] - ETA: 1:49 - loss: 1.4307 - acc: 0.6501 - LRFinder: val_loss: 3.9748 - lr = 0.00379744 \n",
            " 97/390 [======>.......................] - ETA: 1:48 - loss: 1.4307 - acc: 0.6504 - LRFinder: val_loss: 3.9427 - lr = 0.00381846 \n",
            " 98/390 [======>.......................] - ETA: 1:48 - loss: 1.4322 - acc: 0.6498 - LRFinder: val_loss: 4.2508 - lr = 0.00383949 \n",
            " 99/390 [======>.......................] - ETA: 1:47 - loss: 1.4326 - acc: 0.6494 - LRFinder: val_loss: 3.8594 - lr = 0.00386051 \n",
            "100/390 [======>.......................] - ETA: 1:47 - loss: 1.4329 - acc: 0.6491 - LRFinder: val_loss: 4.1297 - lr = 0.00388154 \n",
            "101/390 [======>.......................] - ETA: 1:46 - loss: 1.4326 - acc: 0.6491 - LRFinder: val_loss: 4.2391 - lr = 0.00390256 \n",
            "102/390 [======>.......................] - ETA: 1:46 - loss: 1.4314 - acc: 0.6496 - LRFinder: val_loss: 4.1551 - lr = 0.00392359 \n",
            "103/390 [======>.......................] - ETA: 1:45 - loss: 1.4313 - acc: 0.6493 - LRFinder: val_loss: 3.9375 - lr = 0.00394462 \n",
            "104/390 [=======>......................] - ETA: 1:45 - loss: 1.4310 - acc: 0.6497 - LRFinder: val_loss: 4.2376 - lr = 0.00396564 \n",
            "105/390 [=======>......................] - ETA: 1:45 - loss: 1.4313 - acc: 0.6498 - LRFinder: val_loss: 3.9940 - lr = 0.00398667 \n",
            "106/390 [=======>......................] - ETA: 1:44 - loss: 1.4295 - acc: 0.6505 - LRFinder: val_loss: 4.1199 - lr = 0.00400769 \n",
            "107/390 [=======>......................] - ETA: 1:44 - loss: 1.4305 - acc: 0.6500 - LRFinder: val_loss: 4.2098 - lr = 0.00402872 \n",
            "108/390 [=======>......................] - ETA: 1:43 - loss: 1.4298 - acc: 0.6503 - LRFinder: val_loss: 4.0501 - lr = 0.00404974 \n",
            "109/390 [=======>......................] - ETA: 1:43 - loss: 1.4294 - acc: 0.6504 - LRFinder: val_loss: 4.1800 - lr = 0.00407077 \n",
            "110/390 [=======>......................] - ETA: 1:42 - loss: 1.4297 - acc: 0.6501 - LRFinder: val_loss: 4.2400 - lr = 0.00409179 \n",
            "111/390 [=======>......................] - ETA: 1:42 - loss: 1.4301 - acc: 0.6503 - LRFinder: val_loss: 4.1739 - lr = 0.00411282 \n",
            "112/390 [=======>......................] - ETA: 1:42 - loss: 1.4302 - acc: 0.6501 - LRFinder: val_loss: 4.0976 - lr = 0.00413385 \n",
            "113/390 [=======>......................] - ETA: 1:41 - loss: 1.4293 - acc: 0.6504 - LRFinder: val_loss: 4.0875 - lr = 0.00415487 \n",
            "114/390 [=======>......................] - ETA: 1:41 - loss: 1.4296 - acc: 0.6502 - LRFinder: val_loss: 4.0839 - lr = 0.00417590 \n",
            "115/390 [=======>......................] - ETA: 1:40 - loss: 1.4296 - acc: 0.6503 - LRFinder: val_loss: 3.9890 - lr = 0.00419692 \n",
            "116/390 [=======>......................] - ETA: 1:40 - loss: 1.4291 - acc: 0.6504 - LRFinder: val_loss: 4.1555 - lr = 0.00421795 \n",
            "117/390 [========>.....................] - ETA: 1:39 - loss: 1.4286 - acc: 0.6506 - LRFinder: val_loss: 4.1177 - lr = 0.00423897 \n",
            "118/390 [========>.....................] - ETA: 1:39 - loss: 1.4284 - acc: 0.6509 - LRFinder: val_loss: 4.0555 - lr = 0.00426000 \n",
            "119/390 [========>.....................] - ETA: 1:39 - loss: 1.4283 - acc: 0.6509 - LRFinder: val_loss: 4.2385 - lr = 0.00428103 \n",
            "120/390 [========>.....................] - ETA: 1:38 - loss: 1.4272 - acc: 0.6513 - LRFinder: val_loss: 4.2025 - lr = 0.00430205 \n",
            "121/390 [========>.....................] - ETA: 1:38 - loss: 1.4281 - acc: 0.6508 - LRFinder: val_loss: 4.1771 - lr = 0.00432308 \n",
            "122/390 [========>.....................] - ETA: 1:37 - loss: 1.4287 - acc: 0.6506 - LRFinder: val_loss: 4.1226 - lr = 0.00434410 \n",
            "123/390 [========>.....................] - ETA: 1:37 - loss: 1.4294 - acc: 0.6502 - LRFinder: val_loss: 4.1587 - lr = 0.00436513 \n",
            "124/390 [========>.....................] - ETA: 1:37 - loss: 1.4302 - acc: 0.6498 - LRFinder: val_loss: 4.1993 - lr = 0.00438615 \n",
            "125/390 [========>.....................] - ETA: 1:36 - loss: 1.4297 - acc: 0.6495 - LRFinder: val_loss: 4.3463 - lr = 0.00440718 \n",
            "126/390 [========>.....................] - ETA: 1:36 - loss: 1.4291 - acc: 0.6497 - LRFinder: val_loss: 4.1920 - lr = 0.00442821 \n",
            "127/390 [========>.....................] - ETA: 1:35 - loss: 1.4283 - acc: 0.6500 - LRFinder: val_loss: 4.3587 - lr = 0.00444923 \n",
            "128/390 [========>.....................] - ETA: 1:35 - loss: 1.4282 - acc: 0.6498 - LRFinder: val_loss: 4.2159 - lr = 0.00447026 \n",
            "129/390 [========>.....................] - ETA: 1:34 - loss: 1.4305 - acc: 0.6487 - LRFinder: val_loss: 4.2978 - lr = 0.00449128 \n",
            "130/390 [=========>....................] - ETA: 1:34 - loss: 1.4308 - acc: 0.6486 - LRFinder: val_loss: 4.2767 - lr = 0.00451231 \n",
            "131/390 [=========>....................] - ETA: 1:34 - loss: 1.4302 - acc: 0.6487 - LRFinder: val_loss: 4.4187 - lr = 0.00453333 \n",
            "132/390 [=========>....................] - ETA: 1:33 - loss: 1.4294 - acc: 0.6492 - LRFinder: val_loss: 4.3321 - lr = 0.00455436 \n",
            "133/390 [=========>....................] - ETA: 1:33 - loss: 1.4292 - acc: 0.6493 - LRFinder: val_loss: 4.1397 - lr = 0.00457538 \n",
            "134/390 [=========>....................] - ETA: 1:32 - loss: 1.4291 - acc: 0.6494 - LRFinder: val_loss: 4.2416 - lr = 0.00459641 \n",
            "135/390 [=========>....................] - ETA: 1:32 - loss: 1.4282 - acc: 0.6499 - LRFinder: val_loss: 4.2449 - lr = 0.00461744 \n",
            "136/390 [=========>....................] - ETA: 1:32 - loss: 1.4286 - acc: 0.6499 - LRFinder: val_loss: 4.1986 - lr = 0.00463846 \n",
            "137/390 [=========>....................] - ETA: 1:31 - loss: 1.4288 - acc: 0.6499 - LRFinder: val_loss: 4.0376 - lr = 0.00465949 \n",
            "138/390 [=========>....................] - ETA: 1:31 - loss: 1.4283 - acc: 0.6502 - LRFinder: val_loss: 4.0203 - lr = 0.00468051 \n",
            "139/390 [=========>....................] - ETA: 1:30 - loss: 1.4285 - acc: 0.6499 - LRFinder: val_loss: 4.1493 - lr = 0.00470154 \n",
            "140/390 [=========>....................] - ETA: 1:30 - loss: 1.4288 - acc: 0.6498 - LRFinder: val_loss: 4.1017 - lr = 0.00472256 \n",
            "141/390 [=========>....................] - ETA: 1:30 - loss: 1.4291 - acc: 0.6499 - LRFinder: val_loss: 3.8766 - lr = 0.00474359 \n",
            "142/390 [=========>....................] - ETA: 1:29 - loss: 1.4301 - acc: 0.6493 - LRFinder: val_loss: 4.2382 - lr = 0.00476462 \n",
            "143/390 [==========>...................] - ETA: 1:29 - loss: 1.4294 - acc: 0.6495 - LRFinder: val_loss: 3.9388 - lr = 0.00478564 \n",
            "144/390 [==========>...................] - ETA: 1:29 - loss: 1.4298 - acc: 0.6496 - LRFinder: val_loss: 4.0132 - lr = 0.00480667 \n",
            "145/390 [==========>...................] - ETA: 1:28 - loss: 1.4299 - acc: 0.6494 - LRFinder: val_loss: 4.1257 - lr = 0.00482769 \n",
            "146/390 [==========>...................] - ETA: 1:28 - loss: 1.4297 - acc: 0.6493 - LRFinder: val_loss: 3.8338 - lr = 0.00484872 \n",
            "147/390 [==========>...................] - ETA: 1:27 - loss: 1.4292 - acc: 0.6496 - LRFinder: val_loss: 4.0151 - lr = 0.00486974 \n",
            "148/390 [==========>...................] - ETA: 1:27 - loss: 1.4295 - acc: 0.6496 - LRFinder: val_loss: 4.0807 - lr = 0.00489077 \n",
            "149/390 [==========>...................] - ETA: 1:27 - loss: 1.4298 - acc: 0.6493 - LRFinder: val_loss: 4.1011 - lr = 0.00491179 \n",
            "150/390 [==========>...................] - ETA: 1:26 - loss: 1.4297 - acc: 0.6491 - LRFinder: val_loss: 3.9522 - lr = 0.00493282 \n",
            "151/390 [==========>...................] - ETA: 1:26 - loss: 1.4299 - acc: 0.6490 - LRFinder: val_loss: 3.8016 - lr = 0.00495385 \n",
            "152/390 [==========>...................] - ETA: 1:25 - loss: 1.4306 - acc: 0.6487 - LRFinder: val_loss: 3.9544 - lr = 0.00497487 \n",
            "153/390 [==========>...................] - ETA: 1:25 - loss: 1.4308 - acc: 0.6488 - LRFinder: val_loss: 3.9745 - lr = 0.00499590 \n",
            "154/390 [==========>...................] - ETA: 1:25 - loss: 1.4304 - acc: 0.6489 - LRFinder: val_loss: 3.9214 - lr = 0.00501692 \n",
            "155/390 [==========>...................] - ETA: 1:24 - loss: 1.4306 - acc: 0.6491 - LRFinder: val_loss: 4.0385 - lr = 0.00503795 \n",
            "156/390 [===========>..................] - ETA: 1:24 - loss: 1.4301 - acc: 0.6493 - LRFinder: val_loss: 4.0187 - lr = 0.00505897 \n",
            "157/390 [===========>..................] - ETA: 1:23 - loss: 1.4310 - acc: 0.6488 - LRFinder: val_loss: 4.0220 - lr = 0.00508000 \n",
            "158/390 [===========>..................] - ETA: 1:23 - loss: 1.4324 - acc: 0.6484 - LRFinder: val_loss: 4.1003 - lr = 0.00510103 \n",
            "159/390 [===========>..................] - ETA: 1:23 - loss: 1.4324 - acc: 0.6481 - LRFinder: val_loss: 4.0005 - lr = 0.00512205 \n",
            "160/390 [===========>..................] - ETA: 1:22 - loss: 1.4322 - acc: 0.6482 - LRFinder: val_loss: 4.0196 - lr = 0.00514308 \n",
            "161/390 [===========>..................] - ETA: 1:22 - loss: 1.4321 - acc: 0.6484 - LRFinder: val_loss: 3.9803 - lr = 0.00516410 \n",
            "162/390 [===========>..................] - ETA: 1:21 - loss: 1.4319 - acc: 0.6484 - LRFinder: val_loss: 3.8744 - lr = 0.00518513 \n",
            "163/390 [===========>..................] - ETA: 1:21 - loss: 1.4319 - acc: 0.6485 - LRFinder: val_loss: 3.7774 - lr = 0.00520615 \n",
            "164/390 [===========>..................] - ETA: 1:21 - loss: 1.4312 - acc: 0.6489 - LRFinder: val_loss: 3.8074 - lr = 0.00522718 \n",
            "165/390 [===========>..................] - ETA: 1:20 - loss: 1.4317 - acc: 0.6484 - LRFinder: val_loss: 3.7915 - lr = 0.00524821 \n",
            "166/390 [===========>..................] - ETA: 1:20 - loss: 1.4319 - acc: 0.6486 - LRFinder: val_loss: 3.7343 - lr = 0.00526923 \n",
            "167/390 [===========>..................] - ETA: 1:20 - loss: 1.4311 - acc: 0.6489 - LRFinder: val_loss: 3.7497 - lr = 0.00529026 \n",
            "168/390 [===========>..................] - ETA: 1:19 - loss: 1.4308 - acc: 0.6489 - LRFinder: val_loss: 3.8977 - lr = 0.00531128 \n",
            "169/390 [============>.................] - ETA: 1:19 - loss: 1.4305 - acc: 0.6490 - LRFinder: val_loss: 3.9543 - lr = 0.00533231 \n",
            "170/390 [============>.................] - ETA: 1:18 - loss: 1.4306 - acc: 0.6488 - LRFinder: val_loss: 3.8920 - lr = 0.00535333 \n",
            "171/390 [============>.................] - ETA: 1:18 - loss: 1.4313 - acc: 0.6487 - LRFinder: val_loss: 3.8060 - lr = 0.00537436 \n",
            "172/390 [============>.................] - ETA: 1:18 - loss: 1.4312 - acc: 0.6488 - LRFinder: val_loss: 3.8584 - lr = 0.00539538 \n",
            "173/390 [============>.................] - ETA: 1:17 - loss: 1.4304 - acc: 0.6493 - LRFinder: val_loss: 3.6496 - lr = 0.00541641 \n",
            "174/390 [============>.................] - ETA: 1:17 - loss: 1.4303 - acc: 0.6495 - LRFinder: val_loss: 3.8121 - lr = 0.00543744 \n",
            "175/390 [============>.................] - ETA: 1:17 - loss: 1.4296 - acc: 0.6498 - LRFinder: val_loss: 3.8000 - lr = 0.00545846 \n",
            "176/390 [============>.................] - ETA: 1:16 - loss: 1.4287 - acc: 0.6501 - LRFinder: val_loss: 3.5519 - lr = 0.00547949 \n",
            "177/390 [============>.................] - ETA: 1:16 - loss: 1.4287 - acc: 0.6497 - LRFinder: val_loss: 3.8436 - lr = 0.00550051 \n",
            "178/390 [============>.................] - ETA: 1:15 - loss: 1.4278 - acc: 0.6502 - LRFinder: val_loss: 3.8783 - lr = 0.00552154 \n",
            "179/390 [============>.................] - ETA: 1:15 - loss: 1.4268 - acc: 0.6505 - LRFinder: val_loss: 3.7058 - lr = 0.00554256 \n",
            "180/390 [============>.................] - ETA: 1:15 - loss: 1.4268 - acc: 0.6506 - LRFinder: val_loss: 3.7771 - lr = 0.00556359 \n",
            "181/390 [============>.................] - ETA: 1:14 - loss: 1.4266 - acc: 0.6506 - LRFinder: val_loss: 3.7873 - lr = 0.00558462 \n",
            "182/390 [=============>................] - ETA: 1:14 - loss: 1.4262 - acc: 0.6508 - LRFinder: val_loss: 3.8031 - lr = 0.00560564 \n",
            "183/390 [=============>................] - ETA: 1:14 - loss: 1.4272 - acc: 0.6504 - LRFinder: val_loss: 3.6853 - lr = 0.00562667 \n",
            "184/390 [=============>................] - ETA: 1:13 - loss: 1.4267 - acc: 0.6506 - LRFinder: val_loss: 3.6718 - lr = 0.00564769 \n",
            "185/390 [=============>................] - ETA: 1:13 - loss: 1.4269 - acc: 0.6505 - LRFinder: val_loss: 3.9179 - lr = 0.00566872 \n",
            "186/390 [=============>................] - ETA: 1:12 - loss: 1.4262 - acc: 0.6510 - LRFinder: val_loss: 3.8861 - lr = 0.00568974 \n",
            "187/390 [=============>................] - ETA: 1:12 - loss: 1.4251 - acc: 0.6513 - LRFinder: val_loss: 3.7448 - lr = 0.00571077 \n",
            "188/390 [=============>................] - ETA: 1:12 - loss: 1.4247 - acc: 0.6515 - LRFinder: val_loss: 3.9768 - lr = 0.00573179 \n",
            "189/390 [=============>................] - ETA: 1:11 - loss: 1.4243 - acc: 0.6515 - LRFinder: val_loss: 3.9340 - lr = 0.00575282 \n",
            "190/390 [=============>................] - ETA: 1:11 - loss: 1.4243 - acc: 0.6514 - LRFinder: val_loss: 3.9031 - lr = 0.00577385 \n",
            "191/390 [=============>................] - ETA: 1:11 - loss: 1.4243 - acc: 0.6513 - LRFinder: val_loss: 3.8850 - lr = 0.00579487 \n",
            "192/390 [=============>................] - ETA: 1:10 - loss: 1.4238 - acc: 0.6515 - LRFinder: val_loss: 3.6958 - lr = 0.00581590 \n",
            "193/390 [=============>................] - ETA: 1:10 - loss: 1.4241 - acc: 0.6516 - LRFinder: val_loss: 3.8605 - lr = 0.00583692 \n",
            "194/390 [=============>................] - ETA: 1:09 - loss: 1.4241 - acc: 0.6516 - LRFinder: val_loss: 3.7461 - lr = 0.00585795 \n",
            "195/390 [==============>...............] - ETA: 1:09 - loss: 1.4242 - acc: 0.6516 - LRFinder: val_loss: 3.8683 - lr = 0.00587897 \n",
            "196/390 [==============>...............] - ETA: 1:09 - loss: 1.4250 - acc: 0.6513 - LRFinder: val_loss: 3.8264 - lr = 0.00590000 \n",
            "197/390 [==============>...............] - ETA: 1:08 - loss: 1.4253 - acc: 0.6515 - LRFinder: val_loss: 3.8741 - lr = 0.00592103 \n",
            "198/390 [==============>...............] - ETA: 1:08 - loss: 1.4256 - acc: 0.6512 - LRFinder: val_loss: 3.8614 - lr = 0.00594205 \n",
            "199/390 [==============>...............] - ETA: 1:08 - loss: 1.4254 - acc: 0.6513 - LRFinder: val_loss: 3.6858 - lr = 0.00596308 \n",
            "200/390 [==============>...............] - ETA: 1:07 - loss: 1.4258 - acc: 0.6511 - LRFinder: val_loss: 3.7154 - lr = 0.00598410 \n",
            "201/390 [==============>...............] - ETA: 1:07 - loss: 1.4254 - acc: 0.6512 - LRFinder: val_loss: 3.7850 - lr = 0.00600513 \n",
            "202/390 [==============>...............] - ETA: 1:06 - loss: 1.4253 - acc: 0.6513 - LRFinder: val_loss: 3.8297 - lr = 0.00602615 \n",
            "203/390 [==============>...............] - ETA: 1:06 - loss: 1.4255 - acc: 0.6511 - LRFinder: val_loss: 3.7902 - lr = 0.00604718 \n",
            "204/390 [==============>...............] - ETA: 1:06 - loss: 1.4254 - acc: 0.6512 - LRFinder: val_loss: 3.9115 - lr = 0.00606821 \n",
            "205/390 [==============>...............] - ETA: 1:05 - loss: 1.4254 - acc: 0.6510 - LRFinder: val_loss: 3.8182 - lr = 0.00608923 \n",
            "206/390 [==============>...............] - ETA: 1:05 - loss: 1.4253 - acc: 0.6511 - LRFinder: val_loss: 4.0940 - lr = 0.00611026 \n",
            "207/390 [==============>...............] - ETA: 1:05 - loss: 1.4252 - acc: 0.6512 - LRFinder: val_loss: 3.8176 - lr = 0.00613128 \n",
            "208/390 [===============>..............] - ETA: 1:04 - loss: 1.4251 - acc: 0.6511 - LRFinder: val_loss: 3.6904 - lr = 0.00615231 \n",
            "209/390 [===============>..............] - ETA: 1:04 - loss: 1.4244 - acc: 0.6513 - LRFinder: val_loss: 3.9843 - lr = 0.00617333 \n",
            "210/390 [===============>..............] - ETA: 1:04 - loss: 1.4246 - acc: 0.6514 - LRFinder: val_loss: 3.7866 - lr = 0.00619436 \n",
            "211/390 [===============>..............] - ETA: 1:03 - loss: 1.4250 - acc: 0.6512 - LRFinder: val_loss: 3.8635 - lr = 0.00621538 \n",
            "212/390 [===============>..............] - ETA: 1:03 - loss: 1.4253 - acc: 0.6511 - LRFinder: val_loss: 3.8840 - lr = 0.00623641 \n",
            "213/390 [===============>..............] - ETA: 1:02 - loss: 1.4260 - acc: 0.6510 - LRFinder: val_loss: 3.9902 - lr = 0.00625744 \n",
            "214/390 [===============>..............] - ETA: 1:02 - loss: 1.4261 - acc: 0.6508 - LRFinder: val_loss: 3.8433 - lr = 0.00627846 \n",
            "215/390 [===============>..............] - ETA: 1:02 - loss: 1.4261 - acc: 0.6508 - LRFinder: val_loss: 4.0091 - lr = 0.00629949 \n",
            "216/390 [===============>..............] - ETA: 1:01 - loss: 1.4263 - acc: 0.6508 - LRFinder: val_loss: 4.0171 - lr = 0.00632051 \n",
            "217/390 [===============>..............] - ETA: 1:01 - loss: 1.4263 - acc: 0.6504 - LRFinder: val_loss: 3.9164 - lr = 0.00634154 \n",
            "218/390 [===============>..............] - ETA: 1:01 - loss: 1.4257 - acc: 0.6506 - LRFinder: val_loss: 3.8704 - lr = 0.00636256 \n",
            "219/390 [===============>..............] - ETA: 1:00 - loss: 1.4251 - acc: 0.6509 - LRFinder: val_loss: 3.8994 - lr = 0.00638359 \n",
            "220/390 [===============>..............] - ETA: 1:00 - loss: 1.4249 - acc: 0.6509 - LRFinder: val_loss: 3.9203 - lr = 0.00640462 \n",
            "221/390 [================>.............] - ETA: 59s - loss: 1.4250 - acc: 0.6508  - LRFinder: val_loss: 3.8411 - lr = 0.00642564 \n",
            "222/390 [================>.............] - ETA: 59s - loss: 1.4244 - acc: 0.6510 - LRFinder: val_loss: 3.7742 - lr = 0.00644667 \n",
            "223/390 [================>.............] - ETA: 59s - loss: 1.4243 - acc: 0.6512 - LRFinder: val_loss: 3.7290 - lr = 0.00646769 \n",
            "224/390 [================>.............] - ETA: 58s - loss: 1.4249 - acc: 0.6512 - LRFinder: val_loss: 3.8300 - lr = 0.00648872 \n",
            "225/390 [================>.............] - ETA: 58s - loss: 1.4251 - acc: 0.6513 - LRFinder: val_loss: 3.8919 - lr = 0.00650974 \n",
            "226/390 [================>.............] - ETA: 58s - loss: 1.4245 - acc: 0.6514 - LRFinder: val_loss: 3.9259 - lr = 0.00653077 \n",
            "227/390 [================>.............] - ETA: 57s - loss: 1.4241 - acc: 0.6515 - LRFinder: val_loss: 3.7687 - lr = 0.00655179 \n",
            "228/390 [================>.............] - ETA: 57s - loss: 1.4236 - acc: 0.6515 - LRFinder: val_loss: 3.8753 - lr = 0.00657282 \n",
            "229/390 [================>.............] - ETA: 57s - loss: 1.4233 - acc: 0.6516 - LRFinder: val_loss: 3.8833 - lr = 0.00659385 \n",
            "230/390 [================>.............] - ETA: 56s - loss: 1.4240 - acc: 0.6514 - LRFinder: val_loss: 3.7877 - lr = 0.00661487 \n",
            "231/390 [================>.............] - ETA: 56s - loss: 1.4235 - acc: 0.6518 - LRFinder: val_loss: 3.8612 - lr = 0.00663590 \n",
            "232/390 [================>.............] - ETA: 55s - loss: 1.4229 - acc: 0.6520 - LRFinder: val_loss: 3.8603 - lr = 0.00665692 \n",
            "233/390 [================>.............] - ETA: 55s - loss: 1.4234 - acc: 0.6518 - LRFinder: val_loss: 3.7424 - lr = 0.00667795 \n",
            "234/390 [=================>............] - ETA: 55s - loss: 1.4234 - acc: 0.6518 - LRFinder: val_loss: 3.6100 - lr = 0.00669897 \n",
            "235/390 [=================>............] - ETA: 54s - loss: 1.4235 - acc: 0.6517 - LRFinder: val_loss: 3.6341 - lr = 0.00672000 \n",
            "236/390 [=================>............] - ETA: 54s - loss: 1.4236 - acc: 0.6515 - LRFinder: val_loss: 3.8964 - lr = 0.00674103 \n",
            "237/390 [=================>............] - ETA: 54s - loss: 1.4231 - acc: 0.6519 - LRFinder: val_loss: 3.7545 - lr = 0.00676205 \n",
            "238/390 [=================>............] - ETA: 53s - loss: 1.4232 - acc: 0.6519 - LRFinder: val_loss: 3.6648 - lr = 0.00678308 \n",
            "239/390 [=================>............] - ETA: 53s - loss: 1.4233 - acc: 0.6517 - LRFinder: val_loss: 3.6374 - lr = 0.00680410 \n",
            "240/390 [=================>............] - ETA: 53s - loss: 1.4231 - acc: 0.6519 - LRFinder: val_loss: 3.6633 - lr = 0.00682513 \n",
            "241/390 [=================>............] - ETA: 52s - loss: 1.4226 - acc: 0.6522 - LRFinder: val_loss: 3.6995 - lr = 0.00684615 \n",
            "242/390 [=================>............] - ETA: 52s - loss: 1.4223 - acc: 0.6524 - LRFinder: val_loss: 3.7670 - lr = 0.00686718 \n",
            "243/390 [=================>............] - ETA: 51s - loss: 1.4219 - acc: 0.6525 - LRFinder: val_loss: 3.9739 - lr = 0.00688821 \n",
            "244/390 [=================>............] - ETA: 51s - loss: 1.4218 - acc: 0.6524 - LRFinder: val_loss: 3.7418 - lr = 0.00690923 \n",
            "245/390 [=================>............] - ETA: 51s - loss: 1.4223 - acc: 0.6522 - LRFinder: val_loss: 3.7145 - lr = 0.00693026 \n",
            "246/390 [=================>............] - ETA: 50s - loss: 1.4222 - acc: 0.6522 - LRFinder: val_loss: 3.8682 - lr = 0.00695128 \n",
            "247/390 [==================>...........] - ETA: 50s - loss: 1.4220 - acc: 0.6521 - LRFinder: val_loss: 3.7547 - lr = 0.00697231 \n",
            "248/390 [==================>...........] - ETA: 50s - loss: 1.4220 - acc: 0.6521 - LRFinder: val_loss: 4.0088 - lr = 0.00699333 \n",
            "249/390 [==================>...........] - ETA: 49s - loss: 1.4220 - acc: 0.6521 - LRFinder: val_loss: 3.8807 - lr = 0.00701436 \n",
            "250/390 [==================>...........] - ETA: 49s - loss: 1.4228 - acc: 0.6517 - LRFinder: val_loss: 4.0280 - lr = 0.00703538 \n",
            "251/390 [==================>...........] - ETA: 49s - loss: 1.4232 - acc: 0.6517 - LRFinder: val_loss: 3.9717 - lr = 0.00705641 \n",
            "252/390 [==================>...........] - ETA: 48s - loss: 1.4229 - acc: 0.6518 - LRFinder: val_loss: 4.0183 - lr = 0.00707744 \n",
            "253/390 [==================>...........] - ETA: 48s - loss: 1.4228 - acc: 0.6520 - LRFinder: val_loss: 3.8413 - lr = 0.00709846 \n",
            "254/390 [==================>...........] - ETA: 48s - loss: 1.4230 - acc: 0.6518 - LRFinder: val_loss: 3.9494 - lr = 0.00711949 \n",
            "255/390 [==================>...........] - ETA: 47s - loss: 1.4234 - acc: 0.6515 - LRFinder: val_loss: 3.7467 - lr = 0.00714051 \n",
            "256/390 [==================>...........] - ETA: 47s - loss: 1.4226 - acc: 0.6519 - LRFinder: val_loss: 3.8067 - lr = 0.00716154 \n",
            "257/390 [==================>...........] - ETA: 46s - loss: 1.4231 - acc: 0.6518 - LRFinder: val_loss: 3.5972 - lr = 0.00718256 \n",
            "258/390 [==================>...........] - ETA: 46s - loss: 1.4234 - acc: 0.6516 - LRFinder: val_loss: 3.8235 - lr = 0.00720359 \n",
            "259/390 [==================>...........] - ETA: 46s - loss: 1.4233 - acc: 0.6516 - LRFinder: val_loss: 3.6391 - lr = 0.00722462 \n",
            "260/390 [===================>..........] - ETA: 45s - loss: 1.4232 - acc: 0.6516 - LRFinder: val_loss: 3.8342 - lr = 0.00724564 \n",
            "261/390 [===================>..........] - ETA: 45s - loss: 1.4233 - acc: 0.6517 - LRFinder: val_loss: 3.8823 - lr = 0.00726667 \n",
            "262/390 [===================>..........] - ETA: 45s - loss: 1.4236 - acc: 0.6515 - LRFinder: val_loss: 3.8199 - lr = 0.00728769 \n",
            "263/390 [===================>..........] - ETA: 44s - loss: 1.4239 - acc: 0.6513 - LRFinder: val_loss: 3.7844 - lr = 0.00730872 \n",
            "264/390 [===================>..........] - ETA: 44s - loss: 1.4243 - acc: 0.6511 - LRFinder: val_loss: 3.9044 - lr = 0.00732974 \n",
            "265/390 [===================>..........] - ETA: 44s - loss: 1.4240 - acc: 0.6511 - LRFinder: val_loss: 3.9567 - lr = 0.00735077 \n",
            "266/390 [===================>..........] - ETA: 43s - loss: 1.4238 - acc: 0.6511 - LRFinder: val_loss: 3.8916 - lr = 0.00737179 \n",
            "267/390 [===================>..........] - ETA: 43s - loss: 1.4237 - acc: 0.6512 - LRFinder: val_loss: 3.8172 - lr = 0.00739282 \n",
            "268/390 [===================>..........] - ETA: 43s - loss: 1.4232 - acc: 0.6513 - LRFinder: val_loss: 3.8115 - lr = 0.00741385 \n",
            "269/390 [===================>..........] - ETA: 42s - loss: 1.4229 - acc: 0.6513 - LRFinder: val_loss: 3.7274 - lr = 0.00743487 \n",
            "270/390 [===================>..........] - ETA: 42s - loss: 1.4225 - acc: 0.6514 - LRFinder: val_loss: 4.1068 - lr = 0.00745590 \n",
            "271/390 [===================>..........] - ETA: 41s - loss: 1.4222 - acc: 0.6516 - LRFinder: val_loss: 3.8643 - lr = 0.00747692 \n",
            "272/390 [===================>..........] - ETA: 41s - loss: 1.4227 - acc: 0.6513 - LRFinder: val_loss: 3.9105 - lr = 0.00749795 \n",
            "273/390 [====================>.........] - ETA: 41s - loss: 1.4225 - acc: 0.6513 - LRFinder: val_loss: 3.7724 - lr = 0.00751897 \n",
            "274/390 [====================>.........] - ETA: 40s - loss: 1.4226 - acc: 0.6512 - LRFinder: val_loss: 3.8669 - lr = 0.00754000 \n",
            "275/390 [====================>.........] - ETA: 40s - loss: 1.4225 - acc: 0.6512 - LRFinder: val_loss: 3.9129 - lr = 0.00756103 \n",
            "276/390 [====================>.........] - ETA: 40s - loss: 1.4225 - acc: 0.6513 - LRFinder: val_loss: 3.8750 - lr = 0.00758205 \n",
            "277/390 [====================>.........] - ETA: 39s - loss: 1.4225 - acc: 0.6514 - LRFinder: val_loss: 3.8878 - lr = 0.00760308 \n",
            "278/390 [====================>.........] - ETA: 39s - loss: 1.4226 - acc: 0.6514 - LRFinder: val_loss: 3.7973 - lr = 0.00762410 \n",
            "279/390 [====================>.........] - ETA: 39s - loss: 1.4230 - acc: 0.6516 - LRFinder: val_loss: 3.6915 - lr = 0.00764513 \n",
            "280/390 [====================>.........] - ETA: 38s - loss: 1.4236 - acc: 0.6513 - LRFinder: val_loss: 3.7805 - lr = 0.00766615 \n",
            "281/390 [====================>.........] - ETA: 38s - loss: 1.4232 - acc: 0.6515 - LRFinder: val_loss: 3.7152 - lr = 0.00768718 \n",
            "282/390 [====================>.........] - ETA: 38s - loss: 1.4230 - acc: 0.6515 - LRFinder: val_loss: 3.8252 - lr = 0.00770821 \n",
            "283/390 [====================>.........] - ETA: 37s - loss: 1.4232 - acc: 0.6513 - LRFinder: val_loss: 3.7970 - lr = 0.00772923 \n",
            "284/390 [====================>.........] - ETA: 37s - loss: 1.4228 - acc: 0.6515 - LRFinder: val_loss: 4.0353 - lr = 0.00775026 \n",
            "285/390 [====================>.........] - ETA: 36s - loss: 1.4230 - acc: 0.6515 - LRFinder: val_loss: 4.0023 - lr = 0.00777128 \n",
            "286/390 [=====================>........] - ETA: 36s - loss: 1.4225 - acc: 0.6518 - LRFinder: val_loss: 4.0039 - lr = 0.00779231 \n",
            "287/390 [=====================>........] - ETA: 36s - loss: 1.4228 - acc: 0.6517 - LRFinder: val_loss: 3.9296 - lr = 0.00781333 \n",
            "288/390 [=====================>........] - ETA: 35s - loss: 1.4228 - acc: 0.6517 - LRFinder: val_loss: 4.0222 - lr = 0.00783436 \n",
            "289/390 [=====================>........] - ETA: 35s - loss: 1.4232 - acc: 0.6517 - LRFinder: val_loss: 4.1177 - lr = 0.00785538 \n",
            "290/390 [=====================>........] - ETA: 35s - loss: 1.4230 - acc: 0.6518 - LRFinder: val_loss: 4.0277 - lr = 0.00787641 \n",
            "291/390 [=====================>........] - ETA: 34s - loss: 1.4225 - acc: 0.6520 - LRFinder: val_loss: 4.2032 - lr = 0.00789744 \n",
            "292/390 [=====================>........] - ETA: 34s - loss: 1.4228 - acc: 0.6519 - LRFinder: val_loss: 4.0757 - lr = 0.00791846 \n",
            "293/390 [=====================>........] - ETA: 34s - loss: 1.4233 - acc: 0.6516 - LRFinder: val_loss: 4.0903 - lr = 0.00793949 \n",
            "294/390 [=====================>........] - ETA: 33s - loss: 1.4232 - acc: 0.6516 - LRFinder: val_loss: 4.0739 - lr = 0.00796051 \n",
            "295/390 [=====================>........] - ETA: 33s - loss: 1.4228 - acc: 0.6519 - LRFinder: val_loss: 4.0651 - lr = 0.00798154 \n",
            "296/390 [=====================>........] - ETA: 33s - loss: 1.4226 - acc: 0.6519 - LRFinder: val_loss: 4.0838 - lr = 0.00800256 \n",
            "297/390 [=====================>........] - ETA: 32s - loss: 1.4223 - acc: 0.6520 - LRFinder: val_loss: 3.9588 - lr = 0.00802359 \n",
            "298/390 [=====================>........] - ETA: 32s - loss: 1.4222 - acc: 0.6520 - LRFinder: val_loss: 4.1077 - lr = 0.00804462 \n",
            "299/390 [======================>.......] - ETA: 31s - loss: 1.4218 - acc: 0.6521 - LRFinder: val_loss: 3.9652 - lr = 0.00806564 \n",
            "300/390 [======================>.......] - ETA: 31s - loss: 1.4214 - acc: 0.6522 - LRFinder: val_loss: 3.9614 - lr = 0.00808667 \n",
            "301/390 [======================>.......] - ETA: 31s - loss: 1.4211 - acc: 0.6524 - LRFinder: val_loss: 4.1241 - lr = 0.00810769 \n",
            "302/390 [======================>.......] - ETA: 30s - loss: 1.4209 - acc: 0.6522 - LRFinder: val_loss: 4.0001 - lr = 0.00812872 \n",
            "303/390 [======================>.......] - ETA: 30s - loss: 1.4206 - acc: 0.6523 - LRFinder: val_loss: 3.9580 - lr = 0.00814974 \n",
            "304/390 [======================>.......] - ETA: 30s - loss: 1.4207 - acc: 0.6522 - LRFinder: val_loss: 4.1041 - lr = 0.00817077 \n",
            "305/390 [======================>.......] - ETA: 29s - loss: 1.4206 - acc: 0.6524 - LRFinder: val_loss: 3.8992 - lr = 0.00819179 \n",
            "306/390 [======================>.......] - ETA: 29s - loss: 1.4202 - acc: 0.6526 - LRFinder: val_loss: 3.9440 - lr = 0.00821282 \n",
            "307/390 [======================>.......] - ETA: 29s - loss: 1.4205 - acc: 0.6524 - LRFinder: val_loss: 3.8991 - lr = 0.00823385 \n",
            "308/390 [======================>.......] - ETA: 28s - loss: 1.4206 - acc: 0.6524 - LRFinder: val_loss: 3.8738 - lr = 0.00825487 \n",
            "309/390 [======================>.......] - ETA: 28s - loss: 1.4209 - acc: 0.6524 - LRFinder: val_loss: 3.9676 - lr = 0.00827590 \n",
            "310/390 [======================>.......] - ETA: 28s - loss: 1.4213 - acc: 0.6523 - LRFinder: val_loss: 3.8546 - lr = 0.00829692 \n",
            "311/390 [======================>.......] - ETA: 27s - loss: 1.4211 - acc: 0.6523 - LRFinder: val_loss: 3.8888 - lr = 0.00831795 \n",
            "312/390 [=======================>......] - ETA: 27s - loss: 1.4213 - acc: 0.6522 - LRFinder: val_loss: 3.6682 - lr = 0.00833897 \n",
            "313/390 [=======================>......] - ETA: 27s - loss: 1.4213 - acc: 0.6522 - LRFinder: val_loss: 3.7595 - lr = 0.00836000 \n",
            "314/390 [=======================>......] - ETA: 26s - loss: 1.4216 - acc: 0.6520 - LRFinder: val_loss: 3.9437 - lr = 0.00838103 \n",
            "315/390 [=======================>......] - ETA: 26s - loss: 1.4214 - acc: 0.6521 - LRFinder: val_loss: 3.8605 - lr = 0.00840205 \n",
            "316/390 [=======================>......] - ETA: 25s - loss: 1.4213 - acc: 0.6521 - LRFinder: val_loss: 3.6576 - lr = 0.00842308 \n",
            "317/390 [=======================>......] - ETA: 25s - loss: 1.4220 - acc: 0.6518 - LRFinder: val_loss: 3.8704 - lr = 0.00844410 \n",
            "318/390 [=======================>......] - ETA: 25s - loss: 1.4225 - acc: 0.6517 - LRFinder: val_loss: 3.9129 - lr = 0.00846513 \n",
            "319/390 [=======================>......] - ETA: 24s - loss: 1.4226 - acc: 0.6516 - LRFinder: val_loss: 3.7142 - lr = 0.00848615 \n",
            "320/390 [=======================>......] - ETA: 24s - loss: 1.4228 - acc: 0.6515 - LRFinder: val_loss: 3.8522 - lr = 0.00850718 \n",
            "321/390 [=======================>......] - ETA: 24s - loss: 1.4224 - acc: 0.6517 - LRFinder: val_loss: 3.7946 - lr = 0.00852821 \n",
            "322/390 [=======================>......] - ETA: 23s - loss: 1.4220 - acc: 0.6520 - LRFinder: val_loss: 3.8727 - lr = 0.00854923 \n",
            "323/390 [=======================>......] - ETA: 23s - loss: 1.4217 - acc: 0.6521 - LRFinder: val_loss: 3.9151 - lr = 0.00857026 \n",
            "324/390 [=======================>......] - ETA: 23s - loss: 1.4220 - acc: 0.6520 - LRFinder: val_loss: 3.8527 - lr = 0.00859128 \n",
            "325/390 [========================>.....] - ETA: 22s - loss: 1.4222 - acc: 0.6521 - LRFinder: val_loss: 3.6853 - lr = 0.00861231 \n",
            "326/390 [========================>.....] - ETA: 22s - loss: 1.4217 - acc: 0.6522 - LRFinder: val_loss: 3.8151 - lr = 0.00863333 \n",
            "327/390 [========================>.....] - ETA: 22s - loss: 1.4215 - acc: 0.6522 - LRFinder: val_loss: 3.7547 - lr = 0.00865436 \n",
            "328/390 [========================>.....] - ETA: 21s - loss: 1.4214 - acc: 0.6523 - LRFinder: val_loss: 3.7934 - lr = 0.00867538 \n",
            "329/390 [========================>.....] - ETA: 21s - loss: 1.4212 - acc: 0.6522 - LRFinder: val_loss: 3.6973 - lr = 0.00869641 \n",
            "330/390 [========================>.....] - ETA: 21s - loss: 1.4210 - acc: 0.6522 - LRFinder: val_loss: 3.9564 - lr = 0.00871744 \n",
            "331/390 [========================>.....] - ETA: 20s - loss: 1.4214 - acc: 0.6520 - LRFinder: val_loss: 3.8948 - lr = 0.00873846 \n",
            "332/390 [========================>.....] - ETA: 20s - loss: 1.4211 - acc: 0.6520 - LRFinder: val_loss: 3.7276 - lr = 0.00875949 \n",
            "333/390 [========================>.....] - ETA: 19s - loss: 1.4211 - acc: 0.6519 - LRFinder: val_loss: 3.8462 - lr = 0.00878051 \n",
            "334/390 [========================>.....] - ETA: 19s - loss: 1.4211 - acc: 0.6519 - LRFinder: val_loss: 3.9504 - lr = 0.00880154 \n",
            "335/390 [========================>.....] - ETA: 19s - loss: 1.4211 - acc: 0.6519 - LRFinder: val_loss: 4.1009 - lr = 0.00882256 \n",
            "336/390 [========================>.....] - ETA: 18s - loss: 1.4212 - acc: 0.6519 - LRFinder: val_loss: 3.9966 - lr = 0.00884359 \n",
            "337/390 [========================>.....] - ETA: 18s - loss: 1.4209 - acc: 0.6521 - LRFinder: val_loss: 4.0630 - lr = 0.00886462 \n",
            "338/390 [=========================>....] - ETA: 18s - loss: 1.4209 - acc: 0.6521 - LRFinder: val_loss: 4.2486 - lr = 0.00888564 \n",
            "339/390 [=========================>....] - ETA: 17s - loss: 1.4207 - acc: 0.6522 - LRFinder: val_loss: 4.2049 - lr = 0.00890667 \n",
            "340/390 [=========================>....] - ETA: 17s - loss: 1.4208 - acc: 0.6521 - LRFinder: val_loss: 4.1147 - lr = 0.00892769 \n",
            "341/390 [=========================>....] - ETA: 17s - loss: 1.4206 - acc: 0.6522 - LRFinder: val_loss: 4.2061 - lr = 0.00894872 \n",
            "342/390 [=========================>....] - ETA: 16s - loss: 1.4208 - acc: 0.6520 - LRFinder: val_loss: 4.2012 - lr = 0.00896974 \n",
            "343/390 [=========================>....] - ETA: 16s - loss: 1.4209 - acc: 0.6521 - LRFinder: val_loss: 4.1614 - lr = 0.00899077 \n",
            "344/390 [=========================>....] - ETA: 16s - loss: 1.4208 - acc: 0.6521 - LRFinder: val_loss: 4.1807 - lr = 0.00901179 \n",
            "345/390 [=========================>....] - ETA: 15s - loss: 1.4209 - acc: 0.6522 - LRFinder: val_loss: 4.0142 - lr = 0.00903282 \n",
            "346/390 [=========================>....] - ETA: 15s - loss: 1.4211 - acc: 0.6521 - LRFinder: val_loss: 3.8510 - lr = 0.00905385 \n",
            "347/390 [=========================>....] - ETA: 15s - loss: 1.4213 - acc: 0.6519 - LRFinder: val_loss: 4.1121 - lr = 0.00907487 \n",
            "348/390 [=========================>....] - ETA: 14s - loss: 1.4213 - acc: 0.6520 - LRFinder: val_loss: 4.1018 - lr = 0.00909590 \n",
            "349/390 [=========================>....] - ETA: 14s - loss: 1.4211 - acc: 0.6521 - LRFinder: val_loss: 3.9814 - lr = 0.00911692 \n",
            "350/390 [=========================>....] - ETA: 14s - loss: 1.4213 - acc: 0.6521 - LRFinder: val_loss: 4.0953 - lr = 0.00913795 \n",
            "351/390 [==========================>...] - ETA: 13s - loss: 1.4213 - acc: 0.6521 - LRFinder: val_loss: 4.0342 - lr = 0.00915897 \n",
            "352/390 [==========================>...] - ETA: 13s - loss: 1.4215 - acc: 0.6521 - LRFinder: val_loss: 4.0856 - lr = 0.00918000 \n",
            "353/390 [==========================>...] - ETA: 12s - loss: 1.4211 - acc: 0.6522 - LRFinder: val_loss: 4.0741 - lr = 0.00920103 \n",
            "354/390 [==========================>...] - ETA: 12s - loss: 1.4212 - acc: 0.6522 - LRFinder: val_loss: 4.3226 - lr = 0.00922205 \n",
            "355/390 [==========================>...] - ETA: 12s - loss: 1.4210 - acc: 0.6524 - LRFinder: val_loss: 4.0550 - lr = 0.00924308 \n",
            "356/390 [==========================>...] - ETA: 11s - loss: 1.4210 - acc: 0.6523 - LRFinder: val_loss: 4.1266 - lr = 0.00926410 \n",
            "357/390 [==========================>...] - ETA: 11s - loss: 1.4214 - acc: 0.6522 - LRFinder: val_loss: 4.3041 - lr = 0.00928513 \n",
            "358/390 [==========================>...] - ETA: 11s - loss: 1.4216 - acc: 0.6520 - LRFinder: val_loss: 4.2936 - lr = 0.00930615 \n",
            "359/390 [==========================>...] - ETA: 10s - loss: 1.4213 - acc: 0.6520 - LRFinder: val_loss: 4.2876 - lr = 0.00932718 \n",
            "360/390 [==========================>...] - ETA: 10s - loss: 1.4209 - acc: 0.6521 - LRFinder: val_loss: 4.2654 - lr = 0.00934821 \n",
            "361/390 [==========================>...] - ETA: 10s - loss: 1.4209 - acc: 0.6520 - LRFinder: val_loss: 4.2218 - lr = 0.00936923 \n",
            "362/390 [==========================>...] - ETA: 9s - loss: 1.4208 - acc: 0.6520  - LRFinder: val_loss: 4.1673 - lr = 0.00939026 \n",
            "363/390 [==========================>...] - ETA: 9s - loss: 1.4209 - acc: 0.6520 - LRFinder: val_loss: 4.0367 - lr = 0.00941128 \n",
            "364/390 [===========================>..] - ETA: 9s - loss: 1.4209 - acc: 0.6521 - LRFinder: val_loss: 3.9446 - lr = 0.00943231 \n",
            "365/390 [===========================>..] - ETA: 8s - loss: 1.4211 - acc: 0.6521 - LRFinder: val_loss: 4.1046 - lr = 0.00945333 \n",
            "366/390 [===========================>..] - ETA: 8s - loss: 1.4210 - acc: 0.6521 - LRFinder: val_loss: 3.9649 - lr = 0.00947436 \n",
            "367/390 [===========================>..] - ETA: 8s - loss: 1.4206 - acc: 0.6522 - LRFinder: val_loss: 4.1830 - lr = 0.00949538 \n",
            "368/390 [===========================>..] - ETA: 7s - loss: 1.4202 - acc: 0.6522 - LRFinder: val_loss: 4.0031 - lr = 0.00951641 \n",
            "369/390 [===========================>..] - ETA: 7s - loss: 1.4202 - acc: 0.6521 - LRFinder: val_loss: 3.8066 - lr = 0.00953744 \n",
            "370/390 [===========================>..] - ETA: 6s - loss: 1.4199 - acc: 0.6521 - LRFinder: val_loss: 3.8985 - lr = 0.00955846 \n",
            "371/390 [===========================>..] - ETA: 6s - loss: 1.4199 - acc: 0.6521 - LRFinder: val_loss: 3.9455 - lr = 0.00957949 \n",
            "372/390 [===========================>..] - ETA: 6s - loss: 1.4193 - acc: 0.6522 - LRFinder: val_loss: 3.8065 - lr = 0.00960051 \n",
            "373/390 [===========================>..] - ETA: 5s - loss: 1.4192 - acc: 0.6522 - LRFinder: val_loss: 3.8743 - lr = 0.00962154 \n",
            "374/390 [===========================>..] - ETA: 5s - loss: 1.4190 - acc: 0.6522 - LRFinder: val_loss: 3.9928 - lr = 0.00964256 \n",
            "375/390 [===========================>..] - ETA: 5s - loss: 1.4187 - acc: 0.6522 - LRFinder: val_loss: 4.0501 - lr = 0.00966359 \n",
            "376/390 [===========================>..] - ETA: 4s - loss: 1.4187 - acc: 0.6522 - LRFinder: val_loss: 4.1033 - lr = 0.00968462 \n",
            "377/390 [============================>.] - ETA: 4s - loss: 1.4190 - acc: 0.6522 - LRFinder: val_loss: 4.0573 - lr = 0.00970564 \n",
            "378/390 [============================>.] - ETA: 4s - loss: 1.4185 - acc: 0.6524 - LRFinder: val_loss: 4.0200 - lr = 0.00972667 \n",
            "379/390 [============================>.] - ETA: 3s - loss: 1.4185 - acc: 0.6525 - LRFinder: val_loss: 3.9523 - lr = 0.00974769 \n",
            "380/390 [============================>.] - ETA: 3s - loss: 1.4183 - acc: 0.6524 - LRFinder: val_loss: 3.9798 - lr = 0.00976872 \n",
            "381/390 [============================>.] - ETA: 3s - loss: 1.4181 - acc: 0.6526 - LRFinder: val_loss: 4.1051 - lr = 0.00978974 \n",
            "382/390 [============================>.] - ETA: 2s - loss: 1.4178 - acc: 0.6527 - LRFinder: val_loss: 4.0183 - lr = 0.00981077 \n",
            "383/390 [============================>.] - ETA: 2s - loss: 1.4178 - acc: 0.6526 - LRFinder: val_loss: 4.3428 - lr = 0.00983179 \n",
            "384/390 [============================>.] - ETA: 2s - loss: 1.4176 - acc: 0.6527 - LRFinder: val_loss: 4.1981 - lr = 0.00985282 \n",
            "385/390 [============================>.] - ETA: 1s - loss: 1.4177 - acc: 0.6526 - LRFinder: val_loss: 4.2332 - lr = 0.00987385 \n",
            "386/390 [============================>.] - ETA: 1s - loss: 1.4180 - acc: 0.6525 - LRFinder: val_loss: 3.9107 - lr = 0.00989487 \n",
            "387/390 [============================>.] - ETA: 1s - loss: 1.4181 - acc: 0.6524 - LRFinder: val_loss: 3.9463 - lr = 0.00991590 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 1.4179 - acc: 0.6524 - LRFinder: val_loss: 3.9708 - lr = 0.00993692 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.4181 - acc: 0.6524 - LRFinder: val_loss: 4.0927 - lr = 0.00995795 \n",
            " - LRFinder: val_loss: 4.0950 - lr = 0.00997897 \n",
            "390/390 [==============================] - 140s 358ms/step - loss: 1.4184 - acc: 0.6524 - val_loss: 4.1710 - val_acc: 0.2003\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {weights/weight_decay/weight_decay-3e-07/}\n",
            "Epoch 1/1\n",
            "  1/390 [..............................] - ETA: 16:23 - loss: 1.4335 - acc: 0.6406 - LRFinder: val_loss: 4.2971 - lr = 0.00180000 \n",
            "  2/390 [..............................] - ETA: 12:01 - loss: 1.4292 - acc: 0.6523 - LRFinder: val_loss: 4.1906 - lr = 0.00182103 \n",
            "  3/390 [..............................] - ETA: 8:45 - loss: 1.4240 - acc: 0.6536  - LRFinder: val_loss: 4.1329 - lr = 0.00184205 \n",
            "  4/390 [..............................] - ETA: 7:06 - loss: 1.4209 - acc: 0.6582 - LRFinder: val_loss: 4.2146 - lr = 0.00186308 \n",
            "  5/390 [..............................] - ETA: 6:06 - loss: 1.4011 - acc: 0.6672 - LRFinder: val_loss: 3.9181 - lr = 0.00188410 \n",
            "  6/390 [..............................] - ETA: 5:26 - loss: 1.3922 - acc: 0.6706 - LRFinder: val_loss: 4.1771 - lr = 0.00190513 \n",
            "  7/390 [..............................] - ETA: 4:57 - loss: 1.3976 - acc: 0.6674 - LRFinder: val_loss: 3.9085 - lr = 0.00192615 \n",
            "  8/390 [..............................] - ETA: 4:36 - loss: 1.4005 - acc: 0.6670 - LRFinder: val_loss: 3.8317 - lr = 0.00194718 \n",
            "  9/390 [..............................] - ETA: 4:19 - loss: 1.4004 - acc: 0.6658 - LRFinder: val_loss: 3.9471 - lr = 0.00196821 \n",
            " 10/390 [..............................] - ETA: 4:06 - loss: 1.4119 - acc: 0.6562 - LRFinder: val_loss: 3.9221 - lr = 0.00198923 \n",
            " 11/390 [..............................] - ETA: 3:55 - loss: 1.4054 - acc: 0.6548 - LRFinder: val_loss: 3.8799 - lr = 0.00201026 \n",
            " 12/390 [..............................] - ETA: 3:45 - loss: 1.4035 - acc: 0.6556 - LRFinder: val_loss: 3.9837 - lr = 0.00203128 \n",
            " 13/390 [>.............................] - ETA: 3:37 - loss: 1.4018 - acc: 0.6575 - LRFinder: val_loss: 3.8283 - lr = 0.00205231 \n",
            " 14/390 [>.............................] - ETA: 3:31 - loss: 1.4072 - acc: 0.6546 - LRFinder: val_loss: 3.8308 - lr = 0.00207333 \n",
            " 15/390 [>.............................] - ETA: 3:25 - loss: 1.4036 - acc: 0.6542 - LRFinder: val_loss: 4.0114 - lr = 0.00209436 \n",
            " 16/390 [>.............................] - ETA: 3:19 - loss: 1.4002 - acc: 0.6558 - LRFinder: val_loss: 3.7950 - lr = 0.00211538 \n",
            " 17/390 [>.............................] - ETA: 3:15 - loss: 1.3930 - acc: 0.6581 - LRFinder: val_loss: 3.8626 - lr = 0.00213641 \n",
            " 18/390 [>.............................] - ETA: 3:11 - loss: 1.3993 - acc: 0.6541 - LRFinder: val_loss: 3.9294 - lr = 0.00215744 \n",
            " 19/390 [>.............................] - ETA: 3:07 - loss: 1.4011 - acc: 0.6542 - LRFinder: val_loss: 3.6738 - lr = 0.00217846 \n",
            " 20/390 [>.............................] - ETA: 3:03 - loss: 1.4103 - acc: 0.6512 - LRFinder: val_loss: 3.8777 - lr = 0.00219949 \n",
            " 21/390 [>.............................] - ETA: 3:00 - loss: 1.4116 - acc: 0.6496 - LRFinder: val_loss: 4.0228 - lr = 0.00222051 \n",
            " 22/390 [>.............................] - ETA: 2:57 - loss: 1.4104 - acc: 0.6527 - LRFinder: val_loss: 3.9134 - lr = 0.00224154 \n",
            " 23/390 [>.............................] - ETA: 2:55 - loss: 1.4104 - acc: 0.6518 - LRFinder: val_loss: 3.9911 - lr = 0.00226256 \n",
            " 24/390 [>.............................] - ETA: 2:52 - loss: 1.4112 - acc: 0.6520 - LRFinder: val_loss: 4.0520 - lr = 0.00228359 \n",
            " 25/390 [>.............................] - ETA: 2:50 - loss: 1.4162 - acc: 0.6503 - LRFinder: val_loss: 3.9668 - lr = 0.00230462 \n",
            " 26/390 [=>............................] - ETA: 2:48 - loss: 1.4182 - acc: 0.6490 - LRFinder: val_loss: 3.8646 - lr = 0.00232564 \n",
            " 27/390 [=>............................] - ETA: 2:46 - loss: 1.4172 - acc: 0.6502 - LRFinder: val_loss: 3.9450 - lr = 0.00234667 \n",
            " 28/390 [=>............................] - ETA: 2:44 - loss: 1.4178 - acc: 0.6504 - LRFinder: val_loss: 3.9111 - lr = 0.00236769 \n",
            " 29/390 [=>............................] - ETA: 2:42 - loss: 1.4186 - acc: 0.6498 - LRFinder: val_loss: 3.9947 - lr = 0.00238872 \n",
            " 30/390 [=>............................] - ETA: 2:40 - loss: 1.4120 - acc: 0.6518 - LRFinder: val_loss: 3.8961 - lr = 0.00240974 \n",
            " 31/390 [=>............................] - ETA: 2:39 - loss: 1.4127 - acc: 0.6522 - LRFinder: val_loss: 4.0912 - lr = 0.00243077 \n",
            " 32/390 [=>............................] - ETA: 2:37 - loss: 1.4117 - acc: 0.6536 - LRFinder: val_loss: 4.0697 - lr = 0.00245179 \n",
            " 33/390 [=>............................] - ETA: 2:36 - loss: 1.4118 - acc: 0.6520 - LRFinder: val_loss: 3.9347 - lr = 0.00247282 \n",
            " 34/390 [=>............................] - ETA: 2:34 - loss: 1.4147 - acc: 0.6514 - LRFinder: val_loss: 3.9034 - lr = 0.00249385 \n",
            " 35/390 [=>............................] - ETA: 2:33 - loss: 1.4128 - acc: 0.6511 - LRFinder: val_loss: 4.2638 - lr = 0.00251487 \n",
            " 36/390 [=>............................] - ETA: 2:32 - loss: 1.4124 - acc: 0.6502 - LRFinder: val_loss: 3.9634 - lr = 0.00253590 \n",
            " 37/390 [=>............................] - ETA: 2:31 - loss: 1.4108 - acc: 0.6505 - LRFinder: val_loss: 4.0057 - lr = 0.00255692 \n",
            " 38/390 [=>............................] - ETA: 2:29 - loss: 1.4094 - acc: 0.6515 - LRFinder: val_loss: 3.9937 - lr = 0.00257795 \n",
            " 39/390 [==>...........................] - ETA: 2:28 - loss: 1.4070 - acc: 0.6520 - LRFinder: val_loss: 4.2211 - lr = 0.00259897 \n",
            " 40/390 [==>...........................] - ETA: 2:27 - loss: 1.4068 - acc: 0.6523 - LRFinder: val_loss: 3.8489 - lr = 0.00262000 \n",
            " 41/390 [==>...........................] - ETA: 2:26 - loss: 1.4076 - acc: 0.6521 - LRFinder: val_loss: 3.9972 - lr = 0.00264103 \n",
            " 42/390 [==>...........................] - ETA: 2:25 - loss: 1.4070 - acc: 0.6512 - LRFinder: val_loss: 4.0385 - lr = 0.00266205 \n",
            " 43/390 [==>...........................] - ETA: 2:24 - loss: 1.4073 - acc: 0.6517 - LRFinder: val_loss: 3.9748 - lr = 0.00268308 \n",
            " 44/390 [==>...........................] - ETA: 2:23 - loss: 1.4041 - acc: 0.6534 - LRFinder: val_loss: 4.1333 - lr = 0.00270410 \n",
            " 45/390 [==>...........................] - ETA: 2:22 - loss: 1.4045 - acc: 0.6540 - LRFinder: val_loss: 3.7616 - lr = 0.00272513 \n",
            " 46/390 [==>...........................] - ETA: 2:21 - loss: 1.4025 - acc: 0.6551 - LRFinder: val_loss: 3.8796 - lr = 0.00274615 \n",
            " 47/390 [==>...........................] - ETA: 2:20 - loss: 1.4050 - acc: 0.6544 - LRFinder: val_loss: 4.1743 - lr = 0.00276718 \n",
            " 48/390 [==>...........................] - ETA: 2:19 - loss: 1.4032 - acc: 0.6540 - LRFinder: val_loss: 3.8958 - lr = 0.00278821 \n",
            " 49/390 [==>...........................] - ETA: 2:18 - loss: 1.4011 - acc: 0.6550 - LRFinder: val_loss: 3.9435 - lr = 0.00280923 \n",
            " 50/390 [==>...........................] - ETA: 2:18 - loss: 1.3997 - acc: 0.6555 - LRFinder: val_loss: 3.9526 - lr = 0.00283026 \n",
            " 51/390 [==>...........................] - ETA: 2:17 - loss: 1.3996 - acc: 0.6547 - LRFinder: val_loss: 3.8326 - lr = 0.00285128 \n",
            " 52/390 [===>..........................] - ETA: 2:16 - loss: 1.3989 - acc: 0.6546 - LRFinder: val_loss: 3.7948 - lr = 0.00287231 \n",
            " 53/390 [===>..........................] - ETA: 2:15 - loss: 1.3983 - acc: 0.6548 - LRFinder: val_loss: 3.6686 - lr = 0.00289333 \n",
            " 54/390 [===>..........................] - ETA: 2:15 - loss: 1.3964 - acc: 0.6549 - LRFinder: val_loss: 4.0057 - lr = 0.00291436 \n",
            " 55/390 [===>..........................] - ETA: 2:14 - loss: 1.3939 - acc: 0.6560 - LRFinder: val_loss: 4.0674 - lr = 0.00293538 \n",
            " 56/390 [===>..........................] - ETA: 2:13 - loss: 1.3954 - acc: 0.6550 - LRFinder: val_loss: 3.9662 - lr = 0.00295641 \n",
            " 57/390 [===>..........................] - ETA: 2:12 - loss: 1.3934 - acc: 0.6546 - LRFinder: val_loss: 3.9809 - lr = 0.00297744 \n",
            " 58/390 [===>..........................] - ETA: 2:12 - loss: 1.3904 - acc: 0.6553 - LRFinder: val_loss: 3.9654 - lr = 0.00299846 \n",
            " 59/390 [===>..........................] - ETA: 2:11 - loss: 1.3904 - acc: 0.6556 - LRFinder: val_loss: 4.0148 - lr = 0.00301949 \n",
            " 60/390 [===>..........................] - ETA: 2:10 - loss: 1.3869 - acc: 0.6573 - LRFinder: val_loss: 4.0702 - lr = 0.00304051 \n",
            " 61/390 [===>..........................] - ETA: 2:10 - loss: 1.3846 - acc: 0.6583 - LRFinder: val_loss: 3.8166 - lr = 0.00306154 \n",
            " 62/390 [===>..........................] - ETA: 2:09 - loss: 1.3855 - acc: 0.6581 - LRFinder: val_loss: 4.0192 - lr = 0.00308256 \n",
            " 63/390 [===>..........................] - ETA: 2:08 - loss: 1.3851 - acc: 0.6585 - LRFinder: val_loss: 3.8364 - lr = 0.00310359 \n",
            " 64/390 [===>..........................] - ETA: 2:08 - loss: 1.3862 - acc: 0.6588 - LRFinder: val_loss: 3.9567 - lr = 0.00312462 \n",
            " 65/390 [====>.........................] - ETA: 2:07 - loss: 1.3862 - acc: 0.6590 - LRFinder: val_loss: 4.0947 - lr = 0.00314564 \n",
            " 66/390 [====>.........................] - ETA: 2:06 - loss: 1.3855 - acc: 0.6592 - LRFinder: val_loss: 3.8829 - lr = 0.00316667 \n",
            " 67/390 [====>.........................] - ETA: 2:06 - loss: 1.3852 - acc: 0.6595 - LRFinder: val_loss: 3.8735 - lr = 0.00318769 \n",
            " 68/390 [====>.........................] - ETA: 2:05 - loss: 1.3851 - acc: 0.6594 - LRFinder: val_loss: 3.7793 - lr = 0.00320872 \n",
            " 69/390 [====>.........................] - ETA: 2:05 - loss: 1.3877 - acc: 0.6584 - LRFinder: val_loss: 3.7059 - lr = 0.00322974 \n",
            " 70/390 [====>.........................] - ETA: 2:04 - loss: 1.3840 - acc: 0.6596 - LRFinder: val_loss: 3.7816 - lr = 0.00325077 \n",
            " 71/390 [====>.........................] - ETA: 2:03 - loss: 1.3824 - acc: 0.6607 - LRFinder: val_loss: 3.6780 - lr = 0.00327179 \n",
            " 72/390 [====>.........................] - ETA: 2:03 - loss: 1.3828 - acc: 0.6609 - LRFinder: val_loss: 3.7518 - lr = 0.00329282 \n",
            " 73/390 [====>.........................] - ETA: 2:02 - loss: 1.3812 - acc: 0.6617 - LRFinder: val_loss: 3.8328 - lr = 0.00331385 \n",
            " 74/390 [====>.........................] - ETA: 2:02 - loss: 1.3835 - acc: 0.6604 - LRFinder: val_loss: 3.8854 - lr = 0.00333487 \n",
            " 75/390 [====>.........................] - ETA: 2:01 - loss: 1.3841 - acc: 0.6599 - LRFinder: val_loss: 3.9000 - lr = 0.00335590 \n",
            " 76/390 [====>.........................] - ETA: 2:01 - loss: 1.3849 - acc: 0.6595 - LRFinder: val_loss: 3.9117 - lr = 0.00337692 \n",
            " 77/390 [====>.........................] - ETA: 2:00 - loss: 1.3854 - acc: 0.6597 - LRFinder: val_loss: 3.7138 - lr = 0.00339795 \n",
            " 78/390 [=====>........................] - ETA: 2:00 - loss: 1.3849 - acc: 0.6603 - LRFinder: val_loss: 3.8060 - lr = 0.00341897 \n",
            " 79/390 [=====>........................] - ETA: 1:59 - loss: 1.3841 - acc: 0.6606 - LRFinder: val_loss: 3.7988 - lr = 0.00344000 \n",
            " 80/390 [=====>........................] - ETA: 1:58 - loss: 1.3828 - acc: 0.6610 - LRFinder: val_loss: 3.8328 - lr = 0.00346103 \n",
            " 81/390 [=====>........................] - ETA: 1:58 - loss: 1.3818 - acc: 0.6610 - LRFinder: val_loss: 4.0549 - lr = 0.00348205 \n",
            " 82/390 [=====>........................] - ETA: 1:57 - loss: 1.3822 - acc: 0.6606 - LRFinder: val_loss: 3.9283 - lr = 0.00350308 \n",
            " 83/390 [=====>........................] - ETA: 1:57 - loss: 1.3843 - acc: 0.6597 - LRFinder: val_loss: 4.0356 - lr = 0.00352410 \n",
            " 84/390 [=====>........................] - ETA: 1:56 - loss: 1.3874 - acc: 0.6589 - LRFinder: val_loss: 3.9919 - lr = 0.00354513 \n",
            " 85/390 [=====>........................] - ETA: 1:56 - loss: 1.3869 - acc: 0.6594 - LRFinder: val_loss: 4.1143 - lr = 0.00356615 \n",
            " 86/390 [=====>........................] - ETA: 1:55 - loss: 1.3852 - acc: 0.6598 - LRFinder: val_loss: 3.7930 - lr = 0.00358718 \n",
            " 87/390 [=====>........................] - ETA: 1:55 - loss: 1.3857 - acc: 0.6595 - LRFinder: val_loss: 4.1536 - lr = 0.00360821 \n",
            " 88/390 [=====>........................] - ETA: 1:54 - loss: 1.3868 - acc: 0.6594 - LRFinder: val_loss: 4.1616 - lr = 0.00362923 \n",
            " 89/390 [=====>........................] - ETA: 1:54 - loss: 1.3866 - acc: 0.6597 - LRFinder: val_loss: 4.0009 - lr = 0.00365026 \n",
            " 90/390 [=====>........................] - ETA: 1:53 - loss: 1.3852 - acc: 0.6604 - LRFinder: val_loss: 4.2193 - lr = 0.00367128 \n",
            " 91/390 [======>.......................] - ETA: 1:53 - loss: 1.3877 - acc: 0.6599 - LRFinder: val_loss: 4.2063 - lr = 0.00369231 \n",
            " 92/390 [======>.......................] - ETA: 1:52 - loss: 1.3866 - acc: 0.6603 - LRFinder: val_loss: 4.0638 - lr = 0.00371333 \n",
            " 93/390 [======>.......................] - ETA: 1:52 - loss: 1.3877 - acc: 0.6601 - LRFinder: val_loss: 4.2051 - lr = 0.00373436 \n",
            " 94/390 [======>.......................] - ETA: 1:51 - loss: 1.3866 - acc: 0.6600 - LRFinder: val_loss: 4.1025 - lr = 0.00375538 \n",
            " 95/390 [======>.......................] - ETA: 1:51 - loss: 1.3873 - acc: 0.6600 - LRFinder: val_loss: 4.1184 - lr = 0.00377641 \n",
            " 96/390 [======>.......................] - ETA: 1:50 - loss: 1.3859 - acc: 0.6604 - LRFinder: val_loss: 4.0871 - lr = 0.00379744 \n",
            " 97/390 [======>.......................] - ETA: 1:50 - loss: 1.3865 - acc: 0.6602 - LRFinder: val_loss: 4.1029 - lr = 0.00381846 \n",
            " 98/390 [======>.......................] - ETA: 1:49 - loss: 1.3860 - acc: 0.6605 - LRFinder: val_loss: 3.9453 - lr = 0.00383949 \n",
            " 99/390 [======>.......................] - ETA: 1:49 - loss: 1.3841 - acc: 0.6611 - LRFinder: val_loss: 3.8948 - lr = 0.00386051 \n",
            "100/390 [======>.......................] - ETA: 1:49 - loss: 1.3847 - acc: 0.6611 - LRFinder: val_loss: 3.9109 - lr = 0.00388154 \n",
            "101/390 [======>.......................] - ETA: 1:48 - loss: 1.3835 - acc: 0.6618 - LRFinder: val_loss: 3.8522 - lr = 0.00390256 \n",
            "102/390 [======>.......................] - ETA: 1:48 - loss: 1.3842 - acc: 0.6619 - LRFinder: val_loss: 3.9147 - lr = 0.00392359 \n",
            "103/390 [======>.......................] - ETA: 1:47 - loss: 1.3839 - acc: 0.6622 - LRFinder: val_loss: 3.9697 - lr = 0.00394462 \n",
            "104/390 [=======>......................] - ETA: 1:47 - loss: 1.3847 - acc: 0.6621 - LRFinder: val_loss: 4.0730 - lr = 0.00396564 \n",
            "105/390 [=======>......................] - ETA: 1:46 - loss: 1.3846 - acc: 0.6621 - LRFinder: val_loss: 4.0605 - lr = 0.00398667 \n",
            "106/390 [=======>......................] - ETA: 1:46 - loss: 1.3838 - acc: 0.6622 - LRFinder: val_loss: 4.0161 - lr = 0.00400769 \n",
            "107/390 [=======>......................] - ETA: 1:45 - loss: 1.3837 - acc: 0.6624 - LRFinder: val_loss: 3.8750 - lr = 0.00402872 \n",
            "108/390 [=======>......................] - ETA: 1:45 - loss: 1.3851 - acc: 0.6618 - LRFinder: val_loss: 4.2230 - lr = 0.00404974 \n",
            "109/390 [=======>......................] - ETA: 1:44 - loss: 1.3846 - acc: 0.6618 - LRFinder: val_loss: 3.9118 - lr = 0.00407077 \n",
            "110/390 [=======>......................] - ETA: 1:44 - loss: 1.3852 - acc: 0.6619 - LRFinder: val_loss: 3.8497 - lr = 0.00409179 \n",
            "111/390 [=======>......................] - ETA: 1:44 - loss: 1.3849 - acc: 0.6621 - LRFinder: val_loss: 4.0315 - lr = 0.00411282 \n",
            "112/390 [=======>......................] - ETA: 1:43 - loss: 1.3849 - acc: 0.6619 - LRFinder: val_loss: 4.0881 - lr = 0.00413385 \n",
            "113/390 [=======>......................] - ETA: 1:43 - loss: 1.3852 - acc: 0.6614 - LRFinder: val_loss: 4.0421 - lr = 0.00415487 \n",
            "114/390 [=======>......................] - ETA: 1:42 - loss: 1.3848 - acc: 0.6615 - LRFinder: val_loss: 3.8801 - lr = 0.00417590 \n",
            "115/390 [=======>......................] - ETA: 1:42 - loss: 1.3855 - acc: 0.6613 - LRFinder: val_loss: 4.0656 - lr = 0.00419692 \n",
            "116/390 [=======>......................] - ETA: 1:41 - loss: 1.3854 - acc: 0.6612 - LRFinder: val_loss: 4.2026 - lr = 0.00421795 \n",
            "117/390 [========>.....................] - ETA: 1:41 - loss: 1.3850 - acc: 0.6611 - LRFinder: val_loss: 4.1433 - lr = 0.00423897 \n",
            "118/390 [========>.....................] - ETA: 1:40 - loss: 1.3830 - acc: 0.6619 - LRFinder: val_loss: 4.2097 - lr = 0.00426000 \n",
            "119/390 [========>.....................] - ETA: 1:40 - loss: 1.3825 - acc: 0.6620 - LRFinder: val_loss: 4.0106 - lr = 0.00428103 \n",
            "120/390 [========>.....................] - ETA: 1:40 - loss: 1.3819 - acc: 0.6624 - LRFinder: val_loss: 3.9074 - lr = 0.00430205 \n",
            "121/390 [========>.....................] - ETA: 1:39 - loss: 1.3822 - acc: 0.6626 - LRFinder: val_loss: 4.1630 - lr = 0.00432308 \n",
            "122/390 [========>.....................] - ETA: 1:39 - loss: 1.3826 - acc: 0.6623 - LRFinder: val_loss: 4.1135 - lr = 0.00434410 \n",
            "123/390 [========>.....................] - ETA: 1:38 - loss: 1.3824 - acc: 0.6624 - LRFinder: val_loss: 4.0511 - lr = 0.00436513 \n",
            "124/390 [========>.....................] - ETA: 1:38 - loss: 1.3822 - acc: 0.6624 - LRFinder: val_loss: 3.7370 - lr = 0.00438615 \n",
            "125/390 [========>.....................] - ETA: 1:37 - loss: 1.3825 - acc: 0.6626 - LRFinder: val_loss: 3.8239 - lr = 0.00440718 \n",
            "126/390 [========>.....................] - ETA: 1:37 - loss: 1.3820 - acc: 0.6631 - LRFinder: val_loss: 3.8122 - lr = 0.00442821 \n",
            "127/390 [========>.....................] - ETA: 1:37 - loss: 1.3823 - acc: 0.6632 - LRFinder: val_loss: 3.9584 - lr = 0.00444923 \n",
            "128/390 [========>.....................] - ETA: 1:36 - loss: 1.3819 - acc: 0.6630 - LRFinder: val_loss: 3.9838 - lr = 0.00447026 \n",
            "129/390 [========>.....................] - ETA: 1:36 - loss: 1.3829 - acc: 0.6628 - LRFinder: val_loss: 3.8411 - lr = 0.00449128 \n",
            "130/390 [=========>....................] - ETA: 1:35 - loss: 1.3835 - acc: 0.6628 - LRFinder: val_loss: 3.8622 - lr = 0.00451231 \n",
            "131/390 [=========>....................] - ETA: 1:35 - loss: 1.3824 - acc: 0.6630 - LRFinder: val_loss: 3.8420 - lr = 0.00453333 \n",
            "132/390 [=========>....................] - ETA: 1:35 - loss: 1.3817 - acc: 0.6634 - LRFinder: val_loss: 4.0309 - lr = 0.00455436 \n",
            "133/390 [=========>....................] - ETA: 1:34 - loss: 1.3810 - acc: 0.6634 - LRFinder: val_loss: 3.8413 - lr = 0.00457538 \n",
            "134/390 [=========>....................] - ETA: 1:34 - loss: 1.3814 - acc: 0.6634 - LRFinder: val_loss: 3.8580 - lr = 0.00459641 \n",
            "135/390 [=========>....................] - ETA: 1:33 - loss: 1.3814 - acc: 0.6636 - LRFinder: val_loss: 3.8859 - lr = 0.00461744 \n",
            "136/390 [=========>....................] - ETA: 1:33 - loss: 1.3823 - acc: 0.6633 - LRFinder: val_loss: 3.9153 - lr = 0.00463846 \n",
            "137/390 [=========>....................] - ETA: 1:32 - loss: 1.3830 - acc: 0.6629 - LRFinder: val_loss: 3.7910 - lr = 0.00465949 \n",
            "138/390 [=========>....................] - ETA: 1:32 - loss: 1.3834 - acc: 0.6629 - LRFinder: val_loss: 3.8496 - lr = 0.00468051 \n",
            "139/390 [=========>....................] - ETA: 1:32 - loss: 1.3837 - acc: 0.6629 - LRFinder: val_loss: 4.0347 - lr = 0.00470154 \n",
            "140/390 [=========>....................] - ETA: 1:31 - loss: 1.3829 - acc: 0.6629 - LRFinder: val_loss: 3.9090 - lr = 0.00472256 \n",
            "141/390 [=========>....................] - ETA: 1:31 - loss: 1.3832 - acc: 0.6629 - LRFinder: val_loss: 4.0328 - lr = 0.00474359 \n",
            "142/390 [=========>....................] - ETA: 1:30 - loss: 1.3836 - acc: 0.6626 - LRFinder: val_loss: 3.9002 - lr = 0.00476462 \n",
            "143/390 [==========>...................] - ETA: 1:30 - loss: 1.3835 - acc: 0.6628 - LRFinder: val_loss: 3.9865 - lr = 0.00478564 \n",
            "144/390 [==========>...................] - ETA: 1:30 - loss: 1.3828 - acc: 0.6631 - LRFinder: val_loss: 4.1505 - lr = 0.00480667 \n",
            "145/390 [==========>...................] - ETA: 1:29 - loss: 1.3822 - acc: 0.6634 - LRFinder: val_loss: 3.9690 - lr = 0.00482769 \n",
            "146/390 [==========>...................] - ETA: 1:29 - loss: 1.3823 - acc: 0.6633 - LRFinder: val_loss: 3.9650 - lr = 0.00484872 \n",
            "147/390 [==========>...................] - ETA: 1:28 - loss: 1.3824 - acc: 0.6634 - LRFinder: val_loss: 3.9569 - lr = 0.00486974 \n",
            "148/390 [==========>...................] - ETA: 1:28 - loss: 1.3824 - acc: 0.6634 - LRFinder: val_loss: 3.8737 - lr = 0.00489077 \n",
            "149/390 [==========>...................] - ETA: 1:28 - loss: 1.3824 - acc: 0.6633 - LRFinder: val_loss: 3.9364 - lr = 0.00491179 \n",
            "150/390 [==========>...................] - ETA: 1:27 - loss: 1.3820 - acc: 0.6633 - LRFinder: val_loss: 4.1519 - lr = 0.00493282 \n",
            "151/390 [==========>...................] - ETA: 1:27 - loss: 1.3824 - acc: 0.6630 - LRFinder: val_loss: 4.0678 - lr = 0.00495385 \n",
            "152/390 [==========>...................] - ETA: 1:26 - loss: 1.3828 - acc: 0.6630 - LRFinder: val_loss: 3.7760 - lr = 0.00497487 \n",
            "153/390 [==========>...................] - ETA: 1:26 - loss: 1.3820 - acc: 0.6635 - LRFinder: val_loss: 3.8946 - lr = 0.00499590 \n",
            "154/390 [==========>...................] - ETA: 1:26 - loss: 1.3821 - acc: 0.6635 - LRFinder: val_loss: 3.9239 - lr = 0.00501692 \n",
            "155/390 [==========>...................] - ETA: 1:25 - loss: 1.3824 - acc: 0.6637 - LRFinder: val_loss: 3.9628 - lr = 0.00503795 \n",
            "156/390 [===========>..................] - ETA: 1:25 - loss: 1.3826 - acc: 0.6638 - LRFinder: val_loss: 3.9458 - lr = 0.00505897 \n",
            "157/390 [===========>..................] - ETA: 1:24 - loss: 1.3823 - acc: 0.6636 - LRFinder: val_loss: 3.9135 - lr = 0.00508000 \n",
            "158/390 [===========>..................] - ETA: 1:24 - loss: 1.3814 - acc: 0.6637 - LRFinder: val_loss: 3.9579 - lr = 0.00510103 \n",
            "159/390 [===========>..................] - ETA: 1:24 - loss: 1.3824 - acc: 0.6636 - LRFinder: val_loss: 3.8139 - lr = 0.00512205 \n",
            "160/390 [===========>..................] - ETA: 1:23 - loss: 1.3821 - acc: 0.6636 - LRFinder: val_loss: 3.8555 - lr = 0.00514308 \n",
            "161/390 [===========>..................] - ETA: 1:23 - loss: 1.3821 - acc: 0.6636 - LRFinder: val_loss: 3.8506 - lr = 0.00516410 \n",
            "162/390 [===========>..................] - ETA: 1:22 - loss: 1.3832 - acc: 0.6632 - LRFinder: val_loss: 3.9926 - lr = 0.00518513 \n",
            "163/390 [===========>..................] - ETA: 1:22 - loss: 1.3833 - acc: 0.6632 - LRFinder: val_loss: 3.7604 - lr = 0.00520615 \n",
            "164/390 [===========>..................] - ETA: 1:22 - loss: 1.3830 - acc: 0.6632 - LRFinder: val_loss: 3.8289 - lr = 0.00522718 \n",
            "165/390 [===========>..................] - ETA: 1:21 - loss: 1.3831 - acc: 0.6634 - LRFinder: val_loss: 4.0048 - lr = 0.00524821 \n",
            "166/390 [===========>..................] - ETA: 1:21 - loss: 1.3833 - acc: 0.6630 - LRFinder: val_loss: 3.7047 - lr = 0.00526923 \n",
            "167/390 [===========>..................] - ETA: 1:21 - loss: 1.3841 - acc: 0.6628 - LRFinder: val_loss: 3.6943 - lr = 0.00529026 \n",
            "168/390 [===========>..................] - ETA: 1:20 - loss: 1.3833 - acc: 0.6632 - LRFinder: val_loss: 3.8862 - lr = 0.00531128 \n",
            "169/390 [============>.................] - ETA: 1:20 - loss: 1.3826 - acc: 0.6633 - LRFinder: val_loss: 3.7453 - lr = 0.00533231 \n",
            "170/390 [============>.................] - ETA: 1:19 - loss: 1.3827 - acc: 0.6630 - LRFinder: val_loss: 3.8573 - lr = 0.00535333 \n",
            "171/390 [============>.................] - ETA: 1:19 - loss: 1.3821 - acc: 0.6634 - LRFinder: val_loss: 3.6166 - lr = 0.00537436 \n",
            "172/390 [============>.................] - ETA: 1:19 - loss: 1.3819 - acc: 0.6635 - LRFinder: val_loss: 3.8340 - lr = 0.00539538 \n",
            "173/390 [============>.................] - ETA: 1:18 - loss: 1.3816 - acc: 0.6637 - LRFinder: val_loss: 3.7951 - lr = 0.00541641 \n",
            "174/390 [============>.................] - ETA: 1:18 - loss: 1.3821 - acc: 0.6638 - LRFinder: val_loss: 3.7145 - lr = 0.00543744 \n",
            "175/390 [============>.................] - ETA: 1:17 - loss: 1.3825 - acc: 0.6638 - LRFinder: val_loss: 3.6972 - lr = 0.00545846 \n",
            "176/390 [============>.................] - ETA: 1:17 - loss: 1.3818 - acc: 0.6640 - LRFinder: val_loss: 3.8357 - lr = 0.00547949 \n",
            "177/390 [============>.................] - ETA: 1:17 - loss: 1.3828 - acc: 0.6638 - LRFinder: val_loss: 3.7523 - lr = 0.00550051 \n",
            "178/390 [============>.................] - ETA: 1:16 - loss: 1.3825 - acc: 0.6637 - LRFinder: val_loss: 3.8070 - lr = 0.00552154 \n",
            "179/390 [============>.................] - ETA: 1:16 - loss: 1.3828 - acc: 0.6636 - LRFinder: val_loss: 4.0254 - lr = 0.00554256 \n",
            "180/390 [============>.................] - ETA: 1:16 - loss: 1.3824 - acc: 0.6640 - LRFinder: val_loss: 3.8469 - lr = 0.00556359 \n",
            "181/390 [============>.................] - ETA: 1:15 - loss: 1.3828 - acc: 0.6640 - LRFinder: val_loss: 4.0415 - lr = 0.00558462 \n",
            "182/390 [=============>................] - ETA: 1:15 - loss: 1.3827 - acc: 0.6639 - LRFinder: val_loss: 3.8467 - lr = 0.00560564 \n",
            "183/390 [=============>................] - ETA: 1:14 - loss: 1.3830 - acc: 0.6638 - LRFinder: val_loss: 3.6969 - lr = 0.00562667 \n",
            "184/390 [=============>................] - ETA: 1:14 - loss: 1.3831 - acc: 0.6637 - LRFinder: val_loss: 3.9380 - lr = 0.00564769 \n",
            "185/390 [=============>................] - ETA: 1:14 - loss: 1.3831 - acc: 0.6639 - LRFinder: val_loss: 4.0114 - lr = 0.00566872 \n",
            "186/390 [=============>................] - ETA: 1:13 - loss: 1.3830 - acc: 0.6639 - LRFinder: val_loss: 3.6880 - lr = 0.00568974 \n",
            "187/390 [=============>................] - ETA: 1:13 - loss: 1.3831 - acc: 0.6641 - LRFinder: val_loss: 3.8683 - lr = 0.00571077 \n",
            "188/390 [=============>................] - ETA: 1:12 - loss: 1.3835 - acc: 0.6640 - LRFinder: val_loss: 3.9520 - lr = 0.00573179 \n",
            "189/390 [=============>................] - ETA: 1:12 - loss: 1.3837 - acc: 0.6639 - LRFinder: val_loss: 4.0347 - lr = 0.00575282 \n",
            "190/390 [=============>................] - ETA: 1:12 - loss: 1.3836 - acc: 0.6638 - LRFinder: val_loss: 4.1594 - lr = 0.00577385 \n",
            "191/390 [=============>................] - ETA: 1:11 - loss: 1.3838 - acc: 0.6639 - LRFinder: val_loss: 3.9533 - lr = 0.00579487 \n",
            "192/390 [=============>................] - ETA: 1:11 - loss: 1.3839 - acc: 0.6637 - LRFinder: val_loss: 4.1895 - lr = 0.00581590 \n",
            "193/390 [=============>................] - ETA: 1:11 - loss: 1.3822 - acc: 0.6644 - LRFinder: val_loss: 3.9560 - lr = 0.00583692 \n",
            "194/390 [=============>................] - ETA: 1:10 - loss: 1.3820 - acc: 0.6644 - LRFinder: val_loss: 4.1820 - lr = 0.00585795 \n",
            "195/390 [==============>...............] - ETA: 1:10 - loss: 1.3810 - acc: 0.6648 - LRFinder: val_loss: 3.9361 - lr = 0.00587897 \n",
            "196/390 [==============>...............] - ETA: 1:09 - loss: 1.3810 - acc: 0.6648 - LRFinder: val_loss: 3.9932 - lr = 0.00590000 \n",
            "197/390 [==============>...............] - ETA: 1:09 - loss: 1.3804 - acc: 0.6652 - LRFinder: val_loss: 4.0580 - lr = 0.00592103 \n",
            "198/390 [==============>...............] - ETA: 1:09 - loss: 1.3814 - acc: 0.6649 - LRFinder: val_loss: 4.0274 - lr = 0.00594205 \n",
            "199/390 [==============>...............] - ETA: 1:08 - loss: 1.3818 - acc: 0.6648 - LRFinder: val_loss: 4.0041 - lr = 0.00596308 \n",
            "200/390 [==============>...............] - ETA: 1:08 - loss: 1.3814 - acc: 0.6650 - LRFinder: val_loss: 3.7646 - lr = 0.00598410 \n",
            "201/390 [==============>...............] - ETA: 1:08 - loss: 1.3805 - acc: 0.6652 - LRFinder: val_loss: 3.9487 - lr = 0.00600513 \n",
            "202/390 [==============>...............] - ETA: 1:07 - loss: 1.3808 - acc: 0.6652 - LRFinder: val_loss: 3.7902 - lr = 0.00602615 \n",
            "203/390 [==============>...............] - ETA: 1:07 - loss: 1.3810 - acc: 0.6651 - LRFinder: val_loss: 3.9491 - lr = 0.00604718 \n",
            "204/390 [==============>...............] - ETA: 1:06 - loss: 1.3813 - acc: 0.6651 - LRFinder: val_loss: 3.7745 - lr = 0.00606821 \n",
            "205/390 [==============>...............] - ETA: 1:06 - loss: 1.3808 - acc: 0.6652 - LRFinder: val_loss: 3.8039 - lr = 0.00608923 \n",
            "206/390 [==============>...............] - ETA: 1:06 - loss: 1.3807 - acc: 0.6654 - LRFinder: val_loss: 3.8858 - lr = 0.00611026 \n",
            "207/390 [==============>...............] - ETA: 1:05 - loss: 1.3816 - acc: 0.6649 - LRFinder: val_loss: 4.0393 - lr = 0.00613128 \n",
            "208/390 [===============>..............] - ETA: 1:05 - loss: 1.3815 - acc: 0.6650 - LRFinder: val_loss: 4.0539 - lr = 0.00615231 \n",
            "209/390 [===============>..............] - ETA: 1:05 - loss: 1.3813 - acc: 0.6650 - LRFinder: val_loss: 4.0021 - lr = 0.00617333 \n",
            "210/390 [===============>..............] - ETA: 1:04 - loss: 1.3814 - acc: 0.6649 - LRFinder: val_loss: 3.9114 - lr = 0.00619436 \n",
            "211/390 [===============>..............] - ETA: 1:04 - loss: 1.3810 - acc: 0.6651 - LRFinder: val_loss: 4.0205 - lr = 0.00621538 \n",
            "212/390 [===============>..............] - ETA: 1:03 - loss: 1.3811 - acc: 0.6650 - LRFinder: val_loss: 3.9116 - lr = 0.00623641 \n",
            "213/390 [===============>..............] - ETA: 1:03 - loss: 1.3810 - acc: 0.6651 - LRFinder: val_loss: 3.9552 - lr = 0.00625744 \n",
            "214/390 [===============>..............] - ETA: 1:03 - loss: 1.3803 - acc: 0.6653 - LRFinder: val_loss: 3.8766 - lr = 0.00627846 \n",
            "215/390 [===============>..............] - ETA: 1:02 - loss: 1.3803 - acc: 0.6653 - LRFinder: val_loss: 3.8739 - lr = 0.00629949 \n",
            "216/390 [===============>..............] - ETA: 1:02 - loss: 1.3800 - acc: 0.6654 - LRFinder: val_loss: 3.9236 - lr = 0.00632051 \n",
            "217/390 [===============>..............] - ETA: 1:02 - loss: 1.3798 - acc: 0.6654 - LRFinder: val_loss: 3.8404 - lr = 0.00634154 \n",
            "218/390 [===============>..............] - ETA: 1:01 - loss: 1.3792 - acc: 0.6657 - LRFinder: val_loss: 4.0863 - lr = 0.00636256 \n",
            "219/390 [===============>..............] - ETA: 1:01 - loss: 1.3795 - acc: 0.6655 - LRFinder: val_loss: 3.8978 - lr = 0.00638359 \n",
            "220/390 [===============>..............] - ETA: 1:00 - loss: 1.3794 - acc: 0.6657 - LRFinder: val_loss: 3.8558 - lr = 0.00640462 \n",
            "221/390 [================>.............] - ETA: 1:00 - loss: 1.3798 - acc: 0.6658 - LRFinder: val_loss: 3.8552 - lr = 0.00642564 \n",
            "222/390 [================>.............] - ETA: 1:00 - loss: 1.3794 - acc: 0.6661 - LRFinder: val_loss: 3.8656 - lr = 0.00644667 \n",
            "223/390 [================>.............] - ETA: 59s - loss: 1.3789 - acc: 0.6662  - LRFinder: val_loss: 3.7411 - lr = 0.00646769 \n",
            "224/390 [================>.............] - ETA: 59s - loss: 1.3781 - acc: 0.6664 - LRFinder: val_loss: 3.9867 - lr = 0.00648872 \n",
            "225/390 [================>.............] - ETA: 59s - loss: 1.3774 - acc: 0.6668 - LRFinder: val_loss: 3.6186 - lr = 0.00650974 \n",
            "226/390 [================>.............] - ETA: 58s - loss: 1.3772 - acc: 0.6669 - LRFinder: val_loss: 3.8158 - lr = 0.00653077 \n",
            "227/390 [================>.............] - ETA: 58s - loss: 1.3776 - acc: 0.6666 - LRFinder: val_loss: 3.8342 - lr = 0.00655179 \n",
            "228/390 [================>.............] - ETA: 57s - loss: 1.3785 - acc: 0.6664 - LRFinder: val_loss: 3.8321 - lr = 0.00657282 \n",
            "229/390 [================>.............] - ETA: 57s - loss: 1.3782 - acc: 0.6665 - LRFinder: val_loss: 3.9570 - lr = 0.00659385 \n",
            "230/390 [================>.............] - ETA: 57s - loss: 1.3779 - acc: 0.6665 - LRFinder: val_loss: 4.2330 - lr = 0.00661487 \n",
            "231/390 [================>.............] - ETA: 56s - loss: 1.3779 - acc: 0.6665 - LRFinder: val_loss: 4.2077 - lr = 0.00663590 \n",
            "232/390 [================>.............] - ETA: 56s - loss: 1.3773 - acc: 0.6667 - LRFinder: val_loss: 4.2127 - lr = 0.00665692 \n",
            "233/390 [================>.............] - ETA: 56s - loss: 1.3766 - acc: 0.6670 - LRFinder: val_loss: 3.8200 - lr = 0.00667795 \n",
            "234/390 [=================>............] - ETA: 55s - loss: 1.3768 - acc: 0.6671 - LRFinder: val_loss: 4.1383 - lr = 0.00669897 \n",
            "235/390 [=================>............] - ETA: 55s - loss: 1.3773 - acc: 0.6670 - LRFinder: val_loss: 4.0741 - lr = 0.00672000 \n",
            "236/390 [=================>............] - ETA: 55s - loss: 1.3770 - acc: 0.6672 - LRFinder: val_loss: 3.8327 - lr = 0.00674103 \n",
            "237/390 [=================>............] - ETA: 54s - loss: 1.3775 - acc: 0.6672 - LRFinder: val_loss: 3.9911 - lr = 0.00676205 \n",
            "238/390 [=================>............] - ETA: 54s - loss: 1.3775 - acc: 0.6672 - LRFinder: val_loss: 3.9246 - lr = 0.00678308 \n",
            "239/390 [=================>............] - ETA: 53s - loss: 1.3773 - acc: 0.6672 - LRFinder: val_loss: 3.8394 - lr = 0.00680410 \n",
            "240/390 [=================>............] - ETA: 53s - loss: 1.3770 - acc: 0.6674 - LRFinder: val_loss: 3.8991 - lr = 0.00682513 \n",
            "241/390 [=================>............] - ETA: 53s - loss: 1.3773 - acc: 0.6670 - LRFinder: val_loss: 3.8028 - lr = 0.00684615 \n",
            "242/390 [=================>............] - ETA: 52s - loss: 1.3768 - acc: 0.6671 - LRFinder: val_loss: 3.8871 - lr = 0.00686718 \n",
            "243/390 [=================>............] - ETA: 52s - loss: 1.3778 - acc: 0.6669 - LRFinder: val_loss: 3.9028 - lr = 0.00688821 \n",
            "244/390 [=================>............] - ETA: 52s - loss: 1.3781 - acc: 0.6669 - LRFinder: val_loss: 3.8488 - lr = 0.00690923 \n",
            "245/390 [=================>............] - ETA: 51s - loss: 1.3772 - acc: 0.6673 - LRFinder: val_loss: 3.8152 - lr = 0.00693026 \n",
            "246/390 [=================>............] - ETA: 51s - loss: 1.3774 - acc: 0.6674 - LRFinder: val_loss: 3.9110 - lr = 0.00695128 \n",
            "247/390 [==================>...........] - ETA: 50s - loss: 1.3779 - acc: 0.6671 - LRFinder: val_loss: 3.7583 - lr = 0.00697231 \n",
            "248/390 [==================>...........] - ETA: 50s - loss: 1.3779 - acc: 0.6669 - LRFinder: val_loss: 3.7676 - lr = 0.00699333 \n",
            "249/390 [==================>...........] - ETA: 50s - loss: 1.3783 - acc: 0.6669 - LRFinder: val_loss: 3.7286 - lr = 0.00701436 \n",
            "250/390 [==================>...........] - ETA: 49s - loss: 1.3782 - acc: 0.6671 - LRFinder: val_loss: 3.9189 - lr = 0.00703538 \n",
            "251/390 [==================>...........] - ETA: 49s - loss: 1.3785 - acc: 0.6672 - LRFinder: val_loss: 4.0511 - lr = 0.00705641 \n",
            "252/390 [==================>...........] - ETA: 49s - loss: 1.3784 - acc: 0.6672 - LRFinder: val_loss: 4.1262 - lr = 0.00707744 \n",
            "253/390 [==================>...........] - ETA: 48s - loss: 1.3781 - acc: 0.6672 - LRFinder: val_loss: 4.0445 - lr = 0.00709846 \n",
            "254/390 [==================>...........] - ETA: 48s - loss: 1.3778 - acc: 0.6675 - LRFinder: val_loss: 4.1984 - lr = 0.00711949 \n",
            "255/390 [==================>...........] - ETA: 48s - loss: 1.3773 - acc: 0.6674 - LRFinder: val_loss: 3.9392 - lr = 0.00714051 \n",
            "256/390 [==================>...........] - ETA: 47s - loss: 1.3770 - acc: 0.6676 - LRFinder: val_loss: 4.0885 - lr = 0.00716154 \n",
            "257/390 [==================>...........] - ETA: 47s - loss: 1.3773 - acc: 0.6673 - LRFinder: val_loss: 3.9931 - lr = 0.00718256 \n",
            "258/390 [==================>...........] - ETA: 46s - loss: 1.3773 - acc: 0.6673 - LRFinder: val_loss: 4.1204 - lr = 0.00720359 \n",
            "259/390 [==================>...........] - ETA: 46s - loss: 1.3770 - acc: 0.6675 - LRFinder: val_loss: 3.8780 - lr = 0.00722462 \n",
            "260/390 [===================>..........] - ETA: 46s - loss: 1.3769 - acc: 0.6675 - LRFinder: val_loss: 4.1482 - lr = 0.00724564 \n",
            "261/390 [===================>..........] - ETA: 45s - loss: 1.3767 - acc: 0.6676 - LRFinder: val_loss: 4.0056 - lr = 0.00726667 \n",
            "262/390 [===================>..........] - ETA: 45s - loss: 1.3770 - acc: 0.6675 - LRFinder: val_loss: 4.0897 - lr = 0.00728769 \n",
            "263/390 [===================>..........] - ETA: 45s - loss: 1.3768 - acc: 0.6675 - LRFinder: val_loss: 4.2210 - lr = 0.00730872 \n",
            "264/390 [===================>..........] - ETA: 44s - loss: 1.3769 - acc: 0.6676 - LRFinder: val_loss: 4.1518 - lr = 0.00732974 \n",
            "265/390 [===================>..........] - ETA: 44s - loss: 1.3768 - acc: 0.6678 - LRFinder: val_loss: 4.1541 - lr = 0.00735077 \n",
            "266/390 [===================>..........] - ETA: 44s - loss: 1.3763 - acc: 0.6679 - LRFinder: val_loss: 4.4799 - lr = 0.00737179 \n",
            "267/390 [===================>..........] - ETA: 43s - loss: 1.3760 - acc: 0.6680 - LRFinder: val_loss: 4.2886 - lr = 0.00739282 \n",
            "268/390 [===================>..........] - ETA: 43s - loss: 1.3764 - acc: 0.6678 - LRFinder: val_loss: 4.3941 - lr = 0.00741385 \n",
            "269/390 [===================>..........] - ETA: 43s - loss: 1.3770 - acc: 0.6676 - LRFinder: val_loss: 4.4318 - lr = 0.00743487 \n",
            "270/390 [===================>..........] - ETA: 42s - loss: 1.3783 - acc: 0.6672 - LRFinder: val_loss: 4.2111 - lr = 0.00745590 \n",
            "271/390 [===================>..........] - ETA: 42s - loss: 1.3781 - acc: 0.6673 - LRFinder: val_loss: 3.8926 - lr = 0.00747692 \n",
            "272/390 [===================>..........] - ETA: 41s - loss: 1.3778 - acc: 0.6676 - LRFinder: val_loss: 4.3293 - lr = 0.00749795 \n",
            "273/390 [====================>.........] - ETA: 41s - loss: 1.3777 - acc: 0.6676 - LRFinder: val_loss: 4.0791 - lr = 0.00751897 \n",
            "274/390 [====================>.........] - ETA: 41s - loss: 1.3778 - acc: 0.6676 - LRFinder: val_loss: 4.1140 - lr = 0.00754000 \n",
            "275/390 [====================>.........] - ETA: 40s - loss: 1.3774 - acc: 0.6679 - LRFinder: val_loss: 4.2727 - lr = 0.00756103 \n",
            "276/390 [====================>.........] - ETA: 40s - loss: 1.3776 - acc: 0.6678 - LRFinder: val_loss: 4.2324 - lr = 0.00758205 \n",
            "277/390 [====================>.........] - ETA: 40s - loss: 1.3775 - acc: 0.6678 - LRFinder: val_loss: 4.3029 - lr = 0.00760308 \n",
            "278/390 [====================>.........] - ETA: 39s - loss: 1.3772 - acc: 0.6679 - LRFinder: val_loss: 4.1798 - lr = 0.00762410 \n",
            "279/390 [====================>.........] - ETA: 39s - loss: 1.3768 - acc: 0.6681 - LRFinder: val_loss: 4.3487 - lr = 0.00764513 \n",
            "280/390 [====================>.........] - ETA: 39s - loss: 1.3764 - acc: 0.6683 - LRFinder: val_loss: 4.2175 - lr = 0.00766615 \n",
            "281/390 [====================>.........] - ETA: 38s - loss: 1.3762 - acc: 0.6685 - LRFinder: val_loss: 4.1540 - lr = 0.00768718 \n",
            "282/390 [====================>.........] - ETA: 38s - loss: 1.3767 - acc: 0.6681 - LRFinder: val_loss: 4.3042 - lr = 0.00770821 \n",
            "283/390 [====================>.........] - ETA: 37s - loss: 1.3765 - acc: 0.6682 - LRFinder: val_loss: 4.2310 - lr = 0.00772923 \n",
            "284/390 [====================>.........] - ETA: 37s - loss: 1.3768 - acc: 0.6681 - LRFinder: val_loss: 3.8931 - lr = 0.00775026 \n",
            "285/390 [====================>.........] - ETA: 37s - loss: 1.3767 - acc: 0.6681 - LRFinder: val_loss: 3.9575 - lr = 0.00777128 \n",
            "286/390 [=====================>........] - ETA: 36s - loss: 1.3767 - acc: 0.6682 - LRFinder: val_loss: 4.0491 - lr = 0.00779231 \n",
            "287/390 [=====================>........] - ETA: 36s - loss: 1.3765 - acc: 0.6681 - LRFinder: val_loss: 3.6810 - lr = 0.00781333 \n",
            "288/390 [=====================>........] - ETA: 36s - loss: 1.3763 - acc: 0.6683 - LRFinder: val_loss: 3.9490 - lr = 0.00783436 \n",
            "289/390 [=====================>........] - ETA: 35s - loss: 1.3761 - acc: 0.6682 - LRFinder: val_loss: 3.9909 - lr = 0.00785538 \n",
            "290/390 [=====================>........] - ETA: 35s - loss: 1.3760 - acc: 0.6682 - LRFinder: val_loss: 3.8597 - lr = 0.00787641 \n",
            "291/390 [=====================>........] - ETA: 35s - loss: 1.3763 - acc: 0.6681 - LRFinder: val_loss: 3.6517 - lr = 0.00789744 \n",
            "292/390 [=====================>........] - ETA: 34s - loss: 1.3763 - acc: 0.6680 - LRFinder: val_loss: 4.0911 - lr = 0.00791846 \n",
            "293/390 [=====================>........] - ETA: 34s - loss: 1.3762 - acc: 0.6680 - LRFinder: val_loss: 3.8594 - lr = 0.00793949 \n",
            "294/390 [=====================>........] - ETA: 34s - loss: 1.3758 - acc: 0.6680 - LRFinder: val_loss: 3.9051 - lr = 0.00796051 \n",
            "295/390 [=====================>........] - ETA: 33s - loss: 1.3760 - acc: 0.6680 - LRFinder: val_loss: 4.0725 - lr = 0.00798154 \n",
            "296/390 [=====================>........] - ETA: 33s - loss: 1.3754 - acc: 0.6683 - LRFinder: val_loss: 4.0968 - lr = 0.00800256 \n",
            "297/390 [=====================>........] - ETA: 32s - loss: 1.3754 - acc: 0.6684 - LRFinder: val_loss: 4.0757 - lr = 0.00802359 \n",
            "298/390 [=====================>........] - ETA: 32s - loss: 1.3752 - acc: 0.6684 - LRFinder: val_loss: 4.1215 - lr = 0.00804462 \n",
            "299/390 [======================>.......] - ETA: 32s - loss: 1.3751 - acc: 0.6684 - LRFinder: val_loss: 3.9495 - lr = 0.00806564 \n",
            "300/390 [======================>.......] - ETA: 31s - loss: 1.3756 - acc: 0.6681 - LRFinder: val_loss: 3.9876 - lr = 0.00808667 \n",
            "301/390 [======================>.......] - ETA: 31s - loss: 1.3760 - acc: 0.6679 - LRFinder: val_loss: 3.8762 - lr = 0.00810769 \n",
            "302/390 [======================>.......] - ETA: 31s - loss: 1.3758 - acc: 0.6677 - LRFinder: val_loss: 3.9874 - lr = 0.00812872 \n",
            "303/390 [======================>.......] - ETA: 30s - loss: 1.3759 - acc: 0.6676 - LRFinder: val_loss: 3.9069 - lr = 0.00814974 \n",
            "304/390 [======================>.......] - ETA: 30s - loss: 1.3761 - acc: 0.6675 - LRFinder: val_loss: 3.8630 - lr = 0.00817077 \n",
            "305/390 [======================>.......] - ETA: 30s - loss: 1.3755 - acc: 0.6678 - LRFinder: val_loss: 4.0096 - lr = 0.00819179 \n",
            "306/390 [======================>.......] - ETA: 29s - loss: 1.3754 - acc: 0.6678 - LRFinder: val_loss: 3.9983 - lr = 0.00821282 \n",
            "307/390 [======================>.......] - ETA: 29s - loss: 1.3754 - acc: 0.6677 - LRFinder: val_loss: 3.9710 - lr = 0.00823385 \n",
            "308/390 [======================>.......] - ETA: 29s - loss: 1.3756 - acc: 0.6675 - LRFinder: val_loss: 4.0259 - lr = 0.00825487 \n",
            "309/390 [======================>.......] - ETA: 28s - loss: 1.3754 - acc: 0.6675 - LRFinder: val_loss: 3.8223 - lr = 0.00827590 \n",
            "310/390 [======================>.......] - ETA: 28s - loss: 1.3751 - acc: 0.6675 - LRFinder: val_loss: 3.8531 - lr = 0.00829692 \n",
            "311/390 [======================>.......] - ETA: 27s - loss: 1.3746 - acc: 0.6679 - LRFinder: val_loss: 3.8702 - lr = 0.00831795 \n",
            "312/390 [=======================>......] - ETA: 27s - loss: 1.3748 - acc: 0.6677 - LRFinder: val_loss: 3.8093 - lr = 0.00833897 \n",
            "313/390 [=======================>......] - ETA: 27s - loss: 1.3745 - acc: 0.6679 - LRFinder: val_loss: 3.8330 - lr = 0.00836000 \n",
            "314/390 [=======================>......] - ETA: 26s - loss: 1.3741 - acc: 0.6681 - LRFinder: val_loss: 4.0183 - lr = 0.00838103 \n",
            "315/390 [=======================>......] - ETA: 26s - loss: 1.3738 - acc: 0.6682 - LRFinder: val_loss: 3.9731 - lr = 0.00840205 \n",
            "316/390 [=======================>......] - ETA: 26s - loss: 1.3743 - acc: 0.6680 - LRFinder: val_loss: 4.1572 - lr = 0.00842308 \n",
            "317/390 [=======================>......] - ETA: 25s - loss: 1.3744 - acc: 0.6680 - LRFinder: val_loss: 4.0745 - lr = 0.00844410 \n",
            "318/390 [=======================>......] - ETA: 25s - loss: 1.3739 - acc: 0.6681 - LRFinder: val_loss: 4.1060 - lr = 0.00846513 \n",
            "319/390 [=======================>......] - ETA: 25s - loss: 1.3738 - acc: 0.6682 - LRFinder: val_loss: 3.9847 - lr = 0.00848615 \n",
            "320/390 [=======================>......] - ETA: 24s - loss: 1.3735 - acc: 0.6685 - LRFinder: val_loss: 4.1496 - lr = 0.00850718 \n",
            "321/390 [=======================>......] - ETA: 24s - loss: 1.3739 - acc: 0.6683 - LRFinder: val_loss: 4.0827 - lr = 0.00852821 \n",
            "322/390 [=======================>......] - ETA: 24s - loss: 1.3738 - acc: 0.6683 - LRFinder: val_loss: 4.2073 - lr = 0.00854923 \n",
            "323/390 [=======================>......] - ETA: 23s - loss: 1.3739 - acc: 0.6683 - LRFinder: val_loss: 4.1481 - lr = 0.00857026 \n",
            "324/390 [=======================>......] - ETA: 23s - loss: 1.3744 - acc: 0.6680 - LRFinder: val_loss: 4.2038 - lr = 0.00859128 \n",
            "325/390 [========================>.....] - ETA: 22s - loss: 1.3743 - acc: 0.6680 - LRFinder: val_loss: 3.9741 - lr = 0.00861231 \n",
            "326/390 [========================>.....] - ETA: 22s - loss: 1.3743 - acc: 0.6681 - LRFinder: val_loss: 3.9618 - lr = 0.00863333 \n",
            "327/390 [========================>.....] - ETA: 22s - loss: 1.3748 - acc: 0.6679 - LRFinder: val_loss: 4.0745 - lr = 0.00865436 \n",
            "328/390 [========================>.....] - ETA: 21s - loss: 1.3743 - acc: 0.6681 - LRFinder: val_loss: 3.9055 - lr = 0.00867538 \n",
            "329/390 [========================>.....] - ETA: 21s - loss: 1.3742 - acc: 0.6681 - LRFinder: val_loss: 3.9241 - lr = 0.00869641 \n",
            "330/390 [========================>.....] - ETA: 21s - loss: 1.3738 - acc: 0.6682 - LRFinder: val_loss: 4.1524 - lr = 0.00871744 \n",
            "331/390 [========================>.....] - ETA: 20s - loss: 1.3740 - acc: 0.6681 - LRFinder: val_loss: 3.8908 - lr = 0.00873846 \n",
            "332/390 [========================>.....] - ETA: 20s - loss: 1.3739 - acc: 0.6680 - LRFinder: val_loss: 4.0730 - lr = 0.00875949 \n",
            "333/390 [========================>.....] - ETA: 20s - loss: 1.3736 - acc: 0.6680 - LRFinder: val_loss: 4.1145 - lr = 0.00878051 \n",
            "334/390 [========================>.....] - ETA: 19s - loss: 1.3735 - acc: 0.6681 - LRFinder: val_loss: 3.8798 - lr = 0.00880154 \n",
            "335/390 [========================>.....] - ETA: 19s - loss: 1.3735 - acc: 0.6682 - LRFinder: val_loss: 3.7935 - lr = 0.00882256 \n",
            "336/390 [========================>.....] - ETA: 19s - loss: 1.3732 - acc: 0.6682 - LRFinder: val_loss: 3.7962 - lr = 0.00884359 \n",
            "337/390 [========================>.....] - ETA: 18s - loss: 1.3727 - acc: 0.6684 - LRFinder: val_loss: 3.8372 - lr = 0.00886462 \n",
            "338/390 [=========================>....] - ETA: 18s - loss: 1.3734 - acc: 0.6681 - LRFinder: val_loss: 3.9623 - lr = 0.00888564 \n",
            "339/390 [=========================>....] - ETA: 17s - loss: 1.3732 - acc: 0.6681 - LRFinder: val_loss: 3.7435 - lr = 0.00890667 \n",
            "340/390 [=========================>....] - ETA: 17s - loss: 1.3733 - acc: 0.6681 - LRFinder: val_loss: 3.7685 - lr = 0.00892769 \n",
            "341/390 [=========================>....] - ETA: 17s - loss: 1.3734 - acc: 0.6681 - LRFinder: val_loss: 3.8509 - lr = 0.00894872 \n",
            "342/390 [=========================>....] - ETA: 16s - loss: 1.3740 - acc: 0.6677 - LRFinder: val_loss: 3.8789 - lr = 0.00896974 \n",
            "343/390 [=========================>....] - ETA: 16s - loss: 1.3741 - acc: 0.6677 - LRFinder: val_loss: 3.9885 - lr = 0.00899077 \n",
            "344/390 [=========================>....] - ETA: 16s - loss: 1.3741 - acc: 0.6677 - LRFinder: val_loss: 3.6598 - lr = 0.00901179 \n",
            "345/390 [=========================>....] - ETA: 15s - loss: 1.3744 - acc: 0.6676 - LRFinder: val_loss: 3.9421 - lr = 0.00903282 \n",
            "346/390 [=========================>....] - ETA: 15s - loss: 1.3749 - acc: 0.6674 - LRFinder: val_loss: 4.0634 - lr = 0.00905385 \n",
            "347/390 [=========================>....] - ETA: 15s - loss: 1.3750 - acc: 0.6673 - LRFinder: val_loss: 4.0717 - lr = 0.00907487 \n",
            "348/390 [=========================>....] - ETA: 14s - loss: 1.3751 - acc: 0.6673 - LRFinder: val_loss: 4.0493 - lr = 0.00909590 \n",
            "349/390 [=========================>....] - ETA: 14s - loss: 1.3748 - acc: 0.6674 - LRFinder: val_loss: 4.0986 - lr = 0.00911692 \n",
            "350/390 [=========================>....] - ETA: 14s - loss: 1.3748 - acc: 0.6674 - LRFinder: val_loss: 4.1148 - lr = 0.00913795 \n",
            "351/390 [==========================>...] - ETA: 13s - loss: 1.3749 - acc: 0.6672 - LRFinder: val_loss: 4.0911 - lr = 0.00915897 \n",
            "352/390 [==========================>...] - ETA: 13s - loss: 1.3751 - acc: 0.6671 - LRFinder: val_loss: 4.2085 - lr = 0.00918000 \n",
            "353/390 [==========================>...] - ETA: 13s - loss: 1.3755 - acc: 0.6668 - LRFinder: val_loss: 4.0199 - lr = 0.00920103 \n",
            "354/390 [==========================>...] - ETA: 12s - loss: 1.3756 - acc: 0.6669 - LRFinder: val_loss: 4.2323 - lr = 0.00922205 \n",
            "355/390 [==========================>...] - ETA: 12s - loss: 1.3751 - acc: 0.6670 - LRFinder: val_loss: 4.1307 - lr = 0.00924308 \n",
            "356/390 [==========================>...] - ETA: 11s - loss: 1.3754 - acc: 0.6669 - LRFinder: val_loss: 4.1276 - lr = 0.00926410 \n",
            "357/390 [==========================>...] - ETA: 11s - loss: 1.3757 - acc: 0.6668 - LRFinder: val_loss: 3.9769 - lr = 0.00928513 \n",
            "358/390 [==========================>...] - ETA: 11s - loss: 1.3758 - acc: 0.6667 - LRFinder: val_loss: 3.9435 - lr = 0.00930615 \n",
            "359/390 [==========================>...] - ETA: 10s - loss: 1.3761 - acc: 0.6667 - LRFinder: val_loss: 3.8750 - lr = 0.00932718 \n",
            "360/390 [==========================>...] - ETA: 10s - loss: 1.3759 - acc: 0.6668 - LRFinder: val_loss: 3.8991 - lr = 0.00934821 \n",
            "361/390 [==========================>...] - ETA: 10s - loss: 1.3759 - acc: 0.6668 - LRFinder: val_loss: 3.8941 - lr = 0.00936923 \n",
            "362/390 [==========================>...] - ETA: 9s - loss: 1.3764 - acc: 0.6666  - LRFinder: val_loss: 3.8295 - lr = 0.00939026 \n",
            "363/390 [==========================>...] - ETA: 9s - loss: 1.3766 - acc: 0.6665 - LRFinder: val_loss: 3.9922 - lr = 0.00941128 \n",
            "364/390 [===========================>..] - ETA: 9s - loss: 1.3769 - acc: 0.6664 - LRFinder: val_loss: 3.9602 - lr = 0.00943231 \n",
            "365/390 [===========================>..] - ETA: 8s - loss: 1.3771 - acc: 0.6662 - LRFinder: val_loss: 4.2746 - lr = 0.00945333 \n",
            "366/390 [===========================>..] - ETA: 8s - loss: 1.3774 - acc: 0.6661 - LRFinder: val_loss: 4.2908 - lr = 0.00947436 \n",
            "367/390 [===========================>..] - ETA: 8s - loss: 1.3773 - acc: 0.6661 - LRFinder: val_loss: 4.3277 - lr = 0.00949538 \n",
            "368/390 [===========================>..] - ETA: 7s - loss: 1.3775 - acc: 0.6660 - LRFinder: val_loss: 4.6121 - lr = 0.00951641 \n",
            "369/390 [===========================>..] - ETA: 7s - loss: 1.3773 - acc: 0.6662 - LRFinder: val_loss: 4.3122 - lr = 0.00953744 \n",
            "370/390 [===========================>..] - ETA: 7s - loss: 1.3776 - acc: 0.6662 - LRFinder: val_loss: 4.4299 - lr = 0.00955846 \n",
            "371/390 [===========================>..] - ETA: 6s - loss: 1.3776 - acc: 0.6661 - LRFinder: val_loss: 4.5077 - lr = 0.00957949 \n",
            "372/390 [===========================>..] - ETA: 6s - loss: 1.3775 - acc: 0.6661 - LRFinder: val_loss: 4.5109 - lr = 0.00960051 \n",
            "373/390 [===========================>..] - ETA: 5s - loss: 1.3772 - acc: 0.6662 - LRFinder: val_loss: 4.5425 - lr = 0.00962154 \n",
            "374/390 [===========================>..] - ETA: 5s - loss: 1.3769 - acc: 0.6663 - LRFinder: val_loss: 4.4559 - lr = 0.00964256 \n",
            "375/390 [===========================>..] - ETA: 5s - loss: 1.3772 - acc: 0.6661 - LRFinder: val_loss: 4.4214 - lr = 0.00966359 \n",
            "376/390 [===========================>..] - ETA: 4s - loss: 1.3773 - acc: 0.6662 - LRFinder: val_loss: 4.6931 - lr = 0.00968462 \n",
            "377/390 [============================>.] - ETA: 4s - loss: 1.3771 - acc: 0.6663 - LRFinder: val_loss: 4.7149 - lr = 0.00970564 \n",
            "378/390 [============================>.] - ETA: 4s - loss: 1.3771 - acc: 0.6663 - LRFinder: val_loss: 4.7622 - lr = 0.00972667 \n",
            "379/390 [============================>.] - ETA: 3s - loss: 1.3770 - acc: 0.6664 - LRFinder: val_loss: 4.5514 - lr = 0.00974769 \n",
            "380/390 [============================>.] - ETA: 3s - loss: 1.3771 - acc: 0.6664 - LRFinder: val_loss: 4.7052 - lr = 0.00976872 \n",
            "381/390 [============================>.] - ETA: 3s - loss: 1.3775 - acc: 0.6662 - LRFinder: val_loss: 4.7671 - lr = 0.00978974 \n",
            "382/390 [============================>.] - ETA: 2s - loss: 1.3775 - acc: 0.6662 - LRFinder: val_loss: 4.4356 - lr = 0.00981077 \n",
            "383/390 [============================>.] - ETA: 2s - loss: 1.3774 - acc: 0.6663 - LRFinder: val_loss: 4.3516 - lr = 0.00983179 \n",
            "384/390 [============================>.] - ETA: 2s - loss: 1.3775 - acc: 0.6662 - LRFinder: val_loss: 4.3929 - lr = 0.00985282 \n",
            "385/390 [============================>.] - ETA: 1s - loss: 1.3775 - acc: 0.6662 - LRFinder: val_loss: 4.2699 - lr = 0.00987385 \n",
            "386/390 [============================>.] - ETA: 1s - loss: 1.3775 - acc: 0.6661 - LRFinder: val_loss: 4.2477 - lr = 0.00989487 \n",
            "387/390 [============================>.] - ETA: 1s - loss: 1.3775 - acc: 0.6661 - LRFinder: val_loss: 4.2037 - lr = 0.00991590 \n",
            "388/390 [============================>.] - ETA: 0s - loss: 1.3777 - acc: 0.6660 - LRFinder: val_loss: 4.1097 - lr = 0.00993692 \n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.3778 - acc: 0.6661 - LRFinder: val_loss: 3.9054 - lr = 0.00995795 \n",
            " - LRFinder: val_loss: 3.9834 - lr = 0.00997897 \n",
            "390/390 [==============================] - 140s 360ms/step - loss: 1.3779 - acc: 0.6659 - val_loss: 4.0049 - val_acc: 0.1822\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {weights/weight_decay/weight_decay-3e-06/}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAESCAYAAAASQMmzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsvh/FOD8WDKAkXQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NpJuZrplALd",
        "colab_type": "text"
      },
      "source": [
        "#BEST DECAY IS 0.0000003"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWaEbb0BlBK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = SGD(lr=0.0018, decay=0.000003, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EecCH-q0Ege3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specifying the path to store the weights\n",
        "filepath=\"/content/gdrive/My Drive/Assignment13_v3:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=True, mode='max')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSTRlAv4KUuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from clr import OneCycleLR\n",
        "\n",
        "lr_manager = OneCycleLR(epochs=EPOCHS, batch_size=BATCH_SIZE, samples=n_train, steps=len(train_iterator), max_lr=0.0018,\n",
        "                        end_percentage=0.1, scale=100,\n",
        "                        maximum_momentum=0.95, minimum_momentum=0.9)\n",
        "\n",
        "callbacks = [checkpoint, lr_manager]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaDjaDz3szBx",
        "colab_type": "code",
        "outputId": "b0905bc6-2545-4d10-8d56-80d1639d3314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "\n",
        "model_info = model.fit_generator(train_iterator,\n",
        "                                 steps_per_epoch = len(train_iterator), nb_epoch = EPOCHS, \n",
        "                                 validation_data = validation_iterator, \n",
        "                                 validation_steps = len(validation_iterator),\n",
        "                                 verbose=1, callbacks=callbacks)\n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n",
        "# plot model history\n",
        "plot_model_history(model_info)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=196, validation_data=<keras_pre..., validation_steps=20, verbose=1, callbacks=[<keras.ca..., epochs=300)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:651: DeprecationWarning: `wait_time` is not used anymore.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "  2/196 [..............................] - ETA: 7:30 - loss: 1.3070 - acc: 0.6738 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.327911). Check your callbacks.\n",
            "  % delta_t_median)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 52s 266ms/step - loss: 1.3538 - acc: 0.6755 - val_loss: 1.2599 - val_acc: 0.7089\n",
            " - lr: 0.00003 - momentum: 0.95 \n",
            "Epoch 2/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.3361 - acc: 0.6777 - val_loss: 1.2523 - val_acc: 0.7116\n",
            " - lr: 0.00004 - momentum: 0.95 \n",
            "Epoch 3/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.3287 - acc: 0.6804 - val_loss: 1.2470 - val_acc: 0.7132\n",
            " - lr: 0.00006 - momentum: 0.95 \n",
            "Epoch 4/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 1.3243 - acc: 0.6842 - val_loss: 1.2431 - val_acc: 0.7152\n",
            " - lr: 0.00007 - momentum: 0.95 \n",
            "Epoch 5/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.3204 - acc: 0.6850 - val_loss: 1.2395 - val_acc: 0.7146\n",
            " - lr: 0.00008 - momentum: 0.95 \n",
            "Epoch 6/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.3164 - acc: 0.6876 - val_loss: 1.2367 - val_acc: 0.7169\n",
            " - lr: 0.00010 - momentum: 0.95 \n",
            "Epoch 7/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.3122 - acc: 0.6895 - val_loss: 1.2324 - val_acc: 0.7182\n",
            " - lr: 0.00011 - momentum: 0.95 \n",
            "Epoch 8/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.3131 - acc: 0.6862 - val_loss: 1.2328 - val_acc: 0.7175\n",
            " - lr: 0.00012 - momentum: 0.95 \n",
            "Epoch 9/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.3108 - acc: 0.6893 - val_loss: 1.2309 - val_acc: 0.7182\n",
            " - lr: 0.00014 - momentum: 0.95 \n",
            "Epoch 10/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.3067 - acc: 0.6905 - val_loss: 1.2286 - val_acc: 0.7205\n",
            " - lr: 0.00015 - momentum: 0.95 \n",
            "Epoch 11/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.3035 - acc: 0.6899 - val_loss: 1.2308 - val_acc: 0.7183\n",
            " - lr: 0.00016 - momentum: 0.95 \n",
            "Epoch 12/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.2991 - acc: 0.6938 - val_loss: 1.2283 - val_acc: 0.7192\n",
            " - lr: 0.00018 - momentum: 0.95 \n",
            "Epoch 13/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.3015 - acc: 0.6907 - val_loss: 1.2219 - val_acc: 0.7206\n",
            " - lr: 0.00019 - momentum: 0.95 \n",
            "Epoch 14/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.2990 - acc: 0.6916 - val_loss: 1.2202 - val_acc: 0.7210\n",
            " - lr: 0.00020 - momentum: 0.94 \n",
            "Epoch 15/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2953 - acc: 0.6928 - val_loss: 1.2201 - val_acc: 0.7221\n",
            " - lr: 0.00022 - momentum: 0.94 \n",
            "Epoch 16/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.2969 - acc: 0.6939 - val_loss: 1.2197 - val_acc: 0.7227\n",
            " - lr: 0.00023 - momentum: 0.94 \n",
            "Epoch 17/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2962 - acc: 0.6922 - val_loss: 1.2189 - val_acc: 0.7209\n",
            " - lr: 0.00024 - momentum: 0.94 \n",
            "Epoch 18/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.2927 - acc: 0.6940 - val_loss: 1.2121 - val_acc: 0.7247\n",
            " - lr: 0.00026 - momentum: 0.94 \n",
            "Epoch 19/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.2914 - acc: 0.6968 - val_loss: 1.2118 - val_acc: 0.7248\n",
            " - lr: 0.00027 - momentum: 0.94 \n",
            "Epoch 20/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2881 - acc: 0.6970 - val_loss: 1.2173 - val_acc: 0.7223\n",
            " - lr: 0.00028 - momentum: 0.94 \n",
            "Epoch 21/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.2798 - acc: 0.6997 - val_loss: 1.2104 - val_acc: 0.7250\n",
            " - lr: 0.00030 - momentum: 0.94 \n",
            "Epoch 22/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.2838 - acc: 0.6967 - val_loss: 1.2093 - val_acc: 0.7260\n",
            " - lr: 0.00031 - momentum: 0.94 \n",
            "Epoch 23/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.2821 - acc: 0.6978 - val_loss: 1.2057 - val_acc: 0.7268\n",
            " - lr: 0.00032 - momentum: 0.94 \n",
            "Epoch 24/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2742 - acc: 0.7026 - val_loss: 1.2048 - val_acc: 0.7256\n",
            " - lr: 0.00033 - momentum: 0.94 \n",
            "Epoch 25/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2795 - acc: 0.6990 - val_loss: 1.2024 - val_acc: 0.7269\n",
            " - lr: 0.00035 - momentum: 0.94 \n",
            "Epoch 26/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.2731 - acc: 0.7018 - val_loss: 1.2043 - val_acc: 0.7296\n",
            " - lr: 0.00036 - momentum: 0.94 \n",
            "Epoch 27/300\n",
            "196/196 [==============================] - 46s 235ms/step - loss: 1.2740 - acc: 0.6999 - val_loss: 1.2014 - val_acc: 0.7291\n",
            " - lr: 0.00037 - momentum: 0.94 \n",
            "Epoch 28/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2720 - acc: 0.7014 - val_loss: 1.1996 - val_acc: 0.7288\n",
            " - lr: 0.00039 - momentum: 0.94 \n",
            "Epoch 29/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2673 - acc: 0.7031 - val_loss: 1.1893 - val_acc: 0.7325\n",
            " - lr: 0.00040 - momentum: 0.94 \n",
            "Epoch 30/300\n",
            "196/196 [==============================] - 46s 235ms/step - loss: 1.2621 - acc: 0.7048 - val_loss: 1.1951 - val_acc: 0.7292\n",
            " - lr: 0.00041 - momentum: 0.94 \n",
            "Epoch 31/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2632 - acc: 0.7022 - val_loss: 1.1928 - val_acc: 0.7306\n",
            " - lr: 0.00043 - momentum: 0.94 \n",
            "Epoch 32/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2615 - acc: 0.7062 - val_loss: 1.1934 - val_acc: 0.7289\n",
            " - lr: 0.00044 - momentum: 0.94 \n",
            "Epoch 33/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2517 - acc: 0.7070 - val_loss: 1.1902 - val_acc: 0.7332\n",
            " - lr: 0.00045 - momentum: 0.94 \n",
            "Epoch 34/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2553 - acc: 0.7060 - val_loss: 1.1842 - val_acc: 0.7339\n",
            " - lr: 0.00047 - momentum: 0.94 \n",
            "Epoch 35/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2529 - acc: 0.7073 - val_loss: 1.1817 - val_acc: 0.7343\n",
            " - lr: 0.00048 - momentum: 0.94 \n",
            "Epoch 36/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2527 - acc: 0.7087 - val_loss: 1.1799 - val_acc: 0.7362\n",
            " - lr: 0.00049 - momentum: 0.94 \n",
            "Epoch 37/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2403 - acc: 0.7125 - val_loss: 1.1809 - val_acc: 0.7353\n",
            " - lr: 0.00051 - momentum: 0.94 \n",
            "Epoch 38/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2457 - acc: 0.7100 - val_loss: 1.1754 - val_acc: 0.7367\n",
            " - lr: 0.00052 - momentum: 0.94 \n",
            "Epoch 39/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2396 - acc: 0.7102 - val_loss: 1.1646 - val_acc: 0.7388\n",
            " - lr: 0.00053 - momentum: 0.94 \n",
            "Epoch 40/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2348 - acc: 0.7138 - val_loss: 1.1684 - val_acc: 0.7397\n",
            " - lr: 0.00055 - momentum: 0.94 \n",
            "Epoch 41/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2338 - acc: 0.7141 - val_loss: 1.1617 - val_acc: 0.7423\n",
            " - lr: 0.00056 - momentum: 0.93 \n",
            "Epoch 42/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2318 - acc: 0.7129 - val_loss: 1.1665 - val_acc: 0.7399\n",
            " - lr: 0.00057 - momentum: 0.93 \n",
            "Epoch 43/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2256 - acc: 0.7185 - val_loss: 1.1635 - val_acc: 0.7422\n",
            " - lr: 0.00059 - momentum: 0.93 \n",
            "Epoch 44/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2269 - acc: 0.7171 - val_loss: 1.1729 - val_acc: 0.7380\n",
            " - lr: 0.00060 - momentum: 0.93 \n",
            "Epoch 45/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 1.2235 - acc: 0.7161 - val_loss: 1.1620 - val_acc: 0.7427\n",
            " - lr: 0.00061 - momentum: 0.93 \n",
            "Epoch 46/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2174 - acc: 0.7186 - val_loss: 1.1631 - val_acc: 0.7438\n",
            " - lr: 0.00063 - momentum: 0.93 \n",
            "Epoch 47/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2125 - acc: 0.7195 - val_loss: 1.1622 - val_acc: 0.7431\n",
            " - lr: 0.00064 - momentum: 0.93 \n",
            "Epoch 48/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2117 - acc: 0.7197 - val_loss: 1.1581 - val_acc: 0.7414\n",
            " - lr: 0.00065 - momentum: 0.93 \n",
            "Epoch 49/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2093 - acc: 0.7207 - val_loss: 1.1584 - val_acc: 0.7412\n",
            " - lr: 0.00066 - momentum: 0.93 \n",
            "Epoch 50/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2072 - acc: 0.7219 - val_loss: 1.1532 - val_acc: 0.7454\n",
            " - lr: 0.00068 - momentum: 0.93 \n",
            "Epoch 51/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2048 - acc: 0.7235 - val_loss: 1.1541 - val_acc: 0.7447\n",
            " - lr: 0.00069 - momentum: 0.93 \n",
            "Epoch 52/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.2012 - acc: 0.7209 - val_loss: 1.1503 - val_acc: 0.7456\n",
            " - lr: 0.00070 - momentum: 0.93 \n",
            "Epoch 53/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1967 - acc: 0.7259 - val_loss: 1.1437 - val_acc: 0.7479\n",
            " - lr: 0.00072 - momentum: 0.93 \n",
            "Epoch 54/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1947 - acc: 0.7277 - val_loss: 1.1392 - val_acc: 0.7491\n",
            " - lr: 0.00073 - momentum: 0.93 \n",
            "Epoch 55/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1897 - acc: 0.7273 - val_loss: 1.1297 - val_acc: 0.7524\n",
            " - lr: 0.00074 - momentum: 0.93 \n",
            "Epoch 56/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1813 - acc: 0.7320 - val_loss: 1.1215 - val_acc: 0.7542\n",
            " - lr: 0.00076 - momentum: 0.93 \n",
            "Epoch 57/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1810 - acc: 0.7301 - val_loss: 1.1321 - val_acc: 0.7507\n",
            " - lr: 0.00077 - momentum: 0.93 \n",
            "Epoch 58/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1815 - acc: 0.7304 - val_loss: 1.1262 - val_acc: 0.7529\n",
            " - lr: 0.00078 - momentum: 0.93 \n",
            "Epoch 59/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1753 - acc: 0.7333 - val_loss: 1.1118 - val_acc: 0.7588\n",
            " - lr: 0.00080 - momentum: 0.93 \n",
            "Epoch 60/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1693 - acc: 0.7341 - val_loss: 1.1269 - val_acc: 0.7546\n",
            " - lr: 0.00081 - momentum: 0.93 \n",
            "Epoch 61/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1664 - acc: 0.7365 - val_loss: 1.1195 - val_acc: 0.7586\n",
            " - lr: 0.00082 - momentum: 0.93 \n",
            "Epoch 62/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1653 - acc: 0.7352 - val_loss: 1.1060 - val_acc: 0.7600\n",
            " - lr: 0.00084 - momentum: 0.93 \n",
            "Epoch 63/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1598 - acc: 0.7377 - val_loss: 1.1193 - val_acc: 0.7555\n",
            " - lr: 0.00085 - momentum: 0.93 \n",
            "Epoch 64/300\n",
            "196/196 [==============================] - 46s 235ms/step - loss: 1.1573 - acc: 0.7400 - val_loss: 1.1035 - val_acc: 0.7599\n",
            " - lr: 0.00086 - momentum: 0.93 \n",
            "Epoch 65/300\n",
            "196/196 [==============================] - 46s 234ms/step - loss: 1.1530 - acc: 0.7413 - val_loss: 1.1119 - val_acc: 0.7582\n",
            " - lr: 0.00088 - momentum: 0.93 \n",
            "Epoch 66/300\n",
            "196/196 [==============================] - 46s 235ms/step - loss: 1.1531 - acc: 0.7386 - val_loss: 1.1001 - val_acc: 0.7625\n",
            " - lr: 0.00089 - momentum: 0.93 \n",
            "Epoch 67/300\n",
            "196/196 [==============================] - 46s 233ms/step - loss: 1.1491 - acc: 0.7412 - val_loss: 1.0945 - val_acc: 0.7620\n",
            " - lr: 0.00090 - momentum: 0.93 \n",
            "Epoch 68/300\n",
            "196/196 [==============================] - 46s 233ms/step - loss: 1.1423 - acc: 0.7435 - val_loss: 1.0960 - val_acc: 0.7614\n",
            " - lr: 0.00092 - momentum: 0.92 \n",
            "Epoch 69/300\n",
            "196/196 [==============================] - 46s 234ms/step - loss: 1.1375 - acc: 0.7453 - val_loss: 1.0885 - val_acc: 0.7655\n",
            " - lr: 0.00093 - momentum: 0.92 \n",
            "Epoch 70/300\n",
            "196/196 [==============================] - 46s 234ms/step - loss: 1.1364 - acc: 0.7448 - val_loss: 1.0791 - val_acc: 0.7686\n",
            " - lr: 0.00094 - momentum: 0.92 \n",
            "Epoch 71/300\n",
            "196/196 [==============================] - 46s 235ms/step - loss: 1.1316 - acc: 0.7474 - val_loss: 1.1472 - val_acc: 0.7450\n",
            " - lr: 0.00096 - momentum: 0.92 \n",
            "Epoch 72/300\n",
            "196/196 [==============================] - 46s 235ms/step - loss: 1.1276 - acc: 0.7502 - val_loss: 1.0962 - val_acc: 0.7615\n",
            " - lr: 0.00097 - momentum: 0.92 \n",
            "Epoch 73/300\n",
            "196/196 [==============================] - 46s 235ms/step - loss: 1.1227 - acc: 0.7508 - val_loss: 1.0724 - val_acc: 0.7720\n",
            " - lr: 0.00098 - momentum: 0.92 \n",
            "Epoch 74/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1181 - acc: 0.7494 - val_loss: 1.0762 - val_acc: 0.7687\n",
            " - lr: 0.00099 - momentum: 0.92 \n",
            "Epoch 75/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1125 - acc: 0.7535 - val_loss: 1.0658 - val_acc: 0.7751\n",
            " - lr: 0.00101 - momentum: 0.92 \n",
            "Epoch 76/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1174 - acc: 0.7504 - val_loss: 1.0752 - val_acc: 0.7715\n",
            " - lr: 0.00102 - momentum: 0.92 \n",
            "Epoch 77/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1126 - acc: 0.7522 - val_loss: 1.0573 - val_acc: 0.7750\n",
            " - lr: 0.00103 - momentum: 0.92 \n",
            "Epoch 78/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1051 - acc: 0.7550 - val_loss: 1.0538 - val_acc: 0.7756\n",
            " - lr: 0.00105 - momentum: 0.92 \n",
            "Epoch 79/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.1033 - acc: 0.7555 - val_loss: 1.0546 - val_acc: 0.7783\n",
            " - lr: 0.00106 - momentum: 0.92 \n",
            "Epoch 80/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0998 - acc: 0.7568 - val_loss: 1.0409 - val_acc: 0.7805\n",
            " - lr: 0.00107 - momentum: 0.92 \n",
            "Epoch 81/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0972 - acc: 0.7570 - val_loss: 1.0596 - val_acc: 0.7735\n",
            " - lr: 0.00109 - momentum: 0.92 \n",
            "Epoch 82/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0904 - acc: 0.7597 - val_loss: 1.0493 - val_acc: 0.7801\n",
            " - lr: 0.00110 - momentum: 0.92 \n",
            "Epoch 83/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0853 - acc: 0.7624 - val_loss: 1.0660 - val_acc: 0.7765\n",
            " - lr: 0.00111 - momentum: 0.92 \n",
            "Epoch 84/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0831 - acc: 0.7627 - val_loss: 1.0393 - val_acc: 0.7837\n",
            " - lr: 0.00113 - momentum: 0.92 \n",
            "Epoch 85/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0744 - acc: 0.7655 - val_loss: 1.0679 - val_acc: 0.7738\n",
            " - lr: 0.00114 - momentum: 0.92 \n",
            "Epoch 86/300\n",
            "196/196 [==============================] - 46s 235ms/step - loss: 1.0816 - acc: 0.7618 - val_loss: 1.0429 - val_acc: 0.7841\n",
            " - lr: 0.00115 - momentum: 0.92 \n",
            "Epoch 87/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0731 - acc: 0.7664 - val_loss: 1.0549 - val_acc: 0.7794\n",
            " - lr: 0.00117 - momentum: 0.92 \n",
            "Epoch 88/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0749 - acc: 0.7661 - val_loss: 1.0244 - val_acc: 0.7883\n",
            " - lr: 0.00118 - momentum: 0.92 \n",
            "Epoch 89/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0633 - acc: 0.7679 - val_loss: 1.0307 - val_acc: 0.7867\n",
            " - lr: 0.00119 - momentum: 0.92 \n",
            "Epoch 90/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0573 - acc: 0.7702 - val_loss: 1.0416 - val_acc: 0.7799\n",
            " - lr: 0.00121 - momentum: 0.92 \n",
            "Epoch 91/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0545 - acc: 0.7737 - val_loss: 1.0565 - val_acc: 0.7789\n",
            " - lr: 0.00122 - momentum: 0.92 \n",
            "Epoch 92/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0553 - acc: 0.7702 - val_loss: 1.0204 - val_acc: 0.7875\n",
            " - lr: 0.00123 - momentum: 0.92 \n",
            "Epoch 93/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0498 - acc: 0.7709 - val_loss: 1.0402 - val_acc: 0.7829\n",
            " - lr: 0.00125 - momentum: 0.92 \n",
            "Epoch 94/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0434 - acc: 0.7770 - val_loss: 1.0316 - val_acc: 0.7862\n",
            " - lr: 0.00126 - momentum: 0.92 \n",
            "Epoch 95/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0434 - acc: 0.7757 - val_loss: 1.0024 - val_acc: 0.7924\n",
            " - lr: 0.00127 - momentum: 0.91 \n",
            "Epoch 96/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0322 - acc: 0.7785 - val_loss: 1.0181 - val_acc: 0.7902\n",
            " - lr: 0.00129 - momentum: 0.91 \n",
            "Epoch 97/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0304 - acc: 0.7807 - val_loss: 0.9971 - val_acc: 0.7949\n",
            " - lr: 0.00130 - momentum: 0.91 \n",
            "Epoch 98/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0264 - acc: 0.7814 - val_loss: 1.0215 - val_acc: 0.7890\n",
            " - lr: 0.00131 - momentum: 0.91 \n",
            "Epoch 99/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0266 - acc: 0.7809 - val_loss: 0.9876 - val_acc: 0.8004\n",
            " - lr: 0.00132 - momentum: 0.91 \n",
            "Epoch 100/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0217 - acc: 0.7823 - val_loss: 1.0341 - val_acc: 0.7886\n",
            " - lr: 0.00134 - momentum: 0.91 \n",
            "Epoch 101/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0161 - acc: 0.7854 - val_loss: 0.9950 - val_acc: 0.7986\n",
            " - lr: 0.00135 - momentum: 0.91 \n",
            "Epoch 102/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0089 - acc: 0.7859 - val_loss: 0.9956 - val_acc: 0.7983\n",
            " - lr: 0.00136 - momentum: 0.91 \n",
            "Epoch 103/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0114 - acc: 0.7835 - val_loss: 1.0125 - val_acc: 0.7931\n",
            " - lr: 0.00138 - momentum: 0.91 \n",
            "Epoch 104/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 1.0067 - acc: 0.7867 - val_loss: 1.0224 - val_acc: 0.7885\n",
            " - lr: 0.00139 - momentum: 0.91 \n",
            "Epoch 105/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.9987 - acc: 0.7889 - val_loss: 1.0046 - val_acc: 0.7985\n",
            " - lr: 0.00140 - momentum: 0.91 \n",
            "Epoch 106/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 0.9970 - acc: 0.7898 - val_loss: 1.0170 - val_acc: 0.7916\n",
            " - lr: 0.00142 - momentum: 0.91 \n",
            "Epoch 107/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.9933 - acc: 0.7915 - val_loss: 0.9769 - val_acc: 0.8014\n",
            " - lr: 0.00143 - momentum: 0.91 \n",
            "Epoch 108/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 0.9880 - acc: 0.7949 - val_loss: 0.9740 - val_acc: 0.8061\n",
            " - lr: 0.00144 - momentum: 0.91 \n",
            "Epoch 109/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.9840 - acc: 0.7938 - val_loss: 0.9872 - val_acc: 0.7994\n",
            " - lr: 0.00146 - momentum: 0.91 \n",
            "Epoch 110/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 0.9829 - acc: 0.7936 - val_loss: 0.9621 - val_acc: 0.8038\n",
            " - lr: 0.00147 - momentum: 0.91 \n",
            "Epoch 111/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 0.9765 - acc: 0.7970 - val_loss: 0.9662 - val_acc: 0.8033\n",
            " - lr: 0.00148 - momentum: 0.91 \n",
            "Epoch 112/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 0.9758 - acc: 0.7961 - val_loss: 0.9776 - val_acc: 0.8046\n",
            " - lr: 0.00150 - momentum: 0.91 \n",
            "Epoch 113/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 0.9733 - acc: 0.7973 - val_loss: 0.9630 - val_acc: 0.8058\n",
            " - lr: 0.00151 - momentum: 0.91 \n",
            "Epoch 114/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 0.9668 - acc: 0.8001 - val_loss: 0.9516 - val_acc: 0.8120\n",
            " - lr: 0.00152 - momentum: 0.91 \n",
            "Epoch 115/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.9609 - acc: 0.8003 - val_loss: 0.9610 - val_acc: 0.8090\n",
            " - lr: 0.00154 - momentum: 0.91 \n",
            "Epoch 116/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.9562 - acc: 0.8023 - val_loss: 0.9491 - val_acc: 0.8131\n",
            " - lr: 0.00155 - momentum: 0.91 \n",
            "Epoch 117/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.9539 - acc: 0.8027 - val_loss: 0.9389 - val_acc: 0.8145\n",
            " - lr: 0.00156 - momentum: 0.91 \n",
            "Epoch 118/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.9552 - acc: 0.8043 - val_loss: 0.9503 - val_acc: 0.8079\n",
            " - lr: 0.00158 - momentum: 0.91 \n",
            "Epoch 119/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.9506 - acc: 0.8066 - val_loss: 0.9536 - val_acc: 0.8075\n",
            " - lr: 0.00159 - momentum: 0.91 \n",
            "Epoch 120/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.9434 - acc: 0.8077 - val_loss: 0.9416 - val_acc: 0.8123\n",
            " - lr: 0.00160 - momentum: 0.91 \n",
            "Epoch 121/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.9428 - acc: 0.8068 - val_loss: 0.9700 - val_acc: 0.8058\n",
            " - lr: 0.00162 - momentum: 0.91 \n",
            "Epoch 122/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.9410 - acc: 0.8078 - val_loss: 0.9305 - val_acc: 0.8188\n",
            " - lr: 0.00163 - momentum: 0.90 \n",
            "Epoch 123/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 0.9318 - acc: 0.8095 - val_loss: 0.9420 - val_acc: 0.8112\n",
            " - lr: 0.00164 - momentum: 0.90 \n",
            "Epoch 124/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 0.9337 - acc: 0.8095 - val_loss: 0.9301 - val_acc: 0.8180\n",
            " - lr: 0.00165 - momentum: 0.90 \n",
            "Epoch 125/300\n",
            "196/196 [==============================] - 46s 235ms/step - loss: 0.9278 - acc: 0.8111 - val_loss: 0.9758 - val_acc: 0.8031\n",
            " - lr: 0.00167 - momentum: 0.90 \n",
            "Epoch 126/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 0.9226 - acc: 0.8138 - val_loss: 0.9679 - val_acc: 0.8055\n",
            " - lr: 0.00168 - momentum: 0.90 \n",
            "Epoch 127/300\n",
            "196/196 [==============================] - 46s 235ms/step - loss: 0.9262 - acc: 0.8102 - val_loss: 0.9608 - val_acc: 0.8102\n",
            " - lr: 0.00169 - momentum: 0.90 \n",
            "Epoch 128/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 0.9167 - acc: 0.8149 - val_loss: 0.9153 - val_acc: 0.8230\n",
            " - lr: 0.00171 - momentum: 0.90 \n",
            "Epoch 129/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.9101 - acc: 0.8173 - val_loss: 0.9260 - val_acc: 0.8189\n",
            " - lr: 0.00172 - momentum: 0.90 \n",
            "Epoch 130/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.9115 - acc: 0.8165 - val_loss: 0.9559 - val_acc: 0.8110\n",
            " - lr: 0.00173 - momentum: 0.90 \n",
            "Epoch 131/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.9051 - acc: 0.8171 - val_loss: 0.9217 - val_acc: 0.8203\n",
            " - lr: 0.00175 - momentum: 0.90 \n",
            "Epoch 132/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.9052 - acc: 0.8175 - val_loss: 0.9209 - val_acc: 0.8195\n",
            " - lr: 0.00176 - momentum: 0.90 \n",
            "Epoch 133/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.8993 - acc: 0.8191 - val_loss: 0.9133 - val_acc: 0.8233\n",
            " - lr: 0.00177 - momentum: 0.90 \n",
            "Epoch 134/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.9008 - acc: 0.8193 - val_loss: 0.9381 - val_acc: 0.8130\n",
            " - lr: 0.00179 - momentum: 0.90 \n",
            "Epoch 135/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 0.8934 - acc: 0.8232 - val_loss: 0.9131 - val_acc: 0.8225\n",
            " - lr: 0.00180 - momentum: 0.90 \n",
            "Epoch 136/300\n",
            "196/196 [==============================] - 46s 236ms/step - loss: 2.2574 - acc: 0.3294 - val_loss: 3.3089 - val_acc: 0.1450\n",
            " - lr: 0.17867 - momentum: 0.90 \n",
            "Epoch 137/300\n",
            "196/196 [==============================] - 47s 242ms/step - loss: 1.7422 - acc: 0.5044 - val_loss: 4.1984 - val_acc: 0.1389\n",
            " - lr: 0.17734 - momentum: 0.90 \n",
            "Epoch 138/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 1.4438 - acc: 0.6051 - val_loss: 2.0545 - val_acc: 0.4374\n",
            " - lr: 0.17601 - momentum: 0.90 \n",
            "Epoch 139/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 1.2731 - acc: 0.6570 - val_loss: 2.2974 - val_acc: 0.4123\n",
            " - lr: 0.17467 - momentum: 0.90 \n",
            "Epoch 140/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 1.1603 - acc: 0.6921 - val_loss: 1.3357 - val_acc: 0.6359\n",
            " - lr: 0.17334 - momentum: 0.90 \n",
            "Epoch 141/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 1.0571 - acc: 0.7244 - val_loss: 1.1683 - val_acc: 0.6911\n",
            " - lr: 0.17201 - momentum: 0.90 \n",
            "Epoch 142/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.9739 - acc: 0.7497 - val_loss: 1.2511 - val_acc: 0.6675\n",
            " - lr: 0.17067 - momentum: 0.90 \n",
            "Epoch 143/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.9112 - acc: 0.7680 - val_loss: 1.0402 - val_acc: 0.7157\n",
            " - lr: 0.16934 - momentum: 0.90 \n",
            "Epoch 144/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.8526 - acc: 0.7872 - val_loss: 0.9238 - val_acc: 0.7676\n",
            " - lr: 0.16801 - momentum: 0.90 \n",
            "Epoch 145/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.8079 - acc: 0.8002 - val_loss: 1.0796 - val_acc: 0.7379\n",
            " - lr: 0.16667 - momentum: 0.90 \n",
            "Epoch 146/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.7763 - acc: 0.8088 - val_loss: 1.2109 - val_acc: 0.6824\n",
            " - lr: 0.16534 - momentum: 0.90 \n",
            "Epoch 147/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.7440 - acc: 0.8170 - val_loss: 0.9145 - val_acc: 0.7700\n",
            " - lr: 0.16401 - momentum: 0.90 \n",
            "Epoch 148/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.7217 - acc: 0.8240 - val_loss: 0.8807 - val_acc: 0.7736\n",
            " - lr: 0.16268 - momentum: 0.90 \n",
            "Epoch 149/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.6959 - acc: 0.8345 - val_loss: 1.0430 - val_acc: 0.7468\n",
            " - lr: 0.16134 - momentum: 0.91 \n",
            "Epoch 150/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.6775 - acc: 0.8393 - val_loss: 0.9318 - val_acc: 0.7741\n",
            " - lr: 0.16001 - momentum: 0.91 \n",
            "Epoch 151/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.6620 - acc: 0.8437 - val_loss: 0.9492 - val_acc: 0.7622\n",
            " - lr: 0.15868 - momentum: 0.91 \n",
            "Epoch 152/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.6465 - acc: 0.8458 - val_loss: 1.2985 - val_acc: 0.6916\n",
            " - lr: 0.15734 - momentum: 0.91 \n",
            "Epoch 153/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.6326 - acc: 0.8513 - val_loss: 0.6834 - val_acc: 0.8414\n",
            " - lr: 0.15601 - momentum: 0.91 \n",
            "Epoch 154/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.6225 - acc: 0.8552 - val_loss: 0.7870 - val_acc: 0.8084\n",
            " - lr: 0.15468 - momentum: 0.91 \n",
            "Epoch 155/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.6109 - acc: 0.8579 - val_loss: 0.8215 - val_acc: 0.7972\n",
            " - lr: 0.15334 - momentum: 0.91 \n",
            "Epoch 156/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.6037 - acc: 0.8610 - val_loss: 0.8833 - val_acc: 0.7788\n",
            " - lr: 0.15201 - momentum: 0.91 \n",
            "Epoch 157/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.5927 - acc: 0.8642 - val_loss: 0.6886 - val_acc: 0.8438\n",
            " - lr: 0.15068 - momentum: 0.91 \n",
            "Epoch 158/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.5843 - acc: 0.8689 - val_loss: 0.8616 - val_acc: 0.7977\n",
            " - lr: 0.14934 - momentum: 0.91 \n",
            "Epoch 159/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.5745 - acc: 0.8720 - val_loss: 1.0167 - val_acc: 0.7619\n",
            " - lr: 0.14801 - momentum: 0.91 \n",
            "Epoch 160/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.5721 - acc: 0.8713 - val_loss: 0.8355 - val_acc: 0.8006\n",
            " - lr: 0.14668 - momentum: 0.91 \n",
            "Epoch 161/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.5683 - acc: 0.8736 - val_loss: 0.7832 - val_acc: 0.8170\n",
            " - lr: 0.14534 - momentum: 0.91 \n",
            "Epoch 162/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.5616 - acc: 0.8771 - val_loss: 0.9481 - val_acc: 0.7757\n",
            " - lr: 0.14401 - momentum: 0.91 \n",
            "Epoch 163/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.5496 - acc: 0.8799 - val_loss: 0.9240 - val_acc: 0.7601\n",
            " - lr: 0.14268 - momentum: 0.91 \n",
            "Epoch 164/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.5507 - acc: 0.8814 - val_loss: 0.8729 - val_acc: 0.7925\n",
            " - lr: 0.14134 - momentum: 0.91 \n",
            "Epoch 165/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.5464 - acc: 0.8839 - val_loss: 1.1257 - val_acc: 0.7428\n",
            " - lr: 0.14001 - momentum: 0.91 \n",
            "Epoch 166/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.5367 - acc: 0.8851 - val_loss: 0.9043 - val_acc: 0.7912\n",
            " - lr: 0.13868 - momentum: 0.91 \n",
            "Epoch 167/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.5317 - acc: 0.8888 - val_loss: 0.8030 - val_acc: 0.8164\n",
            " - lr: 0.13734 - momentum: 0.91 \n",
            "Epoch 168/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.5275 - acc: 0.8894 - val_loss: 0.7719 - val_acc: 0.8258\n",
            " - lr: 0.13601 - momentum: 0.91 \n",
            "Epoch 169/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.5283 - acc: 0.8896 - val_loss: 0.7815 - val_acc: 0.8224\n",
            " - lr: 0.13468 - momentum: 0.91 \n",
            "Epoch 170/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.5256 - acc: 0.8899 - val_loss: 0.7811 - val_acc: 0.8140\n",
            " - lr: 0.13334 - momentum: 0.91 \n",
            "Epoch 171/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.5239 - acc: 0.8910 - val_loss: 0.7739 - val_acc: 0.8223\n",
            " - lr: 0.13201 - momentum: 0.91 \n",
            "Epoch 172/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.5166 - acc: 0.8934 - val_loss: 0.6832 - val_acc: 0.8501\n",
            " - lr: 0.13068 - momentum: 0.91 \n",
            "Epoch 173/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.5151 - acc: 0.8953 - val_loss: 0.9294 - val_acc: 0.7861\n",
            " - lr: 0.12935 - momentum: 0.91 \n",
            "Epoch 174/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.5134 - acc: 0.8954 - val_loss: 0.9075 - val_acc: 0.7982\n",
            " - lr: 0.12801 - momentum: 0.91 \n",
            "Epoch 175/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.5111 - acc: 0.8960 - val_loss: 0.6409 - val_acc: 0.8636\n",
            " - lr: 0.12668 - momentum: 0.91 \n",
            "Epoch 176/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.5078 - acc: 0.8983 - val_loss: 0.7695 - val_acc: 0.8284\n",
            " - lr: 0.12535 - momentum: 0.92 \n",
            "Epoch 177/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.5049 - acc: 0.8991 - val_loss: 0.9580 - val_acc: 0.7723\n",
            " - lr: 0.12401 - momentum: 0.92 \n",
            "Epoch 178/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4958 - acc: 0.9034 - val_loss: 0.6233 - val_acc: 0.8694\n",
            " - lr: 0.12268 - momentum: 0.92 \n",
            "Epoch 179/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4964 - acc: 0.9011 - val_loss: 0.6995 - val_acc: 0.8518\n",
            " - lr: 0.12135 - momentum: 0.92 \n",
            "Epoch 180/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4973 - acc: 0.9030 - val_loss: 0.6437 - val_acc: 0.8650\n",
            " - lr: 0.12001 - momentum: 0.92 \n",
            "Epoch 181/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.4975 - acc: 0.9026 - val_loss: 0.9624 - val_acc: 0.7824\n",
            " - lr: 0.11868 - momentum: 0.92 \n",
            "Epoch 182/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4905 - acc: 0.9029 - val_loss: 0.9528 - val_acc: 0.8000\n",
            " - lr: 0.11735 - momentum: 0.92 \n",
            "Epoch 183/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.4938 - acc: 0.9035 - val_loss: 0.7053 - val_acc: 0.8388\n",
            " - lr: 0.11601 - momentum: 0.92 \n",
            "Epoch 184/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4876 - acc: 0.9069 - val_loss: 0.6636 - val_acc: 0.8635\n",
            " - lr: 0.11468 - momentum: 0.92 \n",
            "Epoch 185/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4865 - acc: 0.9063 - val_loss: 0.8447 - val_acc: 0.8119\n",
            " - lr: 0.11335 - momentum: 0.92 \n",
            "Epoch 186/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4795 - acc: 0.9103 - val_loss: 0.6410 - val_acc: 0.8678\n",
            " - lr: 0.11201 - momentum: 0.92 \n",
            "Epoch 187/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4782 - acc: 0.9094 - val_loss: 0.9561 - val_acc: 0.7790\n",
            " - lr: 0.11068 - momentum: 0.92 \n",
            "Epoch 188/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4773 - acc: 0.9103 - val_loss: 0.9976 - val_acc: 0.7871\n",
            " - lr: 0.10935 - momentum: 0.92 \n",
            "Epoch 189/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4749 - acc: 0.9122 - val_loss: 0.8603 - val_acc: 0.8184\n",
            " - lr: 0.10801 - momentum: 0.92 \n",
            "Epoch 190/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4763 - acc: 0.9106 - val_loss: 0.7776 - val_acc: 0.8227\n",
            " - lr: 0.10668 - momentum: 0.92 \n",
            "Epoch 191/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.4701 - acc: 0.9138 - val_loss: 0.6791 - val_acc: 0.8624\n",
            " - lr: 0.10535 - momentum: 0.92 \n",
            "Epoch 192/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4724 - acc: 0.9115 - val_loss: 0.7270 - val_acc: 0.8408\n",
            " - lr: 0.10401 - momentum: 0.92 \n",
            "Epoch 193/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4609 - acc: 0.9167 - val_loss: 0.8854 - val_acc: 0.8148\n",
            " - lr: 0.10268 - momentum: 0.92 \n",
            "Epoch 194/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.4663 - acc: 0.9156 - val_loss: 0.7049 - val_acc: 0.8545\n",
            " - lr: 0.10135 - momentum: 0.92 \n",
            "Epoch 195/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.4571 - acc: 0.9179 - val_loss: 0.8415 - val_acc: 0.8133\n",
            " - lr: 0.10001 - momentum: 0.92 \n",
            "Epoch 196/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.4592 - acc: 0.9176 - val_loss: 0.8003 - val_acc: 0.8180\n",
            " - lr: 0.09868 - momentum: 0.92 \n",
            "Epoch 197/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.4543 - acc: 0.9186 - val_loss: 0.8055 - val_acc: 0.8332\n",
            " - lr: 0.09735 - momentum: 0.92 \n",
            "Epoch 198/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.4539 - acc: 0.9196 - val_loss: 0.7348 - val_acc: 0.8485\n",
            " - lr: 0.09602 - momentum: 0.92 \n",
            "Epoch 199/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4491 - acc: 0.9202 - val_loss: 0.8714 - val_acc: 0.8171\n",
            " - lr: 0.09468 - momentum: 0.92 \n",
            "Epoch 200/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4436 - acc: 0.9219 - val_loss: 0.7481 - val_acc: 0.8407\n",
            " - lr: 0.09335 - momentum: 0.92 \n",
            "Epoch 201/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4438 - acc: 0.9218 - val_loss: 0.8386 - val_acc: 0.8246\n",
            " - lr: 0.09202 - momentum: 0.92 \n",
            "Epoch 202/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4417 - acc: 0.9222 - val_loss: 0.6822 - val_acc: 0.8580\n",
            " - lr: 0.09068 - momentum: 0.92 \n",
            "Epoch 203/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4378 - acc: 0.9247 - val_loss: 0.9361 - val_acc: 0.8043\n",
            " - lr: 0.08935 - momentum: 0.93 \n",
            "Epoch 204/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4360 - acc: 0.9249 - val_loss: 0.6213 - val_acc: 0.8764\n",
            " - lr: 0.08802 - momentum: 0.93 \n",
            "Epoch 205/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4372 - acc: 0.9237 - val_loss: 1.1072 - val_acc: 0.7851\n",
            " - lr: 0.08668 - momentum: 0.93 \n",
            "Epoch 206/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4256 - acc: 0.9274 - val_loss: 0.6649 - val_acc: 0.8661\n",
            " - lr: 0.08535 - momentum: 0.93 \n",
            "Epoch 207/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.4347 - acc: 0.9239 - val_loss: 0.7010 - val_acc: 0.8550\n",
            " - lr: 0.08402 - momentum: 0.93 \n",
            "Epoch 208/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.4256 - acc: 0.9268 - val_loss: 0.6060 - val_acc: 0.8814\n",
            " - lr: 0.08268 - momentum: 0.93 \n",
            "Epoch 209/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4255 - acc: 0.9281 - val_loss: 0.7003 - val_acc: 0.8560\n",
            " - lr: 0.08135 - momentum: 0.93 \n",
            "Epoch 210/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4240 - acc: 0.9274 - val_loss: 0.8931 - val_acc: 0.8051\n",
            " - lr: 0.08002 - momentum: 0.93 \n",
            "Epoch 211/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4199 - acc: 0.9294 - val_loss: 0.6491 - val_acc: 0.8716\n",
            " - lr: 0.07868 - momentum: 0.93 \n",
            "Epoch 212/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.4129 - acc: 0.9317 - val_loss: 0.5653 - val_acc: 0.8942\n",
            " - lr: 0.07735 - momentum: 0.93 \n",
            "Epoch 213/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.4124 - acc: 0.9320 - val_loss: 0.5711 - val_acc: 0.8907\n",
            " - lr: 0.07602 - momentum: 0.93 \n",
            "Epoch 214/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.4083 - acc: 0.9317 - val_loss: 0.6159 - val_acc: 0.8836\n",
            " - lr: 0.07468 - momentum: 0.93 \n",
            "Epoch 215/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.4054 - acc: 0.9335 - val_loss: 0.7254 - val_acc: 0.8539\n",
            " - lr: 0.07335 - momentum: 0.93 \n",
            "Epoch 216/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.4077 - acc: 0.9330 - val_loss: 0.5996 - val_acc: 0.8850\n",
            " - lr: 0.07202 - momentum: 0.93 \n",
            "Epoch 217/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.4047 - acc: 0.9329 - val_loss: 0.7000 - val_acc: 0.8599\n",
            " - lr: 0.07068 - momentum: 0.93 \n",
            "Epoch 218/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.4043 - acc: 0.9332 - val_loss: 0.7201 - val_acc: 0.8533\n",
            " - lr: 0.06935 - momentum: 0.93 \n",
            "Epoch 219/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.3930 - acc: 0.9372 - val_loss: 0.7194 - val_acc: 0.8533\n",
            " - lr: 0.06802 - momentum: 0.93 \n",
            "Epoch 220/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.3920 - acc: 0.9366 - val_loss: 0.6176 - val_acc: 0.8765\n",
            " - lr: 0.06668 - momentum: 0.93 \n",
            "Epoch 221/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.3889 - acc: 0.9382 - val_loss: 0.7133 - val_acc: 0.8609\n",
            " - lr: 0.06535 - momentum: 0.93 \n",
            "Epoch 222/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.3823 - acc: 0.9398 - val_loss: 0.5631 - val_acc: 0.8883\n",
            " - lr: 0.06402 - momentum: 0.93 \n",
            "Epoch 223/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.3789 - acc: 0.9406 - val_loss: 0.6888 - val_acc: 0.8606\n",
            " - lr: 0.06269 - momentum: 0.93 \n",
            "Epoch 224/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.3845 - acc: 0.9379 - val_loss: 0.6699 - val_acc: 0.8619\n",
            " - lr: 0.06135 - momentum: 0.93 \n",
            "Epoch 225/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.3807 - acc: 0.9396 - val_loss: 0.6180 - val_acc: 0.8811\n",
            " - lr: 0.06002 - momentum: 0.93 \n",
            "Epoch 226/300\n",
            "196/196 [==============================] - 46s 237ms/step - loss: 0.3700 - acc: 0.9430 - val_loss: 0.6285 - val_acc: 0.8784\n",
            " - lr: 0.05869 - momentum: 0.93 \n",
            "Epoch 227/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.3711 - acc: 0.9417 - val_loss: 0.7460 - val_acc: 0.8504\n",
            " - lr: 0.05735 - momentum: 0.93 \n",
            "Epoch 228/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.3662 - acc: 0.9445 - val_loss: 0.6669 - val_acc: 0.8810\n",
            " - lr: 0.05602 - momentum: 0.93 \n",
            "Epoch 229/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.3611 - acc: 0.9450 - val_loss: 0.5699 - val_acc: 0.8940\n",
            " - lr: 0.05469 - momentum: 0.93 \n",
            "Epoch 230/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.3572 - acc: 0.9467 - val_loss: 0.5698 - val_acc: 0.8942\n",
            " - lr: 0.05335 - momentum: 0.94 \n",
            "Epoch 231/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.3572 - acc: 0.9445 - val_loss: 0.5368 - val_acc: 0.9050\n",
            " - lr: 0.05202 - momentum: 0.94 \n",
            "Epoch 232/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.3499 - acc: 0.9465 - val_loss: 0.7271 - val_acc: 0.8611\n",
            " - lr: 0.05069 - momentum: 0.94 \n",
            "Epoch 233/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.3437 - acc: 0.9494 - val_loss: 0.6378 - val_acc: 0.8795\n",
            " - lr: 0.04935 - momentum: 0.94 \n",
            "Epoch 234/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.3444 - acc: 0.9481 - val_loss: 0.5309 - val_acc: 0.9024\n",
            " - lr: 0.04802 - momentum: 0.94 \n",
            "Epoch 235/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.3380 - acc: 0.9501 - val_loss: 0.6401 - val_acc: 0.8730\n",
            " - lr: 0.04669 - momentum: 0.94 \n",
            "Epoch 236/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.3333 - acc: 0.9521 - val_loss: 0.9335 - val_acc: 0.8277\n",
            " - lr: 0.04535 - momentum: 0.94 \n",
            "Epoch 237/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.3325 - acc: 0.9520 - val_loss: 0.5854 - val_acc: 0.8896\n",
            " - lr: 0.04402 - momentum: 0.94 \n",
            "Epoch 238/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.3283 - acc: 0.9530 - val_loss: 0.6287 - val_acc: 0.8817\n",
            " - lr: 0.04269 - momentum: 0.94 \n",
            "Epoch 239/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.3197 - acc: 0.9549 - val_loss: 0.6185 - val_acc: 0.8820\n",
            " - lr: 0.04135 - momentum: 0.94 \n",
            "Epoch 240/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.3209 - acc: 0.9541 - val_loss: 0.5681 - val_acc: 0.8915\n",
            " - lr: 0.04002 - momentum: 0.94 \n",
            "Epoch 241/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.3120 - acc: 0.9559 - val_loss: 0.5316 - val_acc: 0.9015\n",
            " - lr: 0.03869 - momentum: 0.94 \n",
            "Epoch 242/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.3074 - acc: 0.9588 - val_loss: 0.6060 - val_acc: 0.8825\n",
            " - lr: 0.03735 - momentum: 0.94 \n",
            "Epoch 243/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.3025 - acc: 0.9592 - val_loss: 0.5207 - val_acc: 0.9055\n",
            " - lr: 0.03602 - momentum: 0.94 \n",
            "Epoch 244/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.2990 - acc: 0.9583 - val_loss: 0.5065 - val_acc: 0.9075\n",
            " - lr: 0.03469 - momentum: 0.94 \n",
            "Epoch 245/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.2983 - acc: 0.9593 - val_loss: 0.5561 - val_acc: 0.8987\n",
            " - lr: 0.03335 - momentum: 0.94 \n",
            "Epoch 246/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.2911 - acc: 0.9617 - val_loss: 0.5113 - val_acc: 0.9149\n",
            " - lr: 0.03202 - momentum: 0.94 \n",
            "Epoch 247/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.2883 - acc: 0.9626 - val_loss: 0.4814 - val_acc: 0.9179\n",
            " - lr: 0.03069 - momentum: 0.94 \n",
            "Epoch 248/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.2842 - acc: 0.9629 - val_loss: 0.5154 - val_acc: 0.9123\n",
            " - lr: 0.02936 - momentum: 0.94 \n",
            "Epoch 249/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.2776 - acc: 0.9655 - val_loss: 0.4769 - val_acc: 0.9156\n",
            " - lr: 0.02802 - momentum: 0.94 \n",
            "Epoch 250/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.2708 - acc: 0.9665 - val_loss: 0.4707 - val_acc: 0.9153\n",
            " - lr: 0.02669 - momentum: 0.94 \n",
            "Epoch 251/300\n",
            "196/196 [==============================] - 47s 237ms/step - loss: 0.2616 - acc: 0.9691 - val_loss: 0.5390 - val_acc: 0.9065\n",
            " - lr: 0.02536 - momentum: 0.94 \n",
            "Epoch 252/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.2577 - acc: 0.9697 - val_loss: 0.4834 - val_acc: 0.9133\n",
            " - lr: 0.02402 - momentum: 0.94 \n",
            "Epoch 253/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.2517 - acc: 0.9716 - val_loss: 0.4674 - val_acc: 0.9193\n",
            " - lr: 0.02269 - momentum: 0.94 \n",
            "Epoch 254/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.2533 - acc: 0.9697 - val_loss: 0.4862 - val_acc: 0.9150\n",
            " - lr: 0.02136 - momentum: 0.94 \n",
            "Epoch 255/300\n",
            "196/196 [==============================] - 47s 240ms/step - loss: 0.2408 - acc: 0.9743 - val_loss: 0.4643 - val_acc: 0.9186\n",
            " - lr: 0.02002 - momentum: 0.94 \n",
            "Epoch 256/300\n",
            "196/196 [==============================] - 47s 240ms/step - loss: 0.2374 - acc: 0.9745 - val_loss: 0.4287 - val_acc: 0.9298\n",
            " - lr: 0.01869 - momentum: 0.94 \n",
            "Epoch 257/300\n",
            "196/196 [==============================] - 47s 240ms/step - loss: 0.2284 - acc: 0.9768 - val_loss: 0.4232 - val_acc: 0.9293\n",
            " - lr: 0.01736 - momentum: 0.95 \n",
            "Epoch 258/300\n",
            "196/196 [==============================] - 47s 241ms/step - loss: 0.2280 - acc: 0.9767 - val_loss: 0.4470 - val_acc: 0.9263\n",
            " - lr: 0.01602 - momentum: 0.95 \n",
            "Epoch 259/300\n",
            "196/196 [==============================] - 47s 241ms/step - loss: 0.2210 - acc: 0.9783 - val_loss: 0.4958 - val_acc: 0.9112\n",
            " - lr: 0.01469 - momentum: 0.95 \n",
            "Epoch 260/300\n",
            "196/196 [==============================] - 47s 240ms/step - loss: 0.2146 - acc: 0.9803 - val_loss: 0.4147 - val_acc: 0.9319\n",
            " - lr: 0.01336 - momentum: 0.95 \n",
            "Epoch 261/300\n",
            "196/196 [==============================] - 47s 241ms/step - loss: 0.2114 - acc: 0.9803 - val_loss: 0.4425 - val_acc: 0.9276\n",
            " - lr: 0.01202 - momentum: 0.95 \n",
            "Epoch 262/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.2039 - acc: 0.9823 - val_loss: 0.3953 - val_acc: 0.9355\n",
            " - lr: 0.01069 - momentum: 0.95 \n",
            "Epoch 263/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1989 - acc: 0.9837 - val_loss: 0.4200 - val_acc: 0.9335\n",
            " - lr: 0.00936 - momentum: 0.95 \n",
            "Epoch 264/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1947 - acc: 0.9848 - val_loss: 0.3903 - val_acc: 0.9365\n",
            " - lr: 0.00802 - momentum: 0.95 \n",
            "Epoch 265/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1893 - acc: 0.9864 - val_loss: 0.3834 - val_acc: 0.9394\n",
            " - lr: 0.00669 - momentum: 0.95 \n",
            "Epoch 266/300\n",
            "196/196 [==============================] - 47s 240ms/step - loss: 0.1846 - acc: 0.9872 - val_loss: 0.3743 - val_acc: 0.9407\n",
            " - lr: 0.00536 - momentum: 0.95 \n",
            "Epoch 267/300\n",
            "196/196 [==============================] - 47s 240ms/step - loss: 0.1814 - acc: 0.9884 - val_loss: 0.3799 - val_acc: 0.9404\n",
            " - lr: 0.00402 - momentum: 0.95 \n",
            "Epoch 268/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1792 - acc: 0.9893 - val_loss: 0.3721 - val_acc: 0.9414\n",
            " - lr: 0.00269 - momentum: 0.95 \n",
            "Epoch 269/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1756 - acc: 0.9903 - val_loss: 0.3701 - val_acc: 0.9422\n",
            " - lr: 0.00136 - momentum: 0.95 \n",
            "Epoch 270/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1745 - acc: 0.9901 - val_loss: 0.3682 - val_acc: 0.9419\n",
            " - lr: 0.00002 - momentum: 0.95 \n",
            "Epoch 271/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1742 - acc: 0.9905 - val_loss: 0.3682 - val_acc: 0.9422\n",
            " - lr: 0.00002 - momentum: 0.95 \n",
            "Epoch 272/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1738 - acc: 0.9910 - val_loss: 0.3681 - val_acc: 0.9423\n",
            " - lr: 0.00002 - momentum: 0.95 \n",
            "Epoch 273/300\n",
            "196/196 [==============================] - 47s 240ms/step - loss: 0.1730 - acc: 0.9913 - val_loss: 0.3681 - val_acc: 0.9423\n",
            " - lr: 0.00002 - momentum: 0.95 \n",
            "Epoch 274/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1752 - acc: 0.9902 - val_loss: 0.3683 - val_acc: 0.9425\n",
            " - lr: 0.00002 - momentum: 0.95 \n",
            "Epoch 275/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1725 - acc: 0.9913 - val_loss: 0.3684 - val_acc: 0.9422\n",
            " - lr: 0.00002 - momentum: 0.95 \n",
            "Epoch 276/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1740 - acc: 0.9906 - val_loss: 0.3678 - val_acc: 0.9422\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 277/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1742 - acc: 0.9905 - val_loss: 0.3680 - val_acc: 0.9424\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 278/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1734 - acc: 0.9905 - val_loss: 0.3681 - val_acc: 0.9427\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 279/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1729 - acc: 0.9914 - val_loss: 0.3681 - val_acc: 0.9422\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 280/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1735 - acc: 0.9903 - val_loss: 0.3678 - val_acc: 0.9422\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 281/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1715 - acc: 0.9913 - val_loss: 0.3679 - val_acc: 0.9424\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 282/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1739 - acc: 0.9908 - val_loss: 0.3679 - val_acc: 0.9425\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 283/300\n",
            "196/196 [==============================] - 47s 240ms/step - loss: 0.1735 - acc: 0.9905 - val_loss: 0.3678 - val_acc: 0.9424\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 284/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1735 - acc: 0.9908 - val_loss: 0.3677 - val_acc: 0.9427\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 285/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1740 - acc: 0.9906 - val_loss: 0.3677 - val_acc: 0.9427\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 286/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1761 - acc: 0.9898 - val_loss: 0.3676 - val_acc: 0.9425\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 287/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1725 - acc: 0.9914 - val_loss: 0.3677 - val_acc: 0.9425\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 288/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1733 - acc: 0.9907 - val_loss: 0.3679 - val_acc: 0.9426\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 289/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1724 - acc: 0.9918 - val_loss: 0.3677 - val_acc: 0.9427\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 290/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1730 - acc: 0.9906 - val_loss: 0.3677 - val_acc: 0.9426\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 291/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.1738 - acc: 0.9904 - val_loss: 0.3677 - val_acc: 0.9424\n",
            " - lr: 0.00001 - momentum: 0.95 \n",
            "Epoch 292/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.1741 - acc: 0.9904 - val_loss: 0.3676 - val_acc: 0.9425\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 293/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1745 - acc: 0.9903 - val_loss: 0.3677 - val_acc: 0.9426\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 294/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1745 - acc: 0.9902 - val_loss: 0.3676 - val_acc: 0.9428\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 295/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1738 - acc: 0.9907 - val_loss: 0.3676 - val_acc: 0.9426\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 296/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1732 - acc: 0.9909 - val_loss: 0.3673 - val_acc: 0.9426\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 297/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1736 - acc: 0.9908 - val_loss: 0.3674 - val_acc: 0.9426\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 298/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1734 - acc: 0.9906 - val_loss: 0.3675 - val_acc: 0.9427\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 299/300\n",
            "196/196 [==============================] - 47s 238ms/step - loss: 0.1748 - acc: 0.9903 - val_loss: 0.3674 - val_acc: 0.9426\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Epoch 300/300\n",
            "196/196 [==============================] - 47s 239ms/step - loss: 0.1741 - acc: 0.9904 - val_loss: 0.3678 - val_acc: 0.9426\n",
            " - lr: 0.00000 - momentum: 0.95 \n",
            "Model took 13982.12 seconds to train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3IAAAFICAYAAAAVqcwDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntbxWHQxSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgwIhCP8pe5_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3a1f2615-524f-4ea2-880e-2d792725006b"
      },
      "source": [
        "# compute test accuracy\n",
        "result = model.evaluate_generator(validation_iterator, steps = len(validation_iterator))\n",
        "print(result)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:651: DeprecationWarning: `wait_time` is not used anymore.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.3466647204875946, 0.9476]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoGeKh9ny4Y9",
        "colab_type": "text"
      },
      "source": [
        "#Best Accuracy - 0.9428\n",
        "\n",
        "Epoch **294**/300\n",
        "196/196 [==============================] - 47s 239ms/step - loss: 0.1745 - acc: 0.9902 - val_loss: 0.3676 - val_acc: **0.9428**\n",
        " - lr: 0.00000 - momentum: 0.95\n",
        "\n",
        "#Reached 90% val accuracy at\n",
        "Epoch **231**/300\n",
        "196/196 [==============================] - 47s 239ms/step - loss: 0.3572 - acc: 0.9445 - val_loss: 0.5368 - val_acc: **0.9050**\n",
        " - lr: 0.05202 - momentum: 0.94"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBKGgGObKeaf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "d78f99b7-b888-4af1-dedc-3f649d4b7418"
      },
      "source": [
        "# Plot Learning Rate\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(lr_manager.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAESCAYAAAABl4lHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+sOcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttq_t-r8KfE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "7062f8cd-7613-4598-f34b-763e2530a5c6"
      },
      "source": [
        "# Plot momentum\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Momentum')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(lr_manager.history['momentum'])\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAESCAYAAAAWtRmOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XlcVQX+//HXZRMREC6yKIoLKioI\npIaamhuiuaS5AObyK0dN+2rupahpkftaTjOVY+OUOoKKZVmimI2WiJkDCIoLCiLKKrKjLPf3BxMT\nQ+pVuRwufJ6PR48HdzmHz0nk7Vnu+6g0Go0GIYQQ4ncMlB5ACCFE7SPhIIQQogoJByGEEFVIOAgh\nhKhCwkEIIUQVEg5CCCGqMFJ6ACFqO41Gw86dOzlw4ADFxcWUlpbSu3dvFixYwLFjxzh06BA7d+6s\nstykSZO4ceMG5ubmAJSWluLk5MSyZcto3bp1DW+FEE9G9hyEeIyNGzfy3XffsWPHDkJDQzl06BDF\nxcW88cYbPO5jQosWLeLIkSMcOXKEY8eO0aNHD5YsWVJDkwvx9CQchHiEe/fu8eWXX7J27Vrs7e0B\nMDMz491332Xq1KmPDYf/NXDgQOLi4nQxqhDVSsJBiEeIiorCwcEBZ2fnSs83aNCAAQMGYGCg/V+h\nkpISgoKCeO6556p7TCGqnYSDEI9w7949bGxsnnr5DRs2MGTIEAYPHoynpyc5OTls2rSpGicUQjfk\nhLQQj2BtbU1qaupTL79o0SJGjhwJgL+/P126dEGtVlfXeELojOw5CPEInp6eZGZmEhsbW+n54uJi\ntmzZQmFhodbrmjdvHh999NETLSOEUiQchHgES0tLpk6dyjvvvENiYiIAhYWFvPvuu1y8eJGGDRtq\nva7u3bvTrl07duzYoatxhag2clhJiMeYPXs2jRs3ZubMmZSWlmJgYMDAgQNZuXIlhw8fJjIykiFD\nhlS8X61Ws2fPnj9c17x585g8eTJ+fn7Y2trW1CYI8cRUcj8HIYQQ/0sOKwkhhKhCwkEIIUQVEg5C\nCCGqkHAQQghRhV5drVRUVERMTAy2trYYGhoqPY4QQuiF0tJS0tPTcXNzw9TUVKtl9CocYmJimDBh\ngtJjCCGEXtq9ezfdunXT6r16FQ6/XRe+e/duHBwcFJ5GCCH0Q0pKChMmTHiiz9boVTj8dijJwcGB\n5s2bKzyNEELolyc5HC8npIUQQlQh4SCEEKIKCQchhBBV6PScw+rVq4mKikKlUhEQEIC7u3vFa2Fh\nYfz1r3/FxMSEYcOGMXHiRCIiIpgzZw7t2rUDoH379ixfvlyXIwohhPgDOguHs2fPkpiYSFBQEPHx\n8QQEBBAUFARAWVkZgYGBHDx4ECsrK6ZNm4a3tzcAXl5efPTRR7oaSwghhBZ0Fg7h4eEVv/CdnZ3J\nzs4mLy8Pc3NzsrKysLS0rLgjVo8ePTh9+jSOjo66GgcAjUaDSqXS6fcQQtRtZWXKFVkbGNTc7y+d\nhUNGRgaurq4Vj9VqNenp6Zibm6NWq8nPzychIQFHR0ciIiLw8vLC0dGRa9euMWPGDLKzs5k1axa9\nevWqlnmy8h8w8uOfCRzlRt/20qMvhHhyP1/L4I0vfyXvfoki33/ZsI5M7dOmRr5XjX3O4fe3jVCp\nVKxdu5aAgAAsLCwqPrPQqlUrZs2axUsvvURSUhKTJ0/m6NGjmJiYPPP3Nzc1oqGxIQuCozg670XU\njZ59nUKI+uNewQPmB0diZ9GAaTX0C/p/9XOxq7HvpbNwsLOzIyMjo+JxWlpapU/neXl5Vdwta9Om\nTTg6OmJvb8/QoUMBcHJyokmTJqSmptKiRYtnnsfY0IAtfp6M/Pgnlh68wF8mdJFDTEIIrS3/OpbM\nvAfs+L/ncXNsrPQ4OqezS1l79epFaGgoALGxsdjZ2WFubl7x+tSpU8nMzKSgoIATJ07Qs2dPDh06\nVHF/3fT0dDIzM7G3t6+2mTo1s2T+IBe+j0nh4L+Tq229Qoi67evIZL6Jus1c73b1IhhAh3sOXbp0\nwdXVFX9/f1QqFStWrCAkJAQLCwsGDRqEr68vU6ZMQaVSMX36dNRqNQMGDGDhwoUcP36c4uJiVq5c\nWS2HlH5v+ott+CEulRVfx9K9jQ2OVtrfIF4IUf/cyS5k+VcxdHGyYkZfZ6XHqTF6dQ/pW7duMXDg\nQI4fP/5M3UpJdwsYsvUknZs3Zs/UHjV6BYAQQn+UlWmY/PlZzt/M4ru3+tCqSSOlR3oqT/O7s15+\nQrqF2ox3R3TizPW7fP7zDaXHEULUUl+EJ/DTtQyWDuuot8HwtOplOAD4dmuBd0d71ode5kpqrtLj\nCCFqmWtpeaz5Po7+Lra86uWk9Dg1rt6Gg0qlYu2Yzlg0MGLu3kgelJQpPZIQopYoLi1jXlAkZiaG\nrBvrXi+vbKy34QDQxLwBa0Z35uKdHLaGXVF6HCFELbHth2tcSM5mzejO2Flod1vNuqZehwOAj6sD\nvt2a88m/4vk18a7S4wghFPbvm1l8fOIao7s4MsStqdLjKKbehwPA8uGdaGbVkHlBUeQr9LF4IYTy\nCh6UMD84CgdLU1a+7Pr4BeowCQfAwtSYzb6eJGUV8MHhi0qPI4RQyOrvLpGQmc/GcR5YmhorPY6i\nJBz+w6u1mukvtuGfZ5M4filV6XGEEDXsx8tp7Dpzkz/1ak1PZxulx1GchMPvzB/Ung4OFrxz4AKZ\nefeVHkcIUUOy8h/w9v5o2tubs3Cwi9Lj1AoSDr/TwMiQrf6e5BQWE3DwAnr04XEhxFPSaDQs+yqG\nrIIHbPb1xNTYUOmRagUJh//RwcGSBT7tCY1N5cB5KecToq77OvI2hy/cYa53+3pTqqcNCYc/MLVP\nG7xaq1l5KJakuwVKjyOE0JHb9wpZ/nUMXVta16tSPW1IOPwBQwMVm8Z5ALBwX5SitwUUQuhGWZmG\nhfuiKC3TsNnXA0Mp4KxEwuEhWqjNWDGiExE37rLjJynnE6Ku+fvpBE7HZ7J8eCda2tSvUj1tSDg8\nwtiuzfHpZM+G0MvEpeQoPY4QoppcTc1l3ZE4Bnaww//5Z7/TZF0k4fAIKpWKNaM7Y9nQiHlBUdwv\nKVV6JCHEM3pQUsa84EjMGxixZkznelmqpw0Jh8ewMW/A2tHuXLqTw9awq0qPI4R4Rh8dv0pMcg6r\nX6m/pXrakHDQgncne/yfb8En/4rnlwQp5xNCX/2amMVffrzG2K7NGeLmoPQ4tZqEg5aWDe9Ec+uG\nzA+OJE/K+YTQO/n3S5gfHEnTxg1ZMaKT0uPUehIOWjJvYMQWX0+SswoJ/EbK+YTQN6u+u8TNuwVs\n9vXAop6X6mlDwuEJdGul5o2+zgSdS+LYRSnnE0JfnIhLY0/ETab3aUP3NlKqpw0Jhyc0z7s9HZta\nsiQkmgwp5xOi1rub/4BF+6Pp4GDBfJ/2So+jNyQcnpCJkQFb/TzJKSxhSYiU8wlRm2k0GpYevEBO\nYTFb/DxpYCSletqScHgKLg4WLBrswrGLqew7d0vpcYQQDxFyPpnvY1KY71O+xy+0J+HwlP7UuzXd\nW6t57xsp5xOiNkq+V8jKQ7F4tVIzrU8bpcfROxIOT8nAQMUmXw9UKhULgsvLu4QQtUNZmYYFwZGU\naTRsklK9pyLh8AyaW5ux8mVXzibc5W+nris9jhDiPz7/+QZnrt9lxQhXWqjNlB5HL0k4PKMxXRwZ\n4urApqNXuHRHyvmEUNrllFzWh15mUCd7xnVrrvQ4ekvC4RmpVCpWj+6MZUNj5gVFSjmfEAp6UFLG\n3KBILE2NWDNaSvWehYRDNVA3MmH92M7EpeSy+dgVpccRot7aGla+B79mtDtNzBsoPY5ek3CoJgM6\n2DPey4nPTl4n4nqm0uMIUe+cS7jLJ/+Kx69bCwZ1sld6HL2n03BYvXo1fn5++Pv7Ex0dXem1sLAw\nxowZw/jx49m1a1el14qKivD29iYkJESX41W7ZcM64qQ2Y8G+KHKLipUeR4h6I+9+CfODo3C0bshy\nKdWrFjoLh7Nnz5KYmEhQUBCrVq1i1apVFa+VlZURGBjI9u3b2b17NydOnCAlJaXi9b/+9a80btxY\nV6PpTKMGRmz29eD2vULel3I+IWrMqsMXScoqYNM4T8wbGCk9Tp2gs3AIDw/H29sbAGdnZ7Kzs8nL\nywMgKysLS0tL1Go1BgYG9OjRg9OnTwMQHx/PtWvX6Nevn65G06muLdXM7OfMvl9vcTQ25fELCCGe\nSdjFVP55Nok3XnTGq7Va6XHqDJ2FQ0ZGBtbW1hWP1Wo16enpFV/n5+eTkJBAcXExERERZGRkALBu\n3ToWL16sq7FqxJyB7XFtZsmSkAuk50o5nxC6kpl3n8Uh5aV68wa1U3qcOqXGTkj/vqBOpVKxdu1a\nAgICmDVrFs2bl1+L/NVXX+Hp6UmLFvp9w28TIwO2+HmSe7+EJSHRUs4nhA5oNBqWhFwgp7CErf5S\nqlfddHZwzs7OrmJvACAtLQ1bW9uKx15eXuzZsweATZs24ejoyLFjx0hKSuLHH38kJSUFExMTHBwc\neOGFF3Q1ps60t7fg7cEufHD4EsHnkvB73knpkYSoU/b/eoujF1MJGNqBDg5SqlfddLbn0KtXL0JD\nQwGIjY3Fzs4Oc3PzitenTp1KZmYmBQUFnDhxgp49e7J161YOHDhAcHAw48aN480339TLYPjNlF6t\n6dnGhve/ucjNTCnnE6K6JN0t4L1vLuLVWs2fekupni7obM+hS5cuuLq64u/vj0qlYsWKFYSEhGBh\nYcGgQYPw9fVlypQpqFQqpk+fjlpd904kGRio2OjrwZAtJ5kfHEnQGz2lAEyIZ1RapmHBvigANo2T\nUj1d0ek1XwsXLqz0uEOHDhVf+/j44OPj89BlZ8+erbO5apKjVUPeH+XKvKAoPjt5nZn9nJUeSQi9\ntuOn65y9cZcNY92lVE+H5BPSNWCUpyNDOzuw+dhlYm9nKz2OEHorLiWHjaFX8Olkz9iuUqqnSxIO\nNUClUrFqVGeszEyYHxRFUbGU8wnxpO6XlDJ3bySWDaVUryZIONQQ60YmrB/rzuVUKecT4mlsOXaV\nuJRc1o1xx0ZK9XROwqEG9XexY0J3J7afus4ZKecTQmu/JNzl05Px+D/fgoEdpVSvJkg41LClwzrS\nUm3GguAocqScT4jHKi/Vi6S5dUOWDZdSvZoi4VDDzEyM2OznyZ3sQt47JOV8QjxO4DcXSc4qZIuv\nlOrVJAkHBXRxsub/+rflwPlbHImRcj4hHuZobApB55J4o68z3VrVvc9C1WYSDgp5a2A7Ojs2JuDg\nBdJyi5QeR4haJyPvPktCLtCxqSXzvNsrPU69I+GgEGNDA7b4eZB/v4TFBy5IOZ8Qv6PRaFh84AK5\nRSVs9fPExEh+VdU0+T+uoLZ2FrwzpAM/xKWx95ckpccRotbYd+4WYZdSeXuICy4OFkqPUy9JOCjs\ntRda0autDYHfXiQxM1/pcYRQXHmpXiw92qiZ0qu10uPUWxIOCjMwULFhbHl52LygSEpKy5QeSQjF\nlJZpmB8ciYFKxcZxHhhIqZ5iJBxqgWZWDQkc6cb5m/f49OR1pccRQjHbT13nl4QsVr7sSnNrKdVT\nkoRDLTHSsxnD3Juy5dgVYpKlnE/UPxdv57Dp6GWGuDowuouj0uPUexIOtUR5OZ8b6kYmzAuKlHI+\nUa/cLyllfnAkjRuasFpK9WoFCYdaxMqsvJzvaloeG0IvKz2OEDVm89ErxKXksn5sZ9SNTJQeRyDh\nUOv0c7FjUo+W7PjpBqevZTx+ASH03JnrmXx26jrjvZwY0EFK9WoLCYdaaMnQDrRu0oiF+6LILpRy\nPlF35RYVsyA4Cie1GcuGdVR6HPE7Eg61kJmJEZt9PUjNvc97h2KVHkcInXn/m4vcyS5ks68njaRU\nr1aRcKilnvtPOV/Iv5P57sIdpccRotqFxqaw79dbzOznTNeW1kqPI/6HhEMtNntAW9yb/6ecL0fK\n+UTdkZ5bXqrn2sySOQOlVK82knCoxYwNDdjs60nhg1LePhAt5XyiTtBoNCwJiSbvvpTq1Wbyp1LL\ntbUzZ8lLHfjxcjp7zt5UehwhnlnQL0mEXUrjnSEdaGcvpXq1lYSDHpjcsxW92zbhg28vcSNDyvmE\n/rqZWUDgtxd5wdmG119opfQ44hEkHPSAgYGKDePcMTZUMT9YyvmEfvp9qd4GKdWr9SQc9ETTxg0J\nHOXGv2/e468/xis9jhBP7NOT8ZxLzOL9Ua44WjVUehzxGBIOemSkpyMjPJrx4fGrXLgl5XxCf8Te\nzmbLsSsM7ezAKE8p1dMHEg56JnCkKzbmJswLlnI+oR+KikuZHxSFlZkJq0ZJqZ6+kHDQM1ZmJmwY\n68G1tDzWHYlTehwhHmvT0ctcTs1l/Vh3rKVUT29IOOihF9vb8v96tuTvPyfws5TziVrszPVM/vbT\nDSZ0d6K/i53S44gnoNNwWL16NX5+fvj7+xMdHV3ptbCwMMaMGcP48ePZtWsXAIWFhcyZM4eJEycy\nbtw4Tpw4ocvx9NrilzrSxlbK+UTtlfOfUr2WajOWSqme3tGq6SovL4+IiAhyc3MrPT9q1KiHLnP2\n7FkSExMJCgoiPj6egIAAgoKCACgrKyMwMJCDBw9iZWXFtGnT8Pb25vz587i5uTFt2jSSk5OZMmUK\n/fv3f4bNq7samhiyxdeT0X89zYqvY9jq/5zSIwlRyXuHykv19s98ATMTKdXTN1r9iU2aNIl27dph\nY2NT8dzjTiqFh4fj7e0NgLOzM9nZ2eTl5WFubk5WVhaWlpao1WoAevTowenTpxk9enTF8nfu3MHe\nXrrdH8WjhRWzB7Rla9hVvDvZM9y9mdIjCQHAkZgUDpy/xewBbeniJKV6+kircLCysmL9+vVPtOKM\njAxcXV0rHqvVatLT0zE3N0etVpOfn09CQgKOjo5ERETg5eVV8V5/f39SUlL45JNPnuh71kf/178t\nJ+LSWHowhudbqbG3NFV6JFHPpeUWEXDwAp0dG/PWwHZKjyOeklbhMHr0aAIDA+nYsSNGRv9d5FGH\nlf7X70vjVCoVa9euJSAgAAsLC5o3b17pvXv37uXSpUssWrSIQ4cOyaVvj2BsaMBmP0+GfXSKRfuj\n+cfrz8v/L6EYjUbD4gMXyL9fwhY/D4wN5ZoXfaXVn9z27dvJzs4mPj6ey5cvc/nyZa5cufLIZezs\n7MjI+O+VNGlpadja2lY89vLyYs+ePXz66adYWFjg6OhITEwMd+6U37ugY8eOlJaWcvfu3afZrnrF\n2dacgKEdOXklnV0RUs4nlLP3lyR+iEtj8UsdaGsnpXr6TKs9B7VazcaNG59oxb169WLbtm34+/sT\nGxuLnZ0d5ubmFa9PnTqVdevW0bBhQ06cOMHrr7/OoUOHSE5OZunSpWRkZFBQUIC1tRyv1MakHi05\ndjGVVYcv0svZhja25o9fSIhqlJiZT+C3F+nV1ob/17OV0uOIZ6RVOLi6urJlyxbc3d0rHVbq27fv\nQ5fp0qULrq6u+Pv7o1KpWLFiBSEhIVhYWDBo0CB8fX2ZMmUKKpWK6dOno1ar8ff3Z+nSpbz66qsU\nFRXx7rvvYmAgu6XaUKlUbBjrweCtJ5kXHMWBGT0xkl16UUPKS/WiMDQo/zmUUj39p1U4/HZoJyws\nrNLzjwoHgIULF1Z63KFDh4qvfXx88PHxqfS6qakpmzZt0mYk8QccGpvywSg3Zv/z3/zlx3g5GShq\nzCf/iufXxCw+9PekmZTq1QlahcPs2bN1PYeoJiM8mhF2KZUPj1+ln4st7s2tlB5J1HExyeWlesPc\nm/Kyh1xOXVdoHQ6/XQFTXFxMUlISrq6ufPnllzodTjyd9192I+L6XeYFRXL4rT6YGhsqPZKoo4qK\nS5kXFIm6kQmrRrnJlXJ1iFbhcODAgUqP09PT+fDDD3UykHh2jc2M2TjOg4k7Ilj7fRwrX3Z9/EJC\nPIUNoZe5mpbHP6Z4YWUmpXp1yVOdsbS1tSUuThpBa7Pe7Zrw2gut2Hk6gVNX05UeR9RBp+Mz2PHT\nDSb1aEnf9raPX0DoFa32HMaMGVOxu6jRaLh79y49evTQ6WDi2S1+qQOnrqazaF80oXNfpLGZsdIj\niToip6iYhcFRtGnSiIChUqpXF2kVDps2bcLYuPwXi0qlwtzcnLIyuY9xbWdqbMhWv+d45S8/8+6h\nGD6Ucj5RTVZ+HUtq7n0OzHyBhiZyTqsueuRhpZKSEgoKCli2bBk2Njao1Wqsra0xMDBg0qRJNTWj\neAadm5f323wdeZtvom4rPY6oA767cIeQfyczq39bPFvI1XB11SP3HE6ePMnf//53oqOjGTp0aMXz\nBgYGlYryRO32Zj9nfohLY9lX5eV8Do2lnE88nbSc8lI99+aNmTWgrdLjCB16ZDgMGDCAAQMG8PXX\nXzNy5MiamklUMyNDAzb7ejDso59YtD+KL6Z4ySWH4olpNBrePhBN4YNSNvt6SqleHafVOQdra2tm\nzZpFbm5upXbVL774QmeDierVxtacgGEdWf5VDF+eSWSydN+IJ7Tn7E1+vJzOey+70tZOurvqOq3C\nYc2aNQQEBODg4KDreYQOTezuRNjFVFZ/d4lebZvgLOV8Qks3MvL54NtL9GnXhEk9Wio9jqgBWu0X\ntmjRgj59+tCuXbtK/wn9Ul7O546psSHzgyIpLpUrzsTjlZSWMT84EhMjAynVq0e02nNo3bo1c+bM\noWvXrhga/veytQkTJuhsMKEbdpamrBrVmf/bc56PT1xjrnd7pUcStdwn/4rn3zfv8dH45+RihnpE\nq3CwsLDAwsKCnJwcXc8jasAw96aEXXJk2w/X6O9ih4dcjige4sKtbLaGXWWERzMp1atntAqHWbNm\nkZKSwq1bt+jWrRsPHjzAxER6VPTZypddOXM9s6KcTz7IJP5XUXEp84IjaWLegMCR0s9V32h1zmHn\nzp3MnTuXwMBAADZs2MD27dt1OpjQrcYNy8v5rmfks/b7S0qPI2qhdUfiuJaWx4Zx7lKqVw9pFQ5h\nYWHs3bsXS0tLAAICAqrc+Efon15tmzClV2v+EZ7IyStSzif+6+drGfz95wT+X8+W9GknpXr1kVbh\nUFpaClDxwan79+9TUlKiu6lEjXl7iAtt7cxZtD+KewUPlB5H1ALZhcUs3BdFG9tGLH5JSvXqK63C\nYfjw4UyePJnExERWrFjBqFGjGDNmjK5nEzWgvJzPk8y8Byz/OlbpcUQtsOLrGNJy77PF11PORdVj\nWp2QnjBhAn379iU6OhoTExNmzJhB06ZNdT2bqCFujo2Z692OjUev4N3RjpGejkqPJBTybfRtvoq8\nzTzv9nIVWz2nVThER0dz+PDhivqM48ePA+WfnBZ1w4y+zhyPS2P5VzF4tVbTtLHcJL6+Sc0pYunB\nGDxaWPF//Z2VHkcoTKtwWLRoEdOmTaNJkya6nkcoxMjQgC2+nrz04Sne3h/NP173kk/C1iMajYZF\n+6O5X1LKFl8PjKRUr97TKhzatGlT6W5wom5q1aQRy4Z3ZOnBGL4IT+C1Xq2VHknUkF0RNzl5JZ3A\nka60kc4tgZbhMHz4cEaNGoWLi0ul+gw5rFT3vOpVXs635vs4erezlfbNeuB6eh6rDl/kxfa2TJRS\nPfEfWoXD1q1bmT59Ora2cr1zXadSqVg31p3BW04yPziSAzNfkN7+OqyktIx5wVE0MDJkw1h3OTog\nKmgVDs7OzowbN07Xs4haws7ClNWvdGbm7vNs++Ea8wdJOV9d9fGJeKKS7vHnV5/D3lJK9cR/aX2z\nnwkTJuDm5lbpsNLbb7+ts8GEsl7q3JTRXRz5+MQ1+rvY8pyTtdIjiWoWfeseH/1wlZGezRjuLqV6\nojKtwsHLy0vuGV0PrXzZlYjrd5kfHMXht3pjZqLVj4vQA4UPSpkXFImteQPef9lN6XFELaTVweRh\nw4ah0WiIjY0lLi4OIyMjuad0PWBpWl7Ol5CZz5rv4pQeR1SjdUfiiE/PZ+M4DxqbGSs9jqiFtAqH\npUuXcvHiRby8vHB3d+fcuXO8++67up5N1AI9nW34U6/WfHkmkR8vpyk9jqgGp66ms/N0Aq+90Ire\n7eSzS+KPaXWcICUlhQ0bNlQ8HjZsGJMnT9bZUKJ2WTjYhZNX03l7fzShc1/EupHUN+ur7IJiFu2L\nxtm2EYtf6qD0OKIW02rPobi4mNTU1IrHKSkpWrWyrl69Gj8/P/z9/YmOjq70WlhYGGPGjGH8+PHs\n2rWr4vn169fj5+fHmDFjOHr0qLbbIXTI1NiQzb6eZBU8YNlXMWg0GqVHEk9p+dcxZOTdZ4ufJ6bG\nUqonHk6rPYf58+fz+uuvo1Kp0Gg0qFSqihv/PMzZs2dJTEwkKCiI+Ph4AgICCAoKAqCsrIzAwEAO\nHjyIlZUV06ZNw9vbm4SEBK5evUpQUBBZWVm88sor+Pj4PPtWimdWXs7Xng2hlxkUac+o56ScT998\nE3WbQ1G3mT+oPe7NpVRPPNojw2HJkiUVX3t4eHDv3j1UKhWNGzdm3759dOnS5aHLhoeH4+3tDZR/\nTiI7O5u8vDzMzc3JysrC0tIStVoNQI8ePTh9+jQjR47E3d0dAEtLSwoLCyktLa10+axQzoy+zvwQ\nl8byr8vL+ZpZSTmfvkjJLmLpwQt4trDizX5Sqice75HhcOXKFXJzc+nduzd9+/bFzMxM60MKGRkZ\nuLr+976zarWa9PR0zM3NUavV5Ofnk5CQgKOjIxEREXh5eWFoaIiZmRkA+/fv58UXX5RgqEUMDVRs\n8fVkyIcnWbgvil1/6i7lfHqgvFQviuJSDZulVE9o6ZHhcODAAW7evMnhw4fZtm0bDg4ODB48mP79\n+2Nu/mSdO78PFZVKxdq1awkICMDCwoLmzZtXem9YWBj79+/n888/f6LvIXTPycaM5cM7sSTkAjtP\nJzClt5Tz1XZfnknk1NUMAke5Same0Npj/wnh5OTEzJkz2b9/P3PmzCE+Pp6XXnqJGTNmPHI5Ozs7\nMjIyKh6npaVV6mby8vJiz549fPrpp1hYWODoWH4M+9SpU3zyySds374dCwuLp90uoUP+z7dgYAc7\n1h2J42pqrtLjiEeIT89j9XdJk5ZUAAAagklEQVSX6NvelondnZQeR+gRrfYvNRoN4eHh7Ny5k2+/\n/ZbevXszfvz4Ry7Tq1cvQkNDAYiNjcXOzq7S3sbUqVPJzMykoKCAEydO0LNnT3Jzc1m/fj2ffvop\nVlZywqy2UqlUrB3jTqMGRswLjuRBSZnSI4k/UFxaxvygSEyNpVRPPLlHHlaKjo7m22+/5fTp07i7\nuzNkyBBWrlyJsfHjP1HZpUsXXF1d8ff3R6VSsWLFCkJCQrCwsGDQoEH4+voyZcoUVCoV06dPR61W\nV1ylNHfu3Ir1rFu3jmbNpPeltrG1aMDqVzozY9evbPvhKgt8XJQeSfyPj09cI+pWNh+/2gU7KdUT\nT0ilecQZ5g4dOuDk5IS7u/sfBkJN38/h1q1bDBw4kOPHj1c5TyGUsXBfFCHnb7Fvxgt0bSnlfLVF\nZNI9xvz1NC97NGOLn6fS4wiFPc3vzkfuOfx2r2ghHmbFiE6Ex2eyIDiS7+b0kXK+WqDwQSnzgyKx\ns2jAypddH7+AEH/gkX+TfztJLMTDWJgas8nXg/Hbz7Dq8CVWvdJZ6ZHqvTXfX+J6Rj67p3ancUMp\n1RNPRy54Fs+sRxsbpvVpw+6Im5yIk3I+JZ28ks4X4YlM6dWaXm2lVE88PQkHUS3mD2qPi70Fbx+I\n5m7+A6XHqZfuFTxg0f4o2tqZ8/YQuUBAPBsJB1EtTI0N2eLnyb2CByw9eEHK+RSw7KsYMvMesFVK\n9UQ1kHAQ1aZTM0vmD3Lh+5gUDv47Welx6pWvI5P5NvoOc73b4ebYWOlxRB0g4SCq1fQX2/B8K2tW\nfB1L8r1CpcepF+5kF7L8qxiec7JiRl8p1RPVQ8JBVCtDAxWbxnlSptGwMDiKsjI5vKRLZWUaFu2L\nprhUwxZfTynVE9VGfpJEtXOyMePdEZ0Iv57J5z/fUHqcOu2L8AR+upbBsuEdadWkkdLjiDpEwkHo\nhG+3Fnh3tGN96GWuSDmfTlxLy2PN93H0d7HlVS8p1RPVS8JB6IRKpWLNaHcsGhgxd6+U81W34tIy\n5gVFYmZiyLoxUqonqp+Eg9AZW4sGrBndmYt3cvjw+BWlx6lTtv1wjQvJ2ax+pbOU6gmdkHAQOuXj\n6sC4rs3564/x/Jp4V+lx6oR/38zi4xPXGN3FkZc6N1V6HFFHSTgInXt3RCeaWTVkXlAU+fdLlB5H\nrxU8KGF+cBQOlqZSqid0SsJB6JyFqTGbfT1Jyirgg8MXlR5Hr635Lo4bGflsGOeOpamU6gndkXAQ\nNcKrtZrpfdrwz7NJHL+UqvQ4eunHy2l8eSaRqb1b84KzlOoJ3ZJwEDVmvk97OjhY8M6BC2Tm3Vd6\nHL2Slf+At/dH097enIWDpVRP6J6Eg6gxDYzKy/lyCosJkHI+rWk0GpZ9FUNWwQM2+0qpnqgZEg6i\nRnVsaskCn/aExqZy4LyU82nj68jbHL5wh7ne7aVUT9QYCQdR46b2aYNXKzUrD8VyK6tA6XFqtdv3\nCln+dQxdW1pLqZ6oURIOosYZGqjY5OsBwAIp53uosjINi/ZHUVqmYbOvB4YG8iloUXMkHIQiWqjL\ny/kibtxlx09SzvdHdp5O4OdrmSwf3omWNlKqJ2qWhINQzLiuzfHpZM+G0MtcTpFyvt+7lpbLuiNx\nDOxgh//zLZQeR9RDEg5CMeXlfJ2xbGjE3KBI7peUKj1SrfCgpIy5QZE0amDEmjGdpVRPKELCQSjK\nxrwBa0e7c+lODlvDrio9Tq2w7YerxCTnlJfqWUipnlCGhINQnHcne/yfb8Gn/4rnXEL9Luc7/59S\nvbFdmzPEzUHpcUQ9JuEgaoVlwzvhaN2Q+cFR5NXTcr6CByXMD4qkaeOGrBjRSelxRD0n4SBqBfMG\nRmzx9eRWVgEffFs/y/lWHb5E4t0CNvl6YCGlekJhEg6i1ujWSs0bfZ3Z+0sSYRfrVznfibg0dkfc\nZFqfNvRoY6P0OEJIOIjaZZ53ezo2tWRxSHS9Kee7m/+Atw9E42JvwfxB7ZUeRwhAx+GwevVq/Pz8\n8Pf3Jzo6utJrYWFhjBkzhvHjx7Nr166K569cuYK3t3el50T9YWJkwFY/T3IKS1gSUvfL+TQaDUsP\nXuBewQO2+Empnqg9dBYOZ8+eJTExkaCgIFatWsWqVasqXisrKyMwMJDt27eze/duTpw4QUpKCgUF\nBQQGBtKzZ09djSX0gIuDBYsGu3D0Yir7f72l9Dg6dfDfyXwfk8L8QS50amap9DhCVNBZOISHh+Pt\n7Q2As7Mz2dnZ5OXlAZCVlYWlpSVqtRoDAwN69OjB6dOnMTExYfv27djZ2elqLKEn/tS7NT3aqHnv\nm4sk3a2b5XzJ9wpZ8XUsz7eyZvqLbZQeR4hKdBYOGRkZWFtbVzxWq9Wkp6dXfJ2fn09CQgLFxcVE\nRESQkZGBkZERpqbyoR8BBgYqNo7zQAUs2FdePleXlJVpWBgcRZlGw2ZfTynVE7VOjZ2Q/v2xY5VK\nxdq1awkICGDWrFk0b968psYQeqS5tRkrXnbl7I277PjputLjVKvPf75B+PVM3h3RiRZqM6XHEaIK\nnYWDnZ0dGRkZFY/T0tKwtbWteOzl5cWePXv49NNPsbCwwNHRUVejCD02posjQ1wd2Bh6hUt3cpQe\np1pcSc1lfehlvDva49tNSvVE7aSzcOjVqxehoaEAxMbGYmdnh7m5ecXrU6dOJTMzk4KCAk6cOCEn\nocUfUqlUrB7dGcuGxsyrA+V8D0rKmLs3EosGRqyVUj1RixnpasVdunTB1dUVf39/VCoVK1asICQk\nBAsLCwYNGoSvry9TpkxBpVIxffp01Go1MTExrFu3juTkZIyMjAgNDWXbtm1YWVnpakyhB9SNTFg/\ntjNTdp5jy7GrLH6pg9IjPbWtYVe4eCeHzyZ1pYl5A6XHEeKhdBYOAAsXLqz0uEOH//6l9vHxwcfH\np9Lrbm5ufPnll7ocSeipAR3sGe/lxKcn4xnQwQ6v1mqlR3pivybe5ZN/xePbrTk+rlKqJ2o3+YS0\n0BvLhnXESW3G/OBIcouKlR7nieTfL2FeUBTNrBqyfLiU6onaT8JB6I1GDYzY7OvB7XuFBOpZOd8H\nhy+SlFXAZl9PKdUTekHCQeiVri3VzOznTPC5WxyNTVF6HK0cv5TKP88mMf3FNnp5OEzUTxIOQu/M\nGdge12aWLAm5QEYtL+fLzLvPOwei6eAgpXpCv0g4CL1jYmTAFj9Pcu+XsPhA7S3n02g0BBy8QE5h\nCVv8PGlgJKV6Qn9IOAi91N7egrcHuxB2KZXgc0lKj/OHDpxPJjQ2lQU+5TXkQugTCQeht6b0ak3P\nNja8/81FbmbWrnK+pLsFrDwUi1crNVP7SKme0D8SDkJvGRio2OjrgYFKxYJ9kbWmnK+sTMPCfVEA\nbPL1kFI9oZckHIRec7RqyHsjXfklIYvPTtaOcr4dP90g4sZdKdUTek3CQei9V55z5CU3BzYfu8zF\n28qW88Wl5LAh9DKDOtkzrqu0DQv9JeEg9J5KpWLVK52xMjNhXlAkRcXKlPPdLyllXlAUlg2NWDNa\nSvWEfpNwEHVCeTmfO5dTc9l87IoiM2wNu8qlOzmsHe0upXpC70k4iDqjv4sdr3Z3Yvup65y5nlmj\n3/uXhPJSPb9uLfDuZF+j31sIXZBwEHXK0qEdaak2Y0FwFDk1VM6Xd7+E+cGRNLduyPIRUqon6gYJ\nB1GnNGpgxGY/T+5kF/LeoZop5/vg24vcyipks68n5g102oIvRI2RcBB1Thcna/6vf1sOnL/FkRjd\nlvMdu5jK3l+SeONFZ55vJaV6ou6QcBB10lsD2+HmaEnAwQuk5Rbp5Htk5N1nSUg0HZtaMm9QO518\nDyGUIuEg6iRjQwO2+HqSp6NyPo1Gw5KQ8lK9rVKqJ+ogCQdRZ7Wzt2DxkA78EJfG3l+qt5xv36+3\nOHYxlUWDXXBxsKjWdQtRG0g4iDrttRda0autDYHfXiQxM79a1pl0t4D3v7lI99Zq/tS7dbWsU4ja\nRsJB1GkGBio2jC0vv5sfHPXM5XylZRoWBP+3VM9ASvVEHSXhIOq8ZlYNCRzpxq+JWXzyr/hnWtff\nTl3nbMJdVr7sSnNrKdUTdZeEg6gXRno2Y1jnpmwNu0JMcvZTrePSnRw2Hb3CYFd7xnRxrOYJhahd\nJBxEvaBSqfhglBvWT1nOV16qF4llQ2NWvyKleqLuk3AQ9Yb1f8r5rqblsTH08hMtu/noFeJSclk/\ntjM2Uqon6gEJB1Gv9HOxY1KPluz4+Qan4zO0WibieiafnbrOeC8nBnSQUj1RP0g4iHpnydAOtLJp\nxEItyvlyi4pZsC8KJ7UZy4Z1rKEJhVCehIOod8xMjNjs60Fq7n1WHop95Hvf/+Yit+8VstnXg0ZS\nqifqEQkHUS89959yvpDzyXx/4c4fvic0NoV9v95iZj9nuraUUj1Rv0g4iHpr9oC2uDdvXF7Ol1O5\nnC899z5LQi7g2sySOQPbKzShEMqRcBD1lrGhAZt9PSl4UMo7B6IryvnKS/WiybtfwhY/T0yM5K+J\nqH90+lO/evVq/Pz88Pf3Jzo6utJrYWFhjBkzhvHjx7Nr1y6tlhGiurW1M2fJSx04cTmdf54tL+cL\nPpdE2KU03h7sQnt7KdUT9ZPOzrCdPXuWxMREgoKCiI+PJyAggKCgIADKysoIDAzk4MGDWFlZMW3a\nNLy9vbl58+ZDlxFCVyb3bEXYpTQCv71Ic+uGvP/NRXq2sWFKLynVE/WXzvYcwsPD8fb2BsDZ2Zns\n7Gzy8vIAyMrKwtLSErVajYGBAT169OD06dOPXEYIXTEwULFhnDvGhiomf34WA5WKjVKqJ+o5nYVD\nRkYG1tbWFY/VajXp6ekVX+fn55OQkEBxcTERERFkZGQ8chkhdKlp44aseqUzhgYqAke54WjVUOmR\nhFBUjV24/fs7calUKtauXUtAQAAWFhY0b978scsIoWsjPJrRz8UWC1NjpUcRQnE6Cwc7OzsyMv5b\nT5CWloatrW3FYy8vL/bs2QPApk2bcHR05P79+49cRghdk2AQopzODiv16tWL0NBQAGJjY7Gzs8Pc\n3Lzi9alTp5KZmUlBQQEnTpygZ8+ej11GCCFEzdDZnkOXLl1wdXXF398flUrFihUrCAkJwcLCgkGD\nBuHr68uUKVNQqVRMnz4dtVqNWq2usowQQoiap9Lo0YH9W7duMXDgQI4fP/7Q8xRCCCEqe5rfnfLR\nTyGEEFVIOAghhKhCwkEIIUQVelVQX1paft/flJQUhScRQgj98dvvzN9+h2pDr8Lht09LT5gwQeFJ\nhBBC/6Snp9OyZUut3qtXVysVFRURExODra0thoaGSo8jhBB6obS0lPT0dNzc3DA1NdVqGb0KByGE\nEDVDTkgLIYSoQq/OOTyL1atXExUVhUqlIiAgAHd3d6VHquLKlSu8+eabvPbaa0ycOJE7d+7w9ttv\nU1paiq2tLRs2bMDExIRDhw7xj3/8AwMDA3x9fRk3bhzFxcUsXryY27dvY2hoyJo1a2jRogVxcXGs\nXLkSABcXF957770a257169fz66+/UlJSwhtvvEHnzp31cnsKCwtZvHgxmZmZ3L9/nzfffJMOHTro\n5bb8XlFREcOHD+fNN9+kZ8+eers9ERERzJkzh3bt2gHQvn17pk6dqrfbA3Do0CH+9re/YWRkxFtv\nvYWLi0vNb4+mHoiIiNBMnz5do9FoNNeuXdP4+voqPFFV+fn5mokTJ2qWLVum+fLLLzUajUazePFi\nzXfffafRaDSaTZs2aXbv3q3Jz8/X+Pj4aHJycjSFhYWaYcOGabKysjQhISGalStXajQajebUqVOa\nOXPmaDQajWbixImaqKgojUaj0cyfP1/z448/1sj2hIeHa6ZOnarRaDSau3fvavr27au323P48GHN\nZ599ptFoNJpbt25pfHx89HZbfm/z5s2a0aNHaw4cOKDX23PmzBnN7NmzKz2nz9tz9+5djY+PjyY3\nN1eTmpqqWbZsmSLbUy8OK+nDTYRMTEzYvn07dnZ2Fc9FREQwcOBAAPr37094eDhRUVF07twZCwsL\nTE1N6dKlC+fPnyc8PJxBgwYB8MILL3D+/HkePHhAcnJyxV7Sb+uoCc8//zwffvghAJaWlhQWFurt\n9gwdOpRp06YBcOfOHezt7fV2W34THx/PtWvX6NevH6DfP2t/RJ+3Jzw8nJ49e2Jubo6dnR2BgYGK\nbE+9CAd9uImQkZFRlasICgsLMTExAcDGxob09HQyMjJQq9UV7/ltW37/vIGBASqVioyMDCwtLSve\n+9s6aoKhoSFmZmYA7N+/nxdffFGvtwfA39+fhQsXEhAQoPfbsm7dOhYvXlzxWN+359q1a8yYMYPx\n48fz888/6/X23Lp1i6KiImbMmMGrr75KeHi4IttTb845/J5GDy/QetjMT/K8EtsdFhbG/v37+fzz\nz/Hx8XnsLLV5e/bu3culS5dYtGhRpe+tb9vy1Vdf4enpSYsWLf7wdX3bnlatWjFr1ixeeuklkpKS\nmDx5cqUPe+nb9gDcu3ePP//5z9y+fZvJkycr8vNWL/YcHnfjodrKzMyMoqIiAFJTU7Gzs/vDbfnt\n+d/+JVBcXIxGo8HW1pZ79+5VvPe3ddSUU6dO8cknn7B9+3YsLCz0dntiYmK4c+cOAB07dqS0tJRG\njRrp5bYA/Pjjjxw/fhxfX1/27dvHX/7yF739swGwt7dn6NChqFQqnJycaNKkCdnZ2Xq7PTY2Njz3\n3HMYGRnh5OREo0aNFPl5qxfhoK83EXrhhRcq5j569Ch9+vTBw8ODCxcukJOTQ35+PufPn6dbt270\n6tWLI0eOAHDixAm6d++OsbExbdq04dy5c5XWURNyc3NZv349n376KVZWVnq9PefOnePzzz8Hyg9R\nFhQU6O22AGzdupUDBw4QHBzMuHHjePPNN/V6ew4dOsSOHTuA8k8AZ2ZmMnr0aL3dnt69e3PmzBnK\nysrIyspS7Oet3nwIbuPGjZw7d67iJkIdOnRQeqRKYmJiWLduHcnJyRgZGWFvb8/GjRtZvHgx9+/f\np1mzZqxZswZjY2OOHDnCjh07UKlUTJw4kZdffpnS0lKWLVtGQkICJiYmrF27lqZNm3Lt2jXeffdd\nysrK8PDwYMmSJTWyPUFBQWzbto3WrVtXPLd27VqWLVumd9tTVFTE0qVLuXPnDkVFRcyaNQs3Nzfe\neecdvduW/7Vt2zYcHR3p3bu33m5PXl4eCxcuJCcnh+LiYmbNmkXHjh31dnug/BDm/v37AZg5cyad\nO3eu8e2pN+EghBBCe/XisJIQQognI+EghBCiCgkHIYQQVUg4CCGEqELCQQghRBUSDkKvrF27lkmT\nJjFkyBD69u3LpEmTmDVrllbLhoSEcOzYsYe+vmrVKpKSkp56tm3btrFr1y4Ajh8/zoMHD556XQBx\ncXHcuHEDgHnz5lV8CEqImiCXsgq9FBISwtWrV3nnnXeUHqXCtm3bsLa2ZuLEiUyaNIlPPvmERo0a\nPdP63Nzc6N+/fzVOKYR26mW3kqh7IiIi+PzzzykoKOCdd97h7NmzhIaGUlZWRt++fZk1a1bFL+92\n7dqxe/duVCoV169fZ/DgwcyaNYtJkyaxfPlyQkNDyc3N5caNG9y8eZOAgAD69u3LZ599xuHDh2nR\nogUlJSW8/vrrdO/evcosX331FZGRkUybNo2dO3eyb98+vvnmGwwMDPD29mbKlCls27aNpKQkbt26\nxc6dO1myZAmpqakUFBQwe/ZsmjVrxt69e1Gr1djY2DB37ly++eYbcnNzCQgIoLi4GJVKxapVq1Cp\nVCxevJgWLVpw+fJlOnbsyKpVq/jpp5/YunUrpqam2NjYsHHjRoyNjRX40xH6SMJB1BlXrlwhNDQU\nExMTzp49y549ezAwMGDgwIG89tprld4bHR3N999/T1lZGQMGDKhyaColJYXt27dz8uRJ9u7di4eH\nB7t37yY0NJS8vDx8fHx4/fXX/3COUaNG8dFHH7F9+3ZSU1M5cuQI//znPwEYP348Q4YMAcp7b/bs\n2UNmZia9e/fmlVdeISkpiTlz5hASEkKfPn0YPHhwpRtTffjhh4wdO5ahQ4dy5MgR/vznPzN79mxi\nY2PZsmULNjY2vPjii+Tk5LBr1y4WL15Mt27dOHr0KPfu3dOLTjFRO0g4iDrDxcWlotbY1NSUiRMn\nYmRkRFZWVqXSMYBOnTrRsGHDh66rS5cuADg4OJCbm8vNmzdp3749pqammJqaan0nwQsXLpCYmMjk\nyZMByM/PJzk5GaBiHZaWlly4cIGgoCAMDAyqzPp7MTExLFiwAIDu3bvz8ccfA+Dk5FTxi9/Ozo7c\n3FyGDBnCihUrGDFiBMOGDZNgEE9EwkHUGb8FQ3JyMjt37uTgwYM0atSI4cOHV3mvkdGjf/T/93WN\nRoOBwX+v31CpVFrNZGxsTL9+/Xj//fcrPX/mzJmKQzzffvst2dnZ7Nmzh3v37jF27NiHrk+lUlXU\nLRcXF1fMZGhoWGXeUaNG0adPH8LCwpg5cyYffvghzs7OWs0thFytJOqcrKws1Go1jRo1IjY2luTk\nZIqLi59pnY6Ojly9epXi4mLu3r1LTEzMI9+vUqkoLS3F1dWViIgICgsL0Wg0fPDBB1WuOsrKyqJ5\n8+YYGBhw7NixiqucflvH73Xu3JmIiAgAfvnlF9zc3B46w8cff4yRkRF+fn4MHTqU+Pj4p9l0UU/J\nnoOoczp27EijRo3w9/ena9eu+Pv7895779G1a9enXmeTJk0YPnw448aNw9nZGXd39yr/Wv89Ly8v\nXn31Vb744gsmT57MhAkTMDQ0xNvbu8od/3x8fJg5cyaRkZGMGTMGBwcH/vznP9OtWzc++OCDSlc8\nvfXWWyxdupTg4GCMjY1ZvXr1Q4OvWbNmvP7661haWmJpafnQcyRC/BG5lFUILYWEhDB8+HCMjIwY\nMWIEO3bswMHBQemxhNAJ2XMQQksZGRn4+vpiYmLCiBEjJBhEnSZ7DkIIIaqQE9JCCCGqkHAQQghR\nhYSDEEKIKiQchBBCVCHhIIQQogoJByGEEFX8fwrtVrJQeDFvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}