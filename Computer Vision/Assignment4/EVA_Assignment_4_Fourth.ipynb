{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA_Assignment_4_Fourth.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtCTZjybrjyU",
        "colab_type": "text"
      },
      "source": [
        "#HEADER NOTE\n",
        "\n",
        "\n",
        "*   Will keep the parameters count within 15k which is a constraint for the assignment.\n",
        "*   Will continue to keep the batch size to 128.\n",
        "*   Will keep the Learning Rate as default one.\n",
        "*   Will keep the optimizer same as 'adam'.\n",
        "*   Will add Dropouts to reduce the gap between Training accuracy and Validation accuracy.\n",
        "*   Changing the position of Dropouts from after convolution layers to before BatchNormalizations. - *an improvement over the previous network* \n",
        "*   Introduce LR Scheduler to change LR (reduce by 10%) every 10 epochs. - *an improvement over the previous network*\n",
        "*   Will add BatchNormalizations before every convolution layer except the first one.\n",
        "*   Will run for more epochs (say 50) to see the capacity of the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "65178ff8-c311-4dab-d3d1-9008db9fd0f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://keras.io/\n",
        "# Installing Keras which is open-source neural-network library written in Python.\n",
        "!pip install -q keras\n",
        "\n",
        "# Importing the keras library.\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing NumPy (Numerical Python) which is a library consisting of multidimensional array objects and a collection of routines for processing those arrays. It also gives an alias to the library.\n",
        "import numpy as np\n",
        "\n",
        "# There are two main types of models available in Keras: the Sequential model and the Model class used with the functional API.\n",
        "# Sequential model is a linear stack of layers. Importing Sequential model from Keras.\n",
        "from keras.models import Sequential\n",
        "\n",
        "# Importing different layers from Keras.\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D, BatchNormalization\n",
        "\n",
        "# Importing the utils library of Keras.\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Importing the callbacks of Keras.\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# MNIST is a database of handwritten digits. It is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. Importing MNIST dataset from Keras.\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the MNIST 60000 Training and 10000 Test data into respective numpy arrays\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "327c76e7-d9cf-4fbb-8078-95bd90468a11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "# Printing the shape of the Training data\n",
        "print (X_train.shape)\n",
        "\n",
        "# Matplotlib is a plotting library for Python. Pyplot is a collection of command style functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc.\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Sets the backend of matplotlib to 'inline' backend. With this backend, the output of plotting commands is displayed inline within frontends like the Jupyter notebook, directly below the code cell that produced it.\n",
        "%matplotlib inline\n",
        "\n",
        "# Renders the image.\n",
        "plt.imshow(X_train[7777])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7f9081cb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshaping the Training and Test data to add the channel parameter as the input to Conv2D layer is expected to be of shape (img_rows, img_cols, num_channels) if data_format=\"channels_last\" which is default.\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalizing the Training and Test values\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "d37293ab-e6a9-488f-c262-29d0291a0a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Printing the first 10 labelled Training data\n",
        "y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "# np.utils.to_categorical is used to convert array of labelled data (from 0 to nb_classes-1) to one-hot vector.\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "5536095f-8c04-4e53-afc9-b66ea847b7d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Printing the first 10 labelled Training data after converting to one-hot vector.\n",
        "Y_train[:10]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "6917d8a9-7528-4a5d-94aa-8ebd1049edd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(8, 3, input_shape=(28,28,1), use_bias=False)) # RF - 3X3, O/P - 26x26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(0.125))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(14, 3, use_bias=False)) # RF - 5X5, O/P - 24x24\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(0.125))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, 3, use_bias=False)) # RF - 7X7, O/P - 22x22\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Convolution2D(8, 1, use_bias=False))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2, 2), strides=None, padding='valid', data_format=None)) # RF - 14X14, O/P - 11x11\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(8, 3, use_bias=False)) # RF - 16X16, O/P - 9X9\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(0.125))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(14, 3, use_bias=False)) # RF - 18X18, O/P - 7X7\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(0.125))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, 3, use_bias=False)) # RF - 20X20, O/P - 5X5\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Convolution2D(10, 1, use_bias=False))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(10, 5, use_bias=False)) # RF - 20X20, O/P - 5X5\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdAYg1k9K7Z",
        "colab_type": "code",
        "outputId": "83a4c00b-5628-4eca-9589-331fa65dff78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 8)         72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 14)        1008      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 14)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 24, 24, 14)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 24, 24, 14)        56        \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 16)        2016      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 22, 22, 8)         128       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 22, 22, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 8)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 11, 11, 8)         32        \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 8)           576       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 9, 9, 8)           0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 9, 9, 8)           0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 9, 9, 8)           32        \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 14)          1008      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 7, 7, 14)          0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 7, 7, 14)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 7, 7, 14)          56        \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 5, 5, 16)          2016      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 5, 5, 10)          160       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 5, 5, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 1, 1, 10)          2500      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 9,724\n",
            "Trainable params: 9,604\n",
            "Non-trainable params: 120\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the model\n",
        "# Loss function is one of the arguments required for compiling a model. categorical_crossentropy loss function is used if the targets are one-hot encoded.\n",
        "# Optimizer is one of the arguments required for compiling a model. Adam is an optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n",
        "# A metric is a function that is used to judge the performance of your model.\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmD4vTkYu_vH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import LearningRateScheduler\n",
        "#def lr_scheduler(epoch, lr):\n",
        "  #return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    if epoch > 40:\n",
        "        lr = 1e-5\n",
        "    elif epoch > 30:\n",
        "        lr = 0.5e-4\n",
        "    elif epoch > 20:\n",
        "        lr = 1e-4\n",
        "    elif epoch > 10:\n",
        "        lr = 0.5e-3\n",
        "    else:\n",
        "        lr = 1e-3\n",
        "    print('Learning rate (from LearningRateScheduler): ', lr)\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xWoKhPY9Of5",
        "colab_type": "code",
        "outputId": "e5a861a6-1be7-43b3-99a5-f87b61359b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2706
        }
      },
      "source": [
        "# Training the model for 25 epochs using batch_size of 128.\n",
        "model.fit(X_train, Y_train, batch_size=128, nb_epoch=50, verbose=1,validation_data=(X_test, Y_test),callbacks=[lr_scheduler])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.4200 - acc: 0.8659 - val_loss: 0.0986 - val_acc: 0.9700\n",
            "Epoch 2/50\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.1163 - acc: 0.9635 - val_loss: 0.0705 - val_acc: 0.9796\n",
            "Epoch 3/50\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0847 - acc: 0.9737 - val_loss: 0.0457 - val_acc: 0.9860\n",
            "Epoch 4/50\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0699 - acc: 0.9785 - val_loss: 0.0449 - val_acc: 0.9865\n",
            "Epoch 5/50\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0590 - acc: 0.9815 - val_loss: 0.0392 - val_acc: 0.9877\n",
            "Epoch 6/50\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0540 - acc: 0.9832 - val_loss: 0.0337 - val_acc: 0.9896\n",
            "Epoch 7/50\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0502 - acc: 0.9837 - val_loss: 0.0428 - val_acc: 0.9865\n",
            "Epoch 8/50\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0477 - acc: 0.9847 - val_loss: 0.0296 - val_acc: 0.9906\n",
            "Epoch 9/50\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0441 - acc: 0.9861 - val_loss: 0.0316 - val_acc: 0.9912\n",
            "Epoch 10/50\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0408 - acc: 0.9870 - val_loss: 0.0348 - val_acc: 0.9896\n",
            "Epoch 11/50\n",
            "Learning rate (from LearningRateScheduler):  0.001\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0397 - acc: 0.9875 - val_loss: 0.0316 - val_acc: 0.9906\n",
            "Epoch 12/50\n",
            "Learning rate (from LearningRateScheduler):  0.0005\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0328 - acc: 0.9890 - val_loss: 0.0254 - val_acc: 0.9925\n",
            "Epoch 13/50\n",
            "Learning rate (from LearningRateScheduler):  0.0005\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0332 - acc: 0.9890 - val_loss: 0.0246 - val_acc: 0.9912\n",
            "Epoch 14/50\n",
            "Learning rate (from LearningRateScheduler):  0.0005\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0322 - acc: 0.9899 - val_loss: 0.0273 - val_acc: 0.9925\n",
            "Epoch 15/50\n",
            "Learning rate (from LearningRateScheduler):  0.0005\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0309 - acc: 0.9903 - val_loss: 0.0278 - val_acc: 0.9923\n",
            "Epoch 16/50\n",
            "Learning rate (from LearningRateScheduler):  0.0005\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0308 - acc: 0.9899 - val_loss: 0.0254 - val_acc: 0.9929\n",
            "Epoch 17/50\n",
            "Learning rate (from LearningRateScheduler):  0.0005\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0294 - acc: 0.9906 - val_loss: 0.0265 - val_acc: 0.9922\n",
            "Epoch 18/50\n",
            "Learning rate (from LearningRateScheduler):  0.0005\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0287 - acc: 0.9907 - val_loss: 0.0268 - val_acc: 0.9915\n",
            "Epoch 19/50\n",
            "Learning rate (from LearningRateScheduler):  0.0005\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0286 - acc: 0.9905 - val_loss: 0.0265 - val_acc: 0.9923\n",
            "Epoch 20/50\n",
            "Learning rate (from LearningRateScheduler):  0.0005\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0273 - acc: 0.9912 - val_loss: 0.0268 - val_acc: 0.9929\n",
            "Epoch 21/50\n",
            "Learning rate (from LearningRateScheduler):  0.0005\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0274 - acc: 0.9911 - val_loss: 0.0254 - val_acc: 0.9925\n",
            "Epoch 22/50\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0232 - acc: 0.9925 - val_loss: 0.0249 - val_acc: 0.9933\n",
            "Epoch 23/50\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0223 - acc: 0.9926 - val_loss: 0.0248 - val_acc: 0.9925\n",
            "Epoch 24/50\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0224 - acc: 0.9925 - val_loss: 0.0239 - val_acc: 0.9934\n",
            "Epoch 25/50\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0223 - acc: 0.9929 - val_loss: 0.0235 - val_acc: 0.9940\n",
            "Epoch 26/50\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0218 - acc: 0.9928 - val_loss: 0.0243 - val_acc: 0.9931\n",
            "Epoch 27/50\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0215 - acc: 0.9932 - val_loss: 0.0237 - val_acc: 0.9929\n",
            "Epoch 28/50\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0222 - acc: 0.9927 - val_loss: 0.0241 - val_acc: 0.9932\n",
            "Epoch 29/50\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0205 - acc: 0.9935 - val_loss: 0.0246 - val_acc: 0.9925\n",
            "Epoch 30/50\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0218 - acc: 0.9928 - val_loss: 0.0246 - val_acc: 0.9926\n",
            "Epoch 31/50\n",
            "Learning rate (from LearningRateScheduler):  0.0001\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0207 - acc: 0.9934 - val_loss: 0.0237 - val_acc: 0.9934\n",
            "Epoch 32/50\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0219 - acc: 0.9928 - val_loss: 0.0235 - val_acc: 0.9935\n",
            "Epoch 33/50\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0209 - acc: 0.9932 - val_loss: 0.0233 - val_acc: 0.9934\n",
            "Epoch 34/50\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0208 - acc: 0.9935 - val_loss: 0.0237 - val_acc: 0.9933\n",
            "Epoch 35/50\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0202 - acc: 0.9934 - val_loss: 0.0235 - val_acc: 0.9934\n",
            "Epoch 36/50\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0214 - acc: 0.9931 - val_loss: 0.0235 - val_acc: 0.9940\n",
            "Epoch 37/50\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0212 - acc: 0.9932 - val_loss: 0.0238 - val_acc: 0.9938\n",
            "Epoch 38/50\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0197 - acc: 0.9935 - val_loss: 0.0240 - val_acc: 0.9937\n",
            "Epoch 39/50\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0197 - acc: 0.9935 - val_loss: 0.0238 - val_acc: 0.9937\n",
            "Epoch 40/50\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0202 - acc: 0.9931 - val_loss: 0.0238 - val_acc: 0.9936\n",
            "Epoch 41/50\n",
            "Learning rate (from LearningRateScheduler):  5e-05\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0197 - acc: 0.9935 - val_loss: 0.0243 - val_acc: 0.9934\n",
            "Epoch 42/50\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0194 - acc: 0.9933 - val_loss: 0.0240 - val_acc: 0.9938\n",
            "Epoch 43/50\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0194 - acc: 0.9937 - val_loss: 0.0240 - val_acc: 0.9937\n",
            "Epoch 44/50\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0205 - acc: 0.9932 - val_loss: 0.0239 - val_acc: 0.9937\n",
            "Epoch 45/50\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0195 - acc: 0.9935 - val_loss: 0.0240 - val_acc: 0.9936\n",
            "Epoch 46/50\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0202 - acc: 0.9935 - val_loss: 0.0239 - val_acc: 0.9939\n",
            "Epoch 47/50\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0197 - acc: 0.9935 - val_loss: 0.0240 - val_acc: 0.9938\n",
            "Epoch 48/50\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0203 - acc: 0.9933 - val_loss: 0.0239 - val_acc: 0.9938\n",
            "Epoch 49/50\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0186 - acc: 0.9940 - val_loss: 0.0240 - val_acc: 0.9938\n",
            "Epoch 50/50\n",
            "Learning rate (from LearningRateScheduler):  1e-05\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0194 - acc: 0.9935 - val_loss: 0.0240 - val_acc: 0.9937\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7fd8dbe8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i8YS_pmgeEi",
        "colab_type": "text"
      },
      "source": [
        "# Best Validation Accuracy\n",
        "Best validation accuracy is at **epoch 25** - **val_acc: 0.9940**\n",
        "\n",
        "Accuracy has increased compared to previous network where we used only dropouts and BatchNormalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluating the model on the Test data using the weights of last epoch. Returns the loss value & metrics values for the model in test mode.\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "d3fe50f5-8f35-4e74-bb9a-ac9f7bc921ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Printing the evaluation result\n",
        "print(score);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.023981187903237878, 0.9937]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generates output predictions for the input samples.\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "3d47004d-b70e-41ee-822d-65e9e9a0e967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# Printing the outputs. y_pred results do not look correct.\n",
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.84504994e-15 1.15541476e-09 2.72507890e-07 3.87021792e-08\n",
            "  1.94028380e-14 4.81114627e-12 1.19618109e-22 9.99999523e-01\n",
            "  3.19155320e-13 1.63626652e-07]\n",
            " [3.86477410e-07 3.89442266e-06 9.99989510e-01 1.01931166e-08\n",
            "  3.31349014e-07 3.11568913e-12 1.81135444e-08 5.89319188e-06\n",
            "  5.39429337e-08 3.91116001e-10]\n",
            " [2.85002744e-10 9.99995232e-01 1.47263413e-09 7.53100002e-13\n",
            "  1.31453010e-06 5.03359709e-09 3.62959902e-08 2.65254084e-06\n",
            "  6.19643345e-07 9.16732503e-08]\n",
            " [9.99884844e-01 1.02019348e-11 2.08294484e-08 2.14444711e-07\n",
            "  2.01879899e-07 1.94679865e-06 6.22164880e-05 1.16332433e-09\n",
            "  6.55660983e-07 4.98924237e-05]\n",
            " [3.41483005e-12 5.97513033e-13 2.92154009e-11 2.26089590e-15\n",
            "  9.99999642e-01 7.49932560e-13 4.02627888e-12 6.50104193e-09\n",
            "  3.47628537e-10 3.76675786e-07]\n",
            " [6.10838713e-10 9.99983191e-01 4.72820005e-09 1.24688102e-13\n",
            "  3.15517582e-06 1.86254442e-10 7.25132043e-09 1.35400896e-05\n",
            "  9.98064635e-08 3.76780420e-08]\n",
            " [8.51873608e-17 1.57303699e-08 1.75882642e-09 1.11679925e-13\n",
            "  9.99992251e-01 7.42262480e-13 1.96987150e-20 2.97680776e-06\n",
            "  1.97590406e-08 4.74699118e-06]\n",
            " [2.26650059e-07 5.91370153e-09 2.31769172e-06 4.38310025e-07\n",
            "  2.65185721e-04 9.56367899e-07 4.79561946e-11 2.24310828e-08\n",
            "  2.66762214e-07 9.99730647e-01]\n",
            " [7.87140209e-07 1.09677560e-13 1.26557065e-09 8.26438196e-10\n",
            "  1.18208707e-10 9.12468553e-01 8.72934982e-02 6.85969022e-14\n",
            "  2.36298089e-04 8.95698122e-07]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GY4Upv4dsUR",
        "colab_type": "code",
        "outputId": "673742e5-cfce-437a-b0cc-b69413927dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        }
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_1'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    else:\n",
        "        plot_x, plot_y = 2, 4\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0, 0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwcAAALyCAYAAACPcKhRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlAMAACCJcgAAAHSUAwAAIEnyv89tFyXx1S0AAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x864 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tvptcn8dxvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}